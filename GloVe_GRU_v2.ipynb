{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with an RNN and a Pretrained Embedding (GloVe)\n",
    "\n",
    "In this notebook, I implement a recursive neural network (RNN) composed of layers of GRU cells. This RNN will be trained using the traning set of the IMDB reviews database and it will be tested on its corresponding testing set. These databases has been downloaded from:\n",
    "__[http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)__\n",
    "\n",
    "In our neural network we will make use of GloVe, a pre-trained embedding layer whose details can be consulted here:\n",
    "__[https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)__\n",
    "\n",
    "The model is implemented using pyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS\n",
    "Let's import the modules we will use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPUs?\n",
    "Let's check if a GPU is available and select the device use for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if it's available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load the pretrained embedding\n",
    "\n",
    "We will use glove.6B.300d.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = 'glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load the embedding information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_int = {'<pad>': 0}\n",
    "emb_vectors = ['Fill at the End']\n",
    "words = ['<pad>']\n",
    "with open(glove_file, 'rb') as f:\n",
    "    idx = 1\n",
    "    for line in f:\n",
    "        line = line.decode().split()\n",
    "        word = line[0]\n",
    "        words.append(word)\n",
    "        word_to_int[word] = idx\n",
    "        emb_vectors.append(np.array(line[1:]).astype(np.float))\n",
    "        idx += 1\n",
    "emb_vectors[0] = np.zeros_like(emb_vectors[1])\n",
    "#convert to np array\n",
    "emb_vectors = np.array(emb_vectors)\n",
    "#convert to torch tensor\n",
    "emb_vectors = torch.from_numpy(emb_vectors).type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400002\n",
      "<pad> the <unk>\n"
     ]
    }
   ],
   "source": [
    "print(len(words))\n",
    "print(words[0],words[1],words[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([ 0.0466,  0.2132, -0.0074, -0.4585, -0.0356,  0.2364, -0.2884,  0.2152,\n",
      "        -0.1349, -1.6413, -0.2609,  0.0324,  0.0566, -0.0433, -0.0217,  0.2248,\n",
      "        -0.0751, -0.0670, -0.1425,  0.0388, -0.1895,  0.2998,  0.3930,  0.1789,\n",
      "        -0.1734, -0.2118,  0.2362, -0.0637, -0.4232, -0.1166,  0.0938,  0.1730,\n",
      "        -0.3307,  0.4911, -0.6899, -0.0925,  0.2474, -0.1799,  0.0979,  0.0831,\n",
      "         0.1530, -0.2728, -0.0389,  0.5445,  0.5374,  0.2910, -0.0074,  0.0479,\n",
      "        -0.4076, -0.0268,  0.1792,  0.0110, -0.1096, -0.2639,  0.0740,  0.2624,\n",
      "        -0.1508,  0.3462,  0.2576,  0.1197, -0.0371, -0.0716,  0.4390, -0.0408,\n",
      "         0.0164, -0.4464,  0.1720,  0.0462,  0.0586,  0.0415,  0.5395,  0.5250,\n",
      "         0.1136, -0.0483, -0.3638,  0.1870,  0.0928, -0.1113, -0.4209,  0.1399,\n",
      "        -0.3934, -0.0679,  0.1219,  0.1671,  0.0752, -0.0155, -0.1950,  0.1964,\n",
      "         0.0532,  0.2517, -0.3485, -0.1064, -0.3469, -0.1902, -0.2004,  0.1215,\n",
      "        -0.2921,  0.0234, -0.1162, -0.3577,  0.0623,  0.3588,  0.0291,  0.0073,\n",
      "         0.0049, -0.1505, -0.1231,  0.1934,  0.1217,  0.4450,  0.2515,  0.1078,\n",
      "        -0.1772,  0.0387,  0.0815,  0.1467,  0.0637,  0.0613, -0.0756, -0.3772,\n",
      "         0.0159, -0.3034,  0.2837, -0.0420, -0.0407, -0.1527,  0.0750,  0.1558,\n",
      "         0.1043,  0.3139,  0.1931,  0.1943,  0.1519, -0.1019, -0.0188,  0.2079,\n",
      "         0.1337,  0.1904, -0.2556,  0.3040, -0.0190,  0.2015, -0.4211, -0.0075,\n",
      "        -0.2798, -0.1931,  0.0462,  0.1997, -0.3021,  0.2573,  0.6811, -0.1941,\n",
      "         0.2398,  0.2249,  0.6522, -0.1356, -0.1738, -0.0482, -0.1186,  0.0022,\n",
      "        -0.0195,  0.1195,  0.1935, -0.4082, -0.0830,  0.1663, -0.1060,  0.3586,\n",
      "         0.1692,  0.0726, -0.2480, -0.1002, -0.5249, -0.1775, -0.3665,  0.2618,\n",
      "        -0.0121,  0.0832, -0.2153,  0.4105,  0.2914,  0.3087,  0.0789,  0.3221,\n",
      "        -0.0410, -0.1097, -0.0920, -0.1234, -0.1642,  0.3538, -0.0828,  0.3317,\n",
      "        -0.2474, -0.0489,  0.1575,  0.1899, -0.0266,  0.0633, -0.0107,  0.3409,\n",
      "         1.4106,  0.1342,  0.2819, -0.2594,  0.0553, -0.0524, -0.2579,  0.0191,\n",
      "        -0.0221,  0.3211,  0.0688,  0.5121,  0.1648, -0.2019,  0.2923,  0.0986,\n",
      "         0.0131, -0.1065,  0.1351, -0.0453,  0.2070, -0.4843, -0.4471,  0.0033,\n",
      "         0.0029, -0.1098, -0.2333,  0.2244, -0.1050,  0.1234,  0.1098,  0.0490,\n",
      "        -0.2516,  0.4032,  0.3532,  0.1865, -0.0236, -0.1273,  0.1147,  0.2736,\n",
      "        -0.2187,  0.0158,  0.8175, -0.0238, -0.8547, -0.1620,  0.1808,  0.0280,\n",
      "        -0.1434,  0.0013, -0.0917, -0.0897,  0.1111, -0.1670,  0.0684, -0.0874,\n",
      "        -0.0398,  0.0142,  0.2119,  0.2858, -0.2880, -0.0590, -0.0324, -0.0047,\n",
      "        -0.1705, -0.0347, -0.1149,  0.0751,  0.0995,  0.0482, -0.0738, -0.4182,\n",
      "         0.0041,  0.4441, -0.1606,  0.1429, -2.2628, -0.0273,  0.8131,  0.7742,\n",
      "        -0.2564, -0.1158, -0.1198, -0.2136,  0.0284,  0.2726,  0.0310,  0.0968,\n",
      "         0.0068,  0.1408, -0.0131, -0.2969, -0.0799,  0.1950,  0.0315,  0.2851,\n",
      "        -0.0875,  0.0091, -0.2099,  0.0539])\n",
      "tensor([ 0.4292, -0.2969,  0.1501,  0.2452, -0.0035, -0.0577,  0.1409, -0.2223,\n",
      "         0.2212,  0.7672, -0.0773, -0.0711,  0.0629, -0.2202, -0.1082, -0.3014,\n",
      "         0.2322,  0.1687, -0.0045,  0.1683, -0.0579, -0.0363, -0.2735, -0.1630,\n",
      "         0.2394, -0.0119,  0.0447,  0.1053,  0.1029, -0.0233, -0.0114, -0.3817,\n",
      "         0.0612,  0.0171,  0.4155, -0.1091,  0.0960,  0.1915, -0.0075, -0.1946,\n",
      "        -0.0432,  0.2598,  0.0053, -0.1836,  0.2252, -0.0188, -0.1582, -0.5869,\n",
      "         0.2493, -0.1303, -0.0537,  0.0316, -0.1856,  0.0610, -0.0851, -0.0965,\n",
      "         0.2786, -0.2473, -0.1539,  0.0418,  0.0704, -0.0623, -0.2849,  0.0152,\n",
      "         0.1440,  0.3359, -0.2883, -0.0025, -0.0876, -0.0574,  0.0067, -0.0753,\n",
      "        -0.0678, -0.0566,  0.1930,  0.0250, -0.3919, -0.1593,  0.2612,  0.1022,\n",
      "         0.0877,  0.0433, -0.1798, -0.1897,  0.0511, -0.0164, -0.0071, -0.3277,\n",
      "        -0.2075, -0.0213,  0.1167, -0.0676,  0.2681,  0.0962,  0.0516, -0.0365,\n",
      "         0.3172, -0.1589, -0.0555,  0.2879, -0.1407, -0.2257, -0.0546,  0.2120,\n",
      "        -0.0359, -0.0980, -0.0192, -0.1864,  0.2986, -0.1337, -0.1143,  0.3033,\n",
      "         0.1427,  0.0511,  0.1112, -0.1064,  0.2469, -0.0652,  0.1377,  0.2276,\n",
      "        -0.0368,  0.1394, -0.1103, -0.0729,  0.0966,  0.0341,  0.2667, -0.0070,\n",
      "         0.0285, -0.2860,  0.1485, -0.3518, -0.1805,  0.0751, -0.0414,  0.0232,\n",
      "         0.1345,  0.2345,  0.0078, -0.4310, -0.1712, -0.0481, -0.1448, -0.1056,\n",
      "         0.4121, -0.0439, -0.1226, -0.1055,  0.1864, -0.0875, -0.3612,  0.1370,\n",
      "        -0.1449,  0.0686, -0.4516, -0.0748,  0.2358, -0.1471, -0.2086,  0.0403,\n",
      "        -0.2592,  0.2911, -0.0382, -0.2061, -0.0899,  0.0436, -0.1813, -0.0927,\n",
      "         0.0721, -0.3280, -0.0489, -0.0824,  0.5907, -0.3320,  0.1504, -0.0933,\n",
      "         0.1255,  0.2314,  0.0384, -0.3100, -0.1767, -0.1762, -0.0883,  0.0158,\n",
      "         0.2118,  0.2100,  0.3736,  0.0085,  0.1620, -0.2207,  0.1932, -0.1714,\n",
      "         0.0398, -0.0031,  0.0125, -0.0605, -0.0638, -0.1928,  0.0692, -0.2356,\n",
      "        -0.6954,  0.0394,  0.0050,  0.0143, -0.0900, -0.1120,  0.1007, -0.1841,\n",
      "        -0.0590, -0.0465, -0.0151, -0.0513, -0.0988, -0.0367, -0.3037, -0.0170,\n",
      "         0.0807, -0.1951,  0.1504, -0.1495, -0.3181,  0.1109,  0.2607, -0.0893,\n",
      "        -0.0852, -0.1558, -0.0601, -0.0389,  0.3090,  0.1097,  0.0357, -0.1583,\n",
      "        -0.1892,  0.0513,  0.0812, -0.3440, -0.2057, -0.1531,  0.2535, -0.0531,\n",
      "         0.0590,  0.0415, -0.1990, -0.0431,  0.3675,  0.0419, -0.1595,  0.1763,\n",
      "         0.3569, -0.0974, -0.1644, -0.0779,  0.2681,  0.1840, -0.2349,  0.2504,\n",
      "         0.1221,  0.0239, -0.2938, -0.0107, -0.1726,  0.0896, -0.2492, -0.1783,\n",
      "        -0.1001,  0.1745,  0.0815, -0.2593, -0.0361, -0.1837,  0.1232,  0.1818,\n",
      "        -0.1183, -0.2754,  0.0096, -0.0358,  0.7829,  0.0814, -0.3090,  0.0045,\n",
      "         0.1725,  0.0388,  0.0374,  0.0566,  0.0239, -0.2575,  0.1575, -0.2822,\n",
      "        -0.1325,  0.2175,  0.1281,  0.0976, -0.1310, -0.1428, -0.1755, -0.1690,\n",
      "        -0.0225,  0.2898,  0.3262, -0.0591])\n"
     ]
    }
   ],
   "source": [
    "print(emb_vectors[0])\n",
    "print(emb_vectors[1])\n",
    "print(emb_vectors[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load in, tokenize and visualize the data\n",
    "\n",
    "The download data is already divied into train and test data. Each folder is further divided into positive (7-10/10 stars reviews) and negative reviews (1-4/10 stars reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SET\n",
      "Number of positive reviews: 12500 Number of negative reviews: 12500\n"
     ]
    }
   ],
   "source": [
    "#training positive reviews directory\n",
    "train_pos_dir = r'aclImdb/train/pos/'\n",
    "#negative positive reviews directory\n",
    "train_neg_dir = r'aclImdb/train/neg/'\n",
    "\n",
    "#List of files with training positive review\n",
    "train_pos_rev_files = os.listdir(train_pos_dir)\n",
    "#List of files with training negative review\n",
    "train_neg_rev_files = os.listdir(train_neg_dir)\n",
    "print('TRAIN SET')\n",
    "print('Number of positive reviews:',len(train_pos_rev_files),'Number of negative reviews:',len(train_neg_rev_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep track of the number of stars (1-10) awarded to each positive and negative review in the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_stars = []\n",
    "for file in train_pos_rev_files:\n",
    "    full_train_stars.append(int(file.split('_')[1].split('.')[0]))\n",
    "for file in train_neg_rev_files:\n",
    "    full_train_stars.append(int(file.split('_')[1].split('.')[0]))    \n",
    "full_train_stars = np.array(full_train_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST SET\n",
      "Number of positive reviews: 12500 Number of negative reviews: 12500\n"
     ]
    }
   ],
   "source": [
    "#training positive reviews directory\n",
    "test_pos_dir = r'aclImdb/test/pos/'\n",
    "#negative positive reviews directory\n",
    "test_neg_dir = r'aclImdb/test/neg/'\n",
    "\n",
    "#List of files with training positive review\n",
    "test_pos_rev_files = os.listdir(test_pos_dir)\n",
    "#List of files with training negative review\n",
    "test_neg_rev_files = os.listdir(test_neg_dir)\n",
    "print('TEST SET')\n",
    "print('Number of positive reviews:',len(test_pos_rev_files),'Number of negative reviews:',len(test_neg_rev_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep track of the number of stars (1-10) awarded to each positive and negative review in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stars = []\n",
    "for file in test_pos_rev_files:\n",
    "    test_stars.append(int(file.split('_')[1].split('.')[0]))\n",
    "for file in test_neg_rev_files:\n",
    "    test_stars.append(int(file.split('_')[1].split('.')[0]))    \n",
    "test_stars = np.array(test_stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that 5- and 6-star reviews are not included in the train or test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  7  8  9 10]\n",
      "[ 1  2  3  4  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(full_train_stars))\n",
    "print(np.unique(test_stars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Matrices\n",
    "Let's create numpy arrays that hold the train and test labels. 1 stands for positive and 0 for negative. Since we will stack first the positive reviews and the the negative ones, the first 12500 elements are ones and the next 12500 are zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train target array\n",
    "full_train_target = np.zeros(len(train_neg_rev_files)+len(train_neg_rev_files), dtype=int)\n",
    "full_train_target[:len(train_neg_rev_files)] = 1\n",
    "#Test target array\n",
    "test_target = np.zeros(len(test_neg_rev_files)+len(test_neg_rev_files), dtype=int)\n",
    "test_target[:len(test_neg_rev_files)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,)\n",
      "0.5 0.5\n"
     ]
    }
   ],
   "source": [
    "print(full_train_target.shape, test_target.shape)\n",
    "print(full_train_target.mean(), test_target.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Contractions\n",
    "The following function expands common english contractions. There is obviously  plenty of room for improvement. Some \n",
    "actions are 100% justified (can't and cannot into can not), while others are rather arbitrary ('s into is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_contractions(text):\n",
    "    # Turn ain't into am not (it could be many other oprions such us is not,...)\n",
    "    text = text.replace(\"ain't\", \"am not\")\n",
    "    #Turn can't and annot into can not\n",
    "    text = text.replace(\"can't\", \"can not\").replace(\"cannot\", \"can not\")\n",
    "    #Turn shan't into shall not\n",
    "    text = text.replace(\"shan't\", \"shall not\")\n",
    "    #Turn won't into will not\n",
    "    text = text.replace(\"won't\", \"will not\")\n",
    "    #Turn y'all into you all\n",
    "    text = text.replace(\"y'all\", \"you all\")\n",
    "    #Turn n't into not\n",
    "    text = text.replace(\"n't\", \" not\")\n",
    "    #Turn 'd into would (it might be had too)\n",
    "    text = text.replace(\"'d\", \" would\")\n",
    "    #Turn 'll into will\n",
    "    text = text.replace(\"'ll\", \" will\")\n",
    "    #Turn 're into are\n",
    "    text = text.replace(\"'re\", \" are\")\n",
    "    #Turn 'm into am\n",
    "    text = text.replace(\"'m\", \" am\")\n",
    "    #Turn 's into is (it could also be has or the posssesive)\n",
    "    text = text.replace(\"'s\", \" is\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviews as List of Words\n",
    "For each review we convert every character to lower case, remove english contructions, then remove punctuation and finally split it into words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_reviews=[]\n",
    "#Read and tokenize positive reviews\n",
    "for file_name in train_pos_rev_files:\n",
    "    with open(train_pos_dir+file_name, 'r',encoding=\"utf8\") as f:\n",
    "        review = f.read().lower()\n",
    "        #remove some contractions\n",
    "        review = remove_contractions(review)\n",
    "        #remove punctuation\n",
    "        review = ''.join([c for c in review if c not in punctuation])\n",
    "        #split in words\n",
    "        review = review.split()  \n",
    "    full_train_reviews.append(review)\n",
    "\n",
    "#Read and tokenize negative reviews\n",
    "for file_name in train_neg_rev_files:\n",
    "    with open(train_neg_dir+file_name, 'r',encoding=\"utf8\") as f:\n",
    "        review = f.read().lower()\n",
    "        #remove some contractions\n",
    "        review = remove_contractions(review)\n",
    "        #remove punctuation\n",
    "        review = ''.join([c for c in review if c not in punctuation])\n",
    "        #split in words\n",
    "        review = review.split()  \n",
    "    full_train_reviews.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "['antwone', 'fisher', 'tells', 'of', 'a', 'young', 'black', 'us', 'navy', 'enlisted', 'man', 'and', 'product', 'of', 'childhood', 'abuse', 'and', 'neglect', 'luke', 'whose', 'hostility', 'toward', 'others', 'gets', 'him', 'a', 'stint', 'with', 'the', 'base', 'shrink', 'washington', 'leading', 'to', 'introspection', 'self', 'appraisal', 'and', 'a', 'return', 'to', 'his', 'roots', 'pat', 'sanitized', 'and', 'sentimental', 'antwone', 'fisher', 'is', 'a', 'solid', 'feelgood', 'flick', 'about', 'the', 'reconciliation', 'of', 'past', 'regrets', 'and', 'closure', 'good', 'old', 'hollywood', 'style', 'entertainment', 'family', 'values', 'entertainment', 'with', 'just', 'a', 'hint', 'of', 'corn', 'b']\n"
     ]
    }
   ],
   "source": [
    "print(len(full_train_reviews))\n",
    "print(full_train_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reviews=[]\n",
    "#Read and tokenize positive reviews\n",
    "for file_name in test_pos_rev_files:\n",
    "    with open(test_pos_dir+file_name, 'r',encoding=\"utf8\") as f:\n",
    "        review = f.read().lower()\n",
    "        #remove some contractions\n",
    "        review = remove_contractions(review)\n",
    "        #remove punctuation\n",
    "        review = ''.join([c for c in review if c not in punctuation])\n",
    "        #split in words\n",
    "        review = review.split()  \n",
    "    test_reviews.append(review)\n",
    "\n",
    "#Read and tokenize negative reviews\n",
    "for file_name in test_neg_rev_files:\n",
    "    with open(test_neg_dir+file_name, 'r',encoding=\"utf8\") as f:\n",
    "        review = f.read().lower()\n",
    "        #remove some contractions\n",
    "        review = remove_contractions(review)\n",
    "        #remove punctuation\n",
    "        review = ''.join([c for c in review if c not in punctuation])\n",
    "        #split in words\n",
    "        review = review.split()  \n",
    "    test_reviews.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "['spoilers', 'even', 'though', 'the', 'movie', 'they', 'made', 'me', 'a', 'criminal', 'is', 'nowhere', 'as', 'good', 'as', 'the', 'later', 'john', 'garfield', 'antihero', 'classics', 'like', 'body', 'soul', 'in', '1947', 'force', 'of', 'evil', 'in', '1948', 'and', 'his', 'last', 'and', 'very', 'underrated', 'he', 'ran', 'all', 'the', 'way', 'in', '1951', 'it', 'is', 'the', 'film', 'that', 'defined', 'his', 'career', 'from', 'that', 'point', 'onward', 'until', 'his', 'untimely', 'death', 'on', 'may', '21', '1952', 'at', 'the', 'young', 'age', 'of', '39', 'br', 'br', 'garfiled', 'plays', 'the', 'part', 'of', 'light', 'weight', 'champion', 'johnnie', 'bradfield', 'and', 'later', 'the', 'fugitive', 'from', 'the', 'law', 'jack', 'dorney', 'who', 'is', 'innocent', 'of', 'the', 'murder', 'that', 'he', 'is', 'charged', 'with', 'even', 'though', 'he', 'is', 'been', 'declared', 'officially', 'dead', 'jonnie', 'is', 'manager', 'doc', 'ward', 'robert', 'doc', 'gleckler', 'who', 'during', 'a', 'drunken', 'victory', 'party', 'killed', 'reporter', 'charles', 'mcgeejohn', 'ridgely', 'who', 'was', 'going', 'to', 'expose', 'to', 'the', 'public', 'his', 'fighter', 'johnnie', 'bradfield', 'lies', 'about', 'him', 'being', 'a', 'one', 'women', 'guy', 'as', 'well', 'as', 'non', 'drinking', 'momma', 'is', 'boy', 'doc', 'gleckler', 'smashed', 'a', 'bottle', 'over', 'mcgee', 'is', 'head', 'killing', 'him', 'as', 'jonnie', 'was', 'almost', 'dead', 'drunk', 'with', 'a', 'number', 'of', 'women', 'partying', 'in', 'his', 'hotel', 'suite', 'br', 'br', 'doc', 'was', 'later', 'killed', 'in', 'a', 'car', 'crash', 'with', 'johnnie', 'is', 'girlfriend', 'goldie', 'ann', 'sheridan', 'but', 'doc', 'burned', 'to', 'a', 'crisp', 'and', 'with', 'johnnie', 'is', 'watch', 'on', 'him', 'was', 'mistaken', 'for', 'johnnie', 'told', 'to', 'stay', 'dead', 'and', 'buried', 'by', 'his', 'lawyer', 'malvin', 'robert', 'strange', 'who', 'took', '975000', 'of', 'the', '1000000', 'of', 'johnnie', 'is', 'money', 'that', 'he', 'had', 'for', 'this', 'great', 'piece', 'of', 'advice', 'malvin', 'told', 'johnnie', 'to', 'take', 'on', 'a', 'new', 'identity', 'and', 'call', 'himself', 'from', 'now', 'on', 'jack', 'dorney', 'and', 'get', 'the', 'hell', 'out', 'of', 'the', 'state', 'of', 'new', 'york', 'talking', 'about', 'sleazy', 'shysters', 'johnnie', 'now', 'jack', 'dorney', 'travels', 'the', 'rails', 'from', 'new', 'york', 'down', 'to', 'arizona', 'ending', 'up', 'at', 'the', 'rancho', 'rafferty', 'date', 'farm', 'where', 'most', 'of', 'the', 'film', 'takes', 'placebr', 'br', 'if', 'it', 'was', 'not', 'for', 'john', 'garfield', 'in', 'the', 'lead', 'role', 'as', 'both', 'jonnie', 'bradfield', 'jack', 'dorney', 'the', 'movie', 'would', 'have', 'long', 'been', 'lost', 'and', 'forgotten', 'garfield', 'who', 'was', 'only', '26', 'at', 'the', 'time', 'brought', 'the', 'best', 'out', 'of', 'everyone', 'in', 'the', 'movie', 'even', 'the', 'transported', 'dead', 'end', 'kids', 'i', 'guess', 'we', 'can', 'call', 'them', 'the', 'arizona', 'kids', 'here', 'acting', 'were', 'notches', 'above', 'what', 'you', 'would', 'have', 'expected', 'from', 'them', 'and', 'they', 'came', 'across', 'as', 'real', 'and', 'sensitive', 'persons', 'not', 'a', 'bunch', 'of', 'slap', 'stick', 'clowns', 'like', 'in', 'almost', 'all', 'of', 'their', 'movies', 'all', 'that', 'due', 'to', 'being', 'on', 'the', 'same', 'stage', 'or', 'filming', 'location', 'with', 'john', 'garfield', 'br', 'br', 'they', 'made', 'me', 'a', 'criminal', 'is', 'a', 'good', 'story', 'that', 'has', 'the', 'undercover', 'champ', 'acting', 'like', 'anything', 'but', 'not', 'to', 'draw', 'any', 'attention', 'on', 'himself', 'and', 'end', 'up', 'not', 'only', 'behind', 'bars', 'but', 'in', 'the', 'electric', 'chair', 'in', 'the', 'end', 'jack', 'showed', 'just', 'what', 'kind', 'of', 'man', 'he', 'is', 'by', 'not', 'fighting', 'the', 'big', 'fight', 'and', 'against', 'all', 'the', 'odds', 'dramatically', 'winning', 'at', 'the', 'last', 'moment', 'but', 'by', 'going', 'four', 'brutal', 'rounds', 'to', 'get', 'the', 'money', 'for', 'his', 'new', 'found', 'family', 'at', 'the', 'date', 'farm', 'including', 'his', 'girl', 'peggy', 'gloria', 'dickson', 'to', 'open', 'up', 'a', 'gas', 'station', 'with', 'it', 'br', 'br', 'giving', 'the', 'european', 'champ', 'gaspar', 'rutchek', 'frank', 'riggi', 'the', 'fight', 'of', 'his', 'life', 'and', 'getting', '200000', 'thats', '50000', 'a', 'round', 'for', 'doing', 'it', 'jack', 'showed', 'everyone', 'who', 'looked', 'up', 'to', 'him', 'like', 'the', 'arizona', 'kids', 'that', 'sometimes', 'taking', 'a', 'punch', 'is', 'far', 'braver', 'and', 'more', 'courageous', 'then', 'throwing', 'onethe', 'fact', 'that', 'jack', 'could', 'have', 'easily', 'clobbered', 'rutched', 'but', 'did', 'not', 'in', 'order', 'not', 'to', 'expose', 'himself', 'to', 'the', 'police', 'as', 'on', 'the', 'loose', 'killer', 'johnnie', 'bradfield', 'but', 'instead', 'went', 'as', 'far', 'as', 'he', 'could', 'taking', 'everything', 'that', 'rutchek', 'could', 'throw', 'at', 'him', 'to', 'help', 'out', 'his', 'friends', 'showed', 'more', 'then', 'all', 'the', 'fights', 'that', 'he', 'won', 'in', 'the', 'boxing', 'ring', 'put', 'together', 'br', 'br', 'i', 'for', 'one', 'did', 'not', 'find', 'the', 'ending', 'of', 'the', 'movie', 'contrived', 'at', 'all', 'but', 'fitting', 'right', 'in', 'with', 'the', 'story', 'the', 'cop', 'morty', 'phelam', 'claude', 'rains', 'who', 'came', 'to', 'arizona', 'from', 'new', 'york', 'to', 'arrest', 'jack', 'had', 'to', 'live', 'with', 'for', 'years', 'the', 'fact', 'that', 'he', 'once', 'sent', 'an', 'innocent', 'man', 'to', 'the', 'electric', 'chair', 'we', 'were', 'told', 'all', 'this', 'right', 'at', 'the', 'start', 'of', 'the', 'movie', 'why', 'knowing', 'that', 'jackjohnnie', 'was', 'innocent', 'of', 'the', 'murder', 'that', 'he', 'is', 'charged', 'with', 'and', 'not', 'knowing', 'for', 'sure', 'if', 'he', 'will', 'be', 'found', 'innocent', 'of', 'it', 'in', 'a', 'court', 'of', 'law', 'would', 'he', 'want', 'to', 'make', 'the', 'same', 'terrible', 'mistake', 'again', 'i', 'can', 'easily', 'see', 'this', 'happening', 'in', 'real', 'life', 'why', 'not', 'then', 'in', 'the', 'movies']\n"
     ]
    }
   ],
   "source": [
    "print(len(test_reviews))\n",
    "print(test_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Sets\n",
    "\n",
    "We will divide our full train set into a train set and a validation set that will help us to control how our model generalize.\n",
    "The validation set will be just 10% of the original train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#90% will remain as training data\n",
    "train_size = 0.9; test_size = 1-train_size\n",
    "(train_reviews, valid_reviews, train_y, valid_y, \n",
    " train_stars, valid_stars) = train_test_split(full_train_reviews, full_train_target, full_train_stars,\n",
    "                                              random_state=123, shuffle=True,\n",
    "                                              train_size=train_size, test_size=test_size,stratify=full_train_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22500 (22500,) (22500,)\n",
      "2500 (2500,) (2500,)\n",
      "0.5 0.5\n"
     ]
    }
   ],
   "source": [
    "#Print out the shapes of your resultant feature data\n",
    "print(len(train_reviews), train_y.shape, train_stars.shape)\n",
    "print(len(valid_reviews), valid_y.shape, valid_stars.shape)\n",
    "print(train_y.mean(), valid_y.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To keep it simple let's create a test_y variable\n",
    "test_y = test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing the size of the reviews\n",
    "It is important to xhoose now the word size of our review before we build our vocabulary. That will prevent us from including\n",
    "words that may appear frequently in our reviews if we keep all the words but not more than five times if we keep, \n",
    "let's say, the first 50 words only. Of course, the larger the sequence considered the less important this would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Sequence length #########\n",
    "seq_length = 350"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "\n",
    "Let's get our data into the proper form to feed into the network. Since we're using embedding layers, we'll need to encode each word with an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the words\n",
    "\n",
    "The embedding lookup requires that we pass in integers to our network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Then we can convert each of our reviews into integers so they can be passed into the network.Also we are going to keep the value zero to represent the padding and the value 1 to represent every word\n",
    "that appears least than five times in our train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's tokenize the reviews into integers\n",
    "#Index of unknown token\n",
    "unk = word_to_int['<unk>']\n",
    "# train set\n",
    "train_reviews_int = []\n",
    "num_words = 0\n",
    "num_unks = 0\n",
    "for review in train_reviews:\n",
    "    review_int = []\n",
    "    for word in review:\n",
    "        if word in word_to_int:\n",
    "            num_words += 1\n",
    "            review_int.append(word_to_int[word])\n",
    "        else:\n",
    "            num_unks += 1\n",
    "            review_int.append(unk)\n",
    "    train_reviews_int.append(review_int)       \n",
    "    \n",
    "# validation set\n",
    "valid_reviews_int = []\n",
    "for review in valid_reviews:\n",
    "    valid_reviews_int.append([word_to_int.get(word,unk) for word in review])\n",
    "# test set\n",
    "test_reviews_int = []\n",
    "for review in test_reviews:\n",
    "    test_reviews_int.append([word_to_int.get(word,unk) for word in review])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "Known words: 5227921 \t Unknown words: 106297\n"
     ]
    }
   ],
   "source": [
    "print('Train Set')\n",
    "print(f'Known words: {num_words} \\t Unknown words: {num_unks}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test your code**\n",
    "\n",
    "As a text that you've implemented the dictionary correctly, print out the number of unique words in your vocabulary and the contents of the first, tokenized review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized review: \n",
      " [1, 1396, 4, 26721, 32, 52, 9290, 5, 101, 110, 72456, 6, 101, 110, 979, 13758, 13, 156, 39, 33, 2711, 11, 8, 495, 922, 4, 1063, 70, 118, 286, 42, 1703, 39, 5573, 1, 26721, 1146, 20, 8, 4314, 6, 34, 691, 9049, 11, 8, 1006, 1063, 13, 929, 1, 1146, 121, 8, 334, 6916, 5, 347, 198, 1, 398, 34120, 12282, 61, 78, 4, 1, 65662, 3690, 1477, 38, 390, 35, 188, 15, 193, 400001, 30411, 42, 914, 3198, 4, 92, 400001, 400001, 20908, 4, 26721, 7477, 149138, 18, 3258, 12006, 6, 1, 3206, 6093, 1063, 13, 842, 5, 15933, 9051, 1819, 95253, 15, 872, 4002, 1396, 1, 631, 4, 8, 38151, 712, 6810, 1335, 2530, 61, 50, 45, 337, 22, 8, 5120, 4, 23573, 39, 1475, 104, 20, 49, 4, 1001, 6, 39, 2463, 2371, 4, 1, 7529, 114, 4, 27, 4741, 474, 23450, 107, 434, 21, 138, 5, 564, 6, 3655, 27, 69, 3005, 872, 74, 1, 3519, 2291, 400001, 508, 3601, 949, 10492, 6, 508, 24243, 33, 27, 1614, 22697, 27, 7272, 8893, 5, 2200, 3735, 712, 505, 174, 4764, 5, 70, 39, 347, 1, 1033, 27, 9550, 4, 12682, 14, 18355, 27, 2175, 22048, 34340, 60, 1, 92, 874, 159, 56, 2154, 34, 7, 862, 33, 1, 312, 26721, 6, 1, 854, 13, 40, 151, 34, 8, 703, 565, 4918, 4451, 24368, 15, 817, 1063, 15, 646, 4, 8, 3625, 118, 25167, 1396, 19, 6053, 220, 712, 6, 15, 20807, 6, 38769, 7, 151, 7221, 7, 8, 2599, 4, 400001, 180, 35, 19, 15, 85, 2371, 149138, 6, 64, 15, 85, 400001, 30411, 144, 38, 320, 15, 7, 8, 246, 2892, 4, 26721, 1589, 6, 42, 825, 21, 949, 8, 2892, 4, 49, 114, 21, 15, 9957, 38, 2892, 2055, 1, 1396, 20, 8, 1908, 11, 4, 65, 655, 3339, 14957, 4918, 335, 22, 122623, 979, 2891, 11108, 7982, 15, 7, 638, 863, 11, 72, 5949, 1, 400001, 1283, 39, 15, 28294, 1, 7529, 863, 11, 646, 43, 15, 333, 9488, 414, 72, 5949, 15, 2838, 5, 34, 52, 1511, 11, 8, 174, 80, 68, 3381, 104, 34495, 1187, 3731, 20, 1, 5949, 15, 1, 255, 874, 188, 19, 3110, 1, 320, 15, 8980, 101, 19, 32, 2906, 129560, 6, 83108, 7, 8, 180, 13, 12911, 286, 4, 1295, 400001, 15, 7690, 10818, 884, 7, 103, 15, 51, 48595, 5949, 5808, 1, 4755, 4, 26721, 78, 2599, 4, 7432, 47, 2944, 19, 400001, 8, 7844, 26, 47, 30, 400001, 1267, 23, 12124, 5, 692, 728, 5, 27, 1132, 39, 15, 23, 38, 390, 16247, 18, 1, 369294, 1861, 5882, 54, 2272, 1, 10481, 70441, 4, 1, 26721, 22697, 5949, 5377, 43, 17478, 261, 37, 1915, 5, 57899, 27, 4904, 1132, 192, 182, 6, 400001, 30411, 128, 26721, 1659, 35, 211, 937, 2748, 19, 211, 937, 106, 1, 1453, 320, 19, 6, 4918, 808, 7, 836, 6, 40, 26927, 205, 2759, 592, 937, 20, 68, 53575, 130334, 400001, 91153, 1, 157, 13, 15, 1, 2220, 30411, 30411, 144, 37, 2797, 64, 15, 53, 8, 1501, 112, 26721, 54857, 71607, 18, 8, 28609, 400001, 97, 3218, 28609, 109, 21, 15, 1, 92, 17760, 49, 662, 542, 4918, 13198, 1, 9316, 18, 1, 843, 4, 8, 42597, 39, 3664, 22, 37503, 67, 1, 12621, 68, 5655, 139, 5, 15423, 1, 5869, 18, 42, 914, 37, 434, 38, 61, 6, 38, 15, 92, 49, 4, 110, 15516, 11, 72, 5, 191, 72, 4001, 400001, 30411, 42, 691, 118, 5, 7631, 8, 1288, 22, 346, 646, 1336, 35, 38, 80, 21, 15, 606, 887, 15, 254, 144, 21, 15, 6152, 5, 10828, 38, 320, 11, 3275, 30, 52762, 152, 414, 54, 65, 347, 44577, 89, 37, 683, 7, 638, 50, 65, 1, 2393, 26721, 2460, 65, 181, 793, 11264, 120, 40, 37, 53, 82, 34, 5, 6339, 13, 11108, 7982, 15, 1923, 7, 622, 6, 684, 35, 7, 13, 306, 739, 1, 5614, 120, 37, 68, 121, 160, 8, 433, 20, 30, 761, 1424, 103, 261, 21, 204, 60, 8, 1006, 62, 21, 2463, 5406, 12218, 5, 663, 23, 8, 1923, 788, 42, 1404, 34, 37, 848, 402, 21, 15, 8, 1336, 47, 8, 2564, 13, 40, 333, 897, 72, 53565, 35, 115, 42, 4224, 842, 20, 42, 108, 42, 87, 37, 157, 14, 8, 1336, 1813, 30411, 30411, 254, 38, 320, 84, 82, 33, 8, 979, 320, 23636, 42, 914, 287953, 188]\n",
      "22500\n"
     ]
    }
   ],
   "source": [
    "# print tokens in first review\n",
    "print('Tokenized review: \\n', train_reviews_int[0])\n",
    "print(len(train_reviews_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of the Reviews \n",
    "\n",
    "We have already set for a value of 500 words but it is convinient to check some statistic about the review lenght (mean,\n",
    "median, std). We may even decide to run the whole notebook with a different sequence lenght."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum review length: 11\n",
      "Maximum review length: 2501\n",
      "Mean and Median review length: 237.07635555555555\t177.0\n",
      "Std review length: 176.56215887784936\n"
     ]
    }
   ],
   "source": [
    "review_lens = np.array([len(x) for x in train_reviews_int])\n",
    "\n",
    "print(f\"Minimum review length: {review_lens.min()}\")\n",
    "print(f\"Maximum review length: {review_lens.max()}\")\n",
    "print(f'Mean and Median review length: {review_lens.mean()}\\t{np.median(review_lens)}')\n",
    "print(f'Std review length: {review_lens.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Padding sequences\n",
    "\n",
    "To deal with both short and very long reviews, we'll pad or truncate all our reviews to a specific length. For reviews shorter than some `seq_length`, we'll pad with 0s. For reviews longer than `seq_length`, we can truncate them to the first `seq_length` words. We will work with a sewuence lenght of 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length, alignment='right'):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    features=np.zeros((len(reviews_ints),seq_length),dtype=int)\n",
    "    \n",
    "    if alignment.lower() == 'right':\n",
    "        my_idxs = lambda length: ((seq_length-length),seq_length) \n",
    "    elif alignment.lower() == 'left':\n",
    "        my_idxs = lambda length: (0,seq_length-(seq_length-length)) \n",
    "    elif alignment.lower() == 'center':\n",
    "        my_idxs = lambda length: ((seq_length-length)//2,seq_length-((seq_length-length)-(seq_length-length)//2))\n",
    "    else:\n",
    "        print(alignment, 'is not a valid option for alignment')\n",
    "        pritn('options: right, left, center')\n",
    "        return None\n",
    "    \n",
    "    for i,review in enumerate(reviews_ints):\n",
    "        text = review[:seq_length]\n",
    "        (a,b) = my_idxs(len(text))\n",
    "        features[i,a:b] = text\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     1   1396      4  26721     32     52   9290      5    101    110\n",
      "   72456      6    101    110    979  13758     13    156     39     33\n",
      "    2711     11      8    495    922      4   1063     70    118    286\n",
      "      42   1703     39   5573      1  26721   1146     20      8   4314\n",
      "       6     34    691   9049     11      8   1006   1063     13    929\n",
      "       1   1146    121      8    334   6916      5    347    198      1\n",
      "     398  34120  12282     61     78      4      1  65662   3690   1477\n",
      "      38    390     35    188     15    193 400001  30411     42    914\n",
      "    3198      4     92 400001 400001  20908      4  26721   7477 149138\n",
      "      18   3258  12006      6      1   3206   6093   1063     13    842\n",
      "       5  15933   9051   1819  95253     15    872   4002   1396      1\n",
      "     631      4      8  38151    712   6810   1335   2530     61     50\n",
      "      45    337     22      8   5120      4  23573     39   1475    104\n",
      "      20     49      4   1001      6     39   2463   2371      4      1\n",
      "    7529    114      4     27   4741    474  23450    107    434     21\n",
      "     138      5    564      6   3655     27     69   3005    872     74\n",
      "       1   3519   2291 400001    508   3601    949  10492      6    508\n",
      "   24243     33     27   1614  22697]]\n",
      "[[    27   7272   8893      5   2200   3735    712    505    174   4764\n",
      "       5     70     39    347      1   1033     27   9550      4  12682\n",
      "      14  18355     27   2175  22048  34340     60      1     92    874\n",
      "     159     56   2154     34      7    862     33      1    312  26721\n",
      "       6      1    854     13     40    151     34      8    703    565\n",
      "    4918   4451  24368     15    817   1063     15    646      4      8\n",
      "    3625    118  25167   1396     19   6053    220    712      6     15\n",
      "   20807      6  38769      7    151   7221      7      8   2599      4\n",
      "  400001    180     35     19     15     85   2371 149138      6     64\n",
      "      15     85 400001  30411    144     38    320     15      7      8\n",
      "     246   2892      4  26721   1589      6     42    825     21    949\n",
      "       8   2892      4     49    114     21     15   9957     38   2892\n",
      "    2055      1   1396     20      8   1908     11      4     65    655\n",
      "    3339  14957   4918    335     22 122623    979   2891  11108   7982\n",
      "      15      7    638    863     11     72   5949      1 400001   1283\n",
      "      39     15  28294      1   7529    863     11    646     43     15\n",
      "     333   9488    414     72   5949     15   2838      5     34     52\n",
      "    1511     11      8    174     80]]\n"
     ]
    }
   ],
   "source": [
    "train_x = pad_features(train_reviews_int, seq_length=seq_length)\n",
    "valid_x = pad_features(valid_reviews_int, seq_length=seq_length)\n",
    "test_x = pad_features(test_reviews_int, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(train_x)==len(train_reviews_int), \"Your features should have as many rows as reviews.\"\n",
    "assert len(train_x[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# Print the first review\n",
    "print(train_x[:1,:seq_length//2])\n",
    "print(train_x[:1,seq_length//2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders and Batching\n",
    "\n",
    "After creating training, test, and validation data, we can create DataLoaders for batching our data into the net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x).long(), torch.from_numpy(train_y).long())\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_x).long(), torch.from_numpy(valid_y).long())\n",
    "test_data = TensorDataset(torch.from_numpy(test_x).long(), torch.from_numpy(test_y).long())\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader_50 = DataLoader(train_data, shuffle=True, batch_size=50)\n",
    "train_loader_100 = DataLoader(train_data, shuffle=True, batch_size=100)\n",
    "train_loader_150 = DataLoader(train_data, shuffle=True, batch_size=150)\n",
    "valid_loader = DataLoader(valid_data, shuffle=False, batch_size=100) #No need to shuffle\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=100) #No need to shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 350])\n",
      "Sample input: \n",
      " tensor([[     0,      0,      0,  ...,     16,    101,    103],\n",
      "        [     0,      0,      0,  ...,   1006,    662,    117],\n",
      "        [     0,      0,      0,  ...,      6,   6091,    979],\n",
      "        ...,\n",
      "        [     1,  15709, 105869,  ...,      4,     51,    197],\n",
      "        [     0,      0,      0,  ...,   6047,     38,   1006],\n",
      "        [     0,      0,      0,  ...,     33,     23,  11496]])\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n",
      "        0, 1])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader_50)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Sentiment Network with PyTorch\n",
    "\n",
    "We are going to use a Recurent Neural Network (RNN) to tackle the sentiment prediction problem. The layers are as follows:\n",
    "1. An embedding layer that converts our word tokens (integers) into embeddings of 300 dimensions. It's coefficients have been pretrained (GloVe embedding).\n",
    "2. An GRU cell defined by a hidden_state size and number of layers\n",
    "3. A fully-connected output layer that maps the GRU layer outputs to a desired output_size\n",
    "4. A sigmoid activation layer which turns all outputs into a value 0-1; it returns **only the last sigmoid output** as the output of this network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_glove(vectors):\n",
    "    vocab_size, embedding_dim = vectors.shape\n",
    "    emb_layer = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "    emb_layer.load_state_dict({'weight': vectors})\n",
    "    #Freeze parameters\n",
    "    emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, vocab_size, embedding_dim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, \n",
    "                 drop_prob=0.5, seq_length=None):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        \n",
    "        #keeps the hidden state\n",
    "        self.hidden = None\n",
    "        \n",
    "        # define all layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden=None, return_hidden=False):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        if hidden:\n",
    "            gru_out, hidden = self.gru(embeds, hidden)\n",
    "        else:\n",
    "            #Get hidden state all zeros\n",
    "            hidden = self.init_hidden(batch_size)\n",
    "            #Ensures hidden is in the same device that x is\n",
    "            hidden = x.new_tensor(data=hidden, dtype=hidden.dtype) \n",
    "            gru_out, hidden = self.gru(embeds, hidden)\n",
    "         # stack up gru outputs\n",
    "        gru_out = gru_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(gru_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        if return_hidden:\n",
    "            # return last sigmoid output and hidden state\n",
    "            return sig_out, hidden\n",
    "        \n",
    "        return sig_out\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        #weight = next(self.parameters()).data\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden\n",
    "    \n",
    "    def get_params(self):\n",
    "        params = {'vocab_size': self.vocab_size, 'output_size': self.output_size, 'hidden_dim': self.hidden_dim,\n",
    "                  'embedding_dim': self.embedding_dim, 'seq_length': self.seq_length, \n",
    "                  'drop_prob': self.drop_prob, 'n_layers': self.n_layers}\n",
    "        return params\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the network\n",
    "\n",
    "Here, we'll instantiate the network. First up, defining the hyperparameters.\n",
    "\n",
    "* `vocab_size`: Size of our vocabulary or the range of values for our input, word tokens.\n",
    "* `output_size`: Size of our desired output; the number of class scores we want to output (pos/neg).\n",
    "* `embedding_dim`: Number of columns in the embedding lookup table; size of our embeddings.\n",
    "* `hidden_dim`: Number of units in the hidden layers of our GRU cells.\n",
    "* `n_layers`: Number of GRU layers in the network. Typically between 1-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(400002, 300, padding_idx=0)\n",
       "  (gru): GRU(300, 128, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.2)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Created the embedding layer\n",
    "glove_layer, vocab_size, embedding_dim = build_glove(emb_vectors)\n",
    "# Instantiate the model w/ hyperparams\n",
    "net_params = {'vocab_size': vocab_size, 'output_size': 1, 'embedding_dim': embedding_dim, \n",
    "              'hidden_dim': 128,'n_layers': 2, 'drop_prob': 0.2}\n",
    "\n",
    "net = SentimentRNN(**net_params)\n",
    "#Include the GloVe embedding\n",
    "net.embedding = glove_layer\n",
    "\n",
    "#Save the encoding dictionary and the list of words\n",
    "net.words = words\n",
    "net.word_to_int = word_to_int\n",
    "#Move to gpu or cpu device\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training\n",
    "\n",
    "Let's train our net. We will use the Adam optimizer and we evaluate the model every few steps and after each epoch. We will save the best model based on the evaluation loss\n",
    "\n",
    "Training hyparameters:\n",
    "\n",
    "* `lr`: Learning rate for our optimizer.\n",
    "* `L2`: L2 regularization used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr = 0.001\n",
    "L2 = 0\n",
    "criterion = nn.BCELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=L2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function saves the model, optimizer and other parameters into a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(path=None, model=None, optimizer=None, params=None, \n",
    "                    epoch=None, train_loss=None, valid_loss=None, word_to_int=None, words=None):\n",
    "    if path:\n",
    "        my_path=path\n",
    "        print('Using', my_path, 'to save')\n",
    "    else:\n",
    "        my_path='my_model.pt'\n",
    "        print('Using', my_path, 'to save')\n",
    "        \n",
    "    checkpoint = {}\n",
    "    \n",
    "    if model:\n",
    "        checkpoint['model_state_dict']= model.state_dict()\n",
    "    else:\n",
    "        print('No model dictionary saved')\n",
    "    \n",
    "    if params:\n",
    "        checkpoint['params'] = params\n",
    "    else:\n",
    "        print('No model parameters saved')\n",
    "        \n",
    "    if optimizer:\n",
    "        checkpoint['optimizer_state_dict']= optimizer.state_dict()\n",
    "    else:\n",
    "        print('NNo optimizer dictionary saved')\n",
    "        \n",
    "    if epoch:\n",
    "        checkpoint['epoch'] = epoch\n",
    "    else:\n",
    "        print('No current epoch value saved')\n",
    "        \n",
    "    if train_loss:\n",
    "        checkpoint['train_loss'] = train_loss\n",
    "    else:\n",
    "        print('No value of the training loss saved')\n",
    "        \n",
    "    if valid_loss:\n",
    "        checkpoint['valid_loss'] = valid_loss\n",
    "    else:\n",
    "        print('No value of the validation loss saved')\n",
    "        \n",
    "    if word_to_int:\n",
    "        checkpoint['word_to_int'] = word_to_int\n",
    "    else:\n",
    "        print('No dictionary for encoding words saved')\n",
    "        \n",
    "    if words:\n",
    "        checkpoint['words'] = words\n",
    "    else:\n",
    "        print('No list of words saved')\n",
    "    \n",
    "    torch.save(checkpoint, my_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function loads the model, optimizer and other parameters into a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path=None, model=None, optimizer=None):\n",
    "    '''\n",
    "    It overrrides the model and optimizer provided with the saved parameters \n",
    "    and returns the saved epoch, train loss and validation loss'''\n",
    "    if path:\n",
    "        checkpoint = torch.load(path, map_location='cpu') \n",
    "    else:\n",
    "        print('Nothing loaded. Plese provide a file')\n",
    "        return None\n",
    "        \n",
    "    #Load the model state dictionary\n",
    "    my_dict = checkpoint.get('model_state_dict', None)\n",
    "    if my_dict:\n",
    "        model.load_state_dict(my_dict)\n",
    "        model.word_to_int = checkpoint.get('word_to_int', None)\n",
    "        model.words = checkpoint.get('words', None)\n",
    "    else:\n",
    "        print('No model dictionary found')\n",
    "    \n",
    "    #Load the optimizer state dictionary\n",
    "    my_dict = checkpoint.get('optimizer_state_dict', None)\n",
    "    if my_dict:\n",
    "        optimizer.load_state_dict(my_dict)\n",
    "    else:\n",
    "        print('No optimizer dictionary found')    \n",
    "    \n",
    "    #Load the epoch value, train loss and validation loss\n",
    "    epoch = checkpoint.get('epoch', None)\n",
    "    train_loss = checkpoint.get('train_loss', None)\n",
    "    valid_loss = checkpoint.get('valid_loss', None)\n",
    "    \n",
    "    return epoch, train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE THE MODEL HERE ###\n",
    "my_path = 'my_GloVe_RNN_0117.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing the number of intermediate evals at epoch 1\n",
      "225\n",
      "[112]\n",
      "Step: 112\n",
      "Epoch: 1/50... Train Loss: 0.581354... Val Loss: 0.462890\n",
      "Validation loss decreased (inf --> 0.462890).  Saving model ...\n",
      "Using my_GloVe_RNN_0117.pt to save\n",
      "Step: 225\n",
      "Epoch: 1/50... Train Loss: 0.529792... Val Loss: 0.380055\n",
      "Validation loss decreased (0.462890 --> 0.380055).  Saving model ...\n",
      "Using my_GloVe_RNN_0117.pt to save\n",
      "Changing the number of intermediate evals at epoch 2\n",
      "225\n",
      "[37, 75, 112, 150, 187]\n",
      "Step: 37\n",
      "Epoch: 2/50... Train Loss: 0.382210... Val Loss: 0.352657\n",
      "Validation loss decreased (0.380055 --> 0.352657).  Saving model ...\n",
      "Using my_GloVe_RNN_0117.pt to save\n",
      "Step: 75\n",
      "Epoch: 2/50... Train Loss: 0.368799... Val Loss: 0.327153\n",
      "Validation loss decreased (0.352657 --> 0.327153).  Saving model ...\n",
      "Using my_GloVe_RNN_0117.pt to save\n",
      "Step: 112\n",
      "Epoch: 2/50... Train Loss: 0.362414... Val Loss: 0.341172\n",
      "Step: 150\n",
      "Epoch: 2/50... Train Loss: 0.352227... Val Loss: 0.340052\n",
      "Step: 187\n",
      "Epoch: 2/50... Train Loss: 0.344687... Val Loss: 0.354819\n",
      "Step: 225\n",
      "Epoch: 2/50... Train Loss: 0.344597... Val Loss: 0.311358\n",
      "Validation loss decreased (0.327153 --> 0.311358).  Saving model ...\n",
      "Using my_GloVe_RNN_0117.pt to save\n",
      "Changing the number of intermediate evals at epoch 3\n",
      "225\n",
      "[22, 45, 67, 90, 112, 135, 157, 180, 202]\n",
      "Step: 22\n",
      "Epoch: 3/50... Train Loss: 0.309087... Val Loss: 0.295682\n",
      "Validation loss decreased (0.311358 --> 0.295682).  Saving model ...\n",
      "Using my_GloVe_RNN_0117.pt to save\n",
      "Step: 45\n",
      "Epoch: 3/50... Train Loss: 0.307606... Val Loss: 0.306674\n",
      "Step: 67\n",
      "Epoch: 3/50... Train Loss: 0.303776... Val Loss: 0.289481\n",
      "Validation loss decreased (0.295682 --> 0.289481).  Saving model ...\n",
      "Using my_GloVe_RNN_0117.pt to save\n",
      "Step: 90\n",
      "Epoch: 3/50... Train Loss: 0.297096... Val Loss: 0.290854\n",
      "Step: 112\n",
      "Epoch: 3/50... Train Loss: 0.295893... Val Loss: 0.307306\n",
      "Step: 135\n",
      "Epoch: 3/50... Train Loss: 0.295245... Val Loss: 0.303898\n",
      "Step: 157\n",
      "Epoch: 3/50... Train Loss: 0.291810... Val Loss: 0.284076\n",
      "Validation loss decreased (0.289481 --> 0.284076).  Saving model ...\n",
      "Using my_GloVe_RNN_0117.pt to save\n",
      "Step: 180\n",
      "Epoch: 3/50... Train Loss: 0.291067... Val Loss: 0.287520\n",
      "Step: 202\n",
      "Epoch: 3/50... Train Loss: 0.291468... Val Loss: 0.290702\n",
      "Step: 225\n",
      "Epoch: 3/50... Train Loss: 0.292668... Val Loss: 0.299963\n",
      "Step: 22\n",
      "Epoch: 4/50... Train Loss: 0.276135... Val Loss: 0.324487\n",
      "Step: 45\n",
      "Epoch: 4/50... Train Loss: 0.271695... Val Loss: 0.283081\n",
      "Validation loss decreased (0.284076 --> 0.283081).  Saving model ...\n",
      "Using my_GloVe_RNN_0117.pt to save\n",
      "Step: 67\n",
      "Epoch: 4/50... Train Loss: 0.266773... Val Loss: 0.282678\n",
      "Validation loss decreased (0.283081 --> 0.282678).  Saving model ...\n",
      "Using my_GloVe_RNN_0117.pt to save\n",
      "Step: 90\n",
      "Epoch: 4/50... Train Loss: 0.267318... Val Loss: 0.279006\n",
      "Validation loss decreased (0.282678 --> 0.279006).  Saving model ...\n",
      "Using my_GloVe_RNN_0117.pt to save\n",
      "Step: 112\n",
      "Epoch: 4/50... Train Loss: 0.270340... Val Loss: 0.277931\n",
      "Validation loss decreased (0.279006 --> 0.277931).  Saving model ...\n",
      "Using my_GloVe_RNN_0117.pt to save\n",
      "Step: 135\n",
      "Epoch: 4/50... Train Loss: 0.268498... Val Loss: 0.281664\n",
      "Step: 157\n",
      "Epoch: 4/50... Train Loss: 0.267891... Val Loss: 0.282626\n",
      "Step: 180\n",
      "Epoch: 4/50... Train Loss: 0.269053... Val Loss: 0.277162\n",
      "Validation loss decreased (0.277931 --> 0.277162).  Saving model ...\n",
      "Using my_GloVe_RNN_0117.pt to save\n",
      "Step: 202\n",
      "Epoch: 4/50... Train Loss: 0.269661... Val Loss: 0.266347\n",
      "Validation loss decreased (0.277162 --> 0.266347).  Saving model ...\n",
      "Using my_GloVe_RNN_0117.pt to save\n",
      "Step: 225\n",
      "Epoch: 4/50... Train Loss: 0.268447... Val Loss: 0.275722\n",
      "Step: 22\n",
      "Epoch: 5/50... Train Loss: 0.235849... Val Loss: 0.273371\n",
      "Step: 45\n",
      "Epoch: 5/50... Train Loss: 0.231721... Val Loss: 0.328135\n",
      "Step: 67\n",
      "Epoch: 5/50... Train Loss: 0.233206... Val Loss: 0.277407\n",
      "Step: 90\n",
      "Epoch: 5/50... Train Loss: 0.235888... Val Loss: 0.301778\n",
      "Step: 112\n",
      "Epoch: 5/50... Train Loss: 0.240066... Val Loss: 0.288242\n",
      "Step: 135\n",
      "Epoch: 5/50... Train Loss: 0.242430... Val Loss: 0.301078\n",
      "Step: 157\n",
      "Epoch: 5/50... Train Loss: 0.241777... Val Loss: 0.264446\n",
      "Validation loss decreased (0.266347 --> 0.264446).  Saving model ...\n",
      "Using my_GloVe_RNN_0117.pt to save\n",
      "Step: 180\n",
      "Epoch: 5/50... Train Loss: 0.239959... Val Loss: 0.260248\n",
      "Validation loss decreased (0.264446 --> 0.260248).  Saving model ...\n",
      "Using my_GloVe_RNN_0117.pt to save\n",
      "Step: 202\n",
      "Epoch: 5/50... Train Loss: 0.239558... Val Loss: 0.287890\n",
      "Step: 225\n",
      "Epoch: 5/50... Train Loss: 0.242682... Val Loss: 0.284803\n",
      "Step: 22\n",
      "Epoch: 6/50... Train Loss: 0.243456... Val Loss: 0.283059\n",
      "Step: 45\n",
      "Epoch: 6/50... Train Loss: 0.223675... Val Loss: 0.272950\n",
      "Step: 67\n",
      "Epoch: 6/50... Train Loss: 0.221823... Val Loss: 0.279188\n",
      "Step: 90\n",
      "Epoch: 6/50... Train Loss: 0.223752... Val Loss: 0.264214\n",
      "Step: 112\n",
      "Epoch: 6/50... Train Loss: 0.221288... Val Loss: 0.271491\n",
      "Step: 135\n",
      "Epoch: 6/50... Train Loss: 0.223283... Val Loss: 0.277156\n",
      "Step: 157\n",
      "Epoch: 6/50... Train Loss: 0.219745... Val Loss: 0.272975\n",
      "Step: 180\n",
      "Epoch: 6/50... Train Loss: 0.219596... Val Loss: 0.267544\n",
      "Step: 202\n",
      "Epoch: 6/50... Train Loss: 0.217520... Val Loss: 0.263437\n",
      "Step: 225\n",
      "Epoch: 6/50... Train Loss: 0.216637... Val Loss: 0.276383\n",
      "Step: 22\n",
      "Epoch: 7/50... Train Loss: 0.190222... Val Loss: 0.306637\n",
      "Step: 45\n",
      "Epoch: 7/50... Train Loss: 0.199550... Val Loss: 0.311555\n",
      "Step: 67\n",
      "Epoch: 7/50... Train Loss: 0.205598... Val Loss: 0.271878\n",
      "Step: 90\n",
      "Epoch: 7/50... Train Loss: 0.205154... Val Loss: 0.311052\n",
      "Step: 112\n",
      "Epoch: 7/50... Train Loss: 0.204695... Val Loss: 0.295614\n",
      "Step: 135\n",
      "Epoch: 7/50... Train Loss: 0.200039... Val Loss: 0.319225\n",
      "Step: 157\n",
      "Epoch: 7/50... Train Loss: 0.200965... Val Loss: 0.281222\n",
      "Step: 180\n",
      "Epoch: 7/50... Train Loss: 0.200479... Val Loss: 0.283493\n",
      "Step: 202\n",
      "Epoch: 7/50... Train Loss: 0.199066... Val Loss: 0.274924\n",
      "Step: 225\n",
      "Epoch: 7/50... Train Loss: 0.199807... Val Loss: 0.280396\n",
      "Step: 22\n",
      "Epoch: 8/50... Train Loss: 0.165250... Val Loss: 0.298106\n",
      "Step: 45\n",
      "Epoch: 8/50... Train Loss: 0.152944... Val Loss: 0.293311\n",
      "Step: 67\n",
      "Epoch: 8/50... Train Loss: 0.155596... Val Loss: 0.300387\n",
      "Step: 90\n",
      "Epoch: 8/50... Train Loss: 0.163714... Val Loss: 0.285564\n",
      "Step: 112\n",
      "Epoch: 8/50... Train Loss: 0.163739... Val Loss: 0.304496\n",
      "Step: 135\n",
      "Epoch: 8/50... Train Loss: 0.163883... Val Loss: 0.300768\n",
      "Step: 157\n",
      "Epoch: 8/50... Train Loss: 0.164280... Val Loss: 0.296427\n",
      "Step: 180\n",
      "Epoch: 8/50... Train Loss: 0.164397... Val Loss: 0.320971\n",
      "Step: 202\n",
      "Epoch: 8/50... Train Loss: 0.165486... Val Loss: 0.329218\n",
      "Step: 225\n",
      "Epoch: 8/50... Train Loss: 0.167268... Val Loss: 0.281632\n",
      "Stopping optimization... DONE!\n",
      "...DONE\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "\n",
    "epochs = 50 \n",
    "patience=31\n",
    "missteps = 0\n",
    "init_epoch = 1\n",
    "\n",
    "train_loader = train_loader_100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "eval_schedule = [1,2,3]\n",
    "inter_evals_list = [1,5,9]\n",
    "inter_evals = 9\n",
    "eval_list = [int(len(train_loader)/(inter_evals+1)*ii) for ii in range(1,inter_evals+1)]\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "# train for some number of epochs\n",
    "for e in range(init_epoch, init_epoch+epochs):\n",
    "    if missteps > patience:\n",
    "        print('...DONE')\n",
    "        break\n",
    "    if e in eval_schedule:\n",
    "        print('Changing the number of intermediate evals at epoch',e)\n",
    "        idx = eval_schedule.index(e)\n",
    "        #train_loader = loader_list[idx]\n",
    "        inter_evals = inter_evals_list[idx]\n",
    "        print(len(train_loader))\n",
    "        eval_list = [int(len(train_loader)/(inter_evals+1)*ii) for ii in range(1,inter_evals+1)]\n",
    "        print(eval_list)\n",
    "        \n",
    "    net.train()\n",
    "    # batch loop\n",
    "    train_loss = 0.0\n",
    "    train_size = 0\n",
    "    step = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        net.train()\n",
    "        step+=1\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        #Number of items in the batch\n",
    "        train_size += inputs.size(0)\n",
    "        '''# initialize hidden state\n",
    "        h = net.init_hidden(inputs.size(0)).to(device)'''\n",
    "        \n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output and hidden state from the model\n",
    "        #output, h = net(inputs, h)\n",
    "        output = net(inputs)\n",
    "        \n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        if step in eval_list + [len(train_loader)]:\n",
    "            print('Step:', step)\n",
    "            \n",
    "            # Get validation loss\n",
    "            valid_loss = 0.0\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "                # move tensors to GPU if CUDA is available\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                '''# initialize hidden state\n",
    "                val_h = net.init_hidden(inputs.size(0)).to(device)'''\n",
    "                #Compute validation loss\n",
    "                #output, val_h = net(inputs, val_h)\n",
    "                output = net(inputs)\n",
    "                valid_loss += criterion(output.squeeze(), labels.float()).item()\n",
    "\n",
    "            # Print results\n",
    "            print(\"Epoch: {}/{}...\".format(e, epochs),\n",
    "                  \"Train Loss: {:.6f}...\".format(train_loss/train_size),\n",
    "                  \"Val Loss: {:.6f}\".format(valid_loss/len(valid_loader.dataset)))\n",
    "            if valid_loss <= valid_loss_min:\n",
    "                missteps = 0  \n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                valid_loss_min/len(valid_loader.dataset),\n",
    "                valid_loss/len(valid_loader.dataset)))\n",
    "                save_checkpoint(my_path, net, optimizer, epoch=e, train_loss=train_loss, valid_loss=valid_loss,\n",
    "                                words=net.words, word_to_int= net.word_to_int, params=net.get_params())\n",
    "                valid_loss_min = valid_loss\n",
    "            else:\n",
    "                missteps+=1\n",
    "                if missteps > patience:\n",
    "                    print('Stopping optimization... DONE!')\n",
    "                    break\n",
    "        \n",
    "else:\n",
    "    print('DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(400002, 300, padding_idx=0)\n",
       "  (gru): GRU(300, 128, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.2)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch, train_loss, valid_loss = load_checkpoint(my_path, net, optimizer)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Testing\n",
    "\n",
    "We will test or neural network in two ways\n",
    "\n",
    "* **Test data performance:** First, we'll see how our trained model performs on all of our defined test_data, above. We'll calculate the average loss and accuracy over the test data, and plot some parameters to assess the model.\n",
    "\n",
    "* **Inference on user-generated data:** Second, we'll see if we can input just one example review at a time (without a label), and see what the trained model predicts and if the results are the ones expected based on the sentiment we infer from the used sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.2625\n",
      "Test accuracy: 0.8906\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_loss = 0.0 # track loss\n",
    "num_correct = 0\n",
    "test_probas = []\n",
    "net.eval()\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    '''# init hidden state\n",
    "    h = net.init_hidden(inputs.size(0)).to(device)'''\n",
    "\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output = net(inputs)\n",
    "    test_probas.extend(output.squeeze().detach().cpu().numpy())\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss += criterion(output.squeeze(), labels.float()).item()\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "# Convert test_probas to numpy array\n",
    "test_probas = np.array(test_probas)\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.4f}\".format(test_loss/len(test_loader.dataset)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's save the predictions annd the test lables\n",
    "with open('GloVe_GRU_probas.pkl','wb') as f:\n",
    "    pickle.dump(test_probas, f)\n",
    "with open('test_y.pkl','wb') as f:\n",
    "    pickle.dump(test_y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred_1</th>\n",
       "      <th>Pred_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Label_1</th>\n",
       "      <td>11152</td>\n",
       "      <td>1348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label_0</th>\n",
       "      <td>1387</td>\n",
       "      <td>11113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Pred_1  Pred_0\n",
       "Label_1   11152    1348\n",
       "Label_0    1387   11113"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cm = pd.DataFrame(cm(test_y, test_probas>=0.5, labels=[1,0]), index=['Label_1', 'Label_0'], columns=['Pred_1','Pred_0'] )\n",
    "test_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems almost equally good at identifying positive and negative reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's provide some metrics for the classification focused on the possitive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 0.8906\n",
      "Precision of the model for positive reviews: 0.8894\n",
      "Recall of the model for positive reviews: 0.8922\n",
      "f1-score of the model for positive reviews: 0.8908\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy of the model: {accuracy_score(test_y, test_probas>=0.5):.4f}')\n",
    "print(f'Precision of the model for positive reviews: {precision_score(test_y, test_probas>=0.5, pos_label=1):.4f}')\n",
    "print(f'Recall of the model for positive reviews: {recall_score(test_y, test_probas>=0.5, pos_label=1):.4f}')\n",
    "print(f'f1-score of the model for positive reviews: {f1_score(test_y, test_probas>=0.5, pos_label=1):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's provide some metrics for the classification focused on the negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 0.8906\n",
      "Precision of the model for negative reviews: 0.8918\n",
      "Recall of the model for negative reviews: 0.8890\n",
      "f1-score of the model for negative reviews: 0.8904\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy of the model: {accuracy_score(test_y, test_probas>=0.5):.4f}')\n",
    "print(f'Precision of the model for negative reviews: {precision_score(test_y, test_probas>=0.5, pos_label=0):.4f}')\n",
    "print(f'Recall of the model for negative reviews: {recall_score(test_y, test_probas>=0.5, pos_label=0):.4f}')\n",
    "print(f'f1-score of the model for negative reviews: {f1_score(test_y, test_probas>=0.5, pos_label=0):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the precision-recall curves for positive and negative reviews look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positive reviews\n",
    "pos_pre_rec = pd.DataFrame({name: values for values, name in \n",
    "                            zip(precision_recall_curve(test_y, test_probas, pos_label=1)[:2],['precision','recall'])})\n",
    "#Negative reviews\n",
    "neg_pre_rec = pd.DataFrame({name: values for values, name in \n",
    "                            zip(precision_recall_curve(test_y, 1-test_probas, pos_label=0)[:2],['precision','recall'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(num=1,figsize=(10,5))\n",
    "#Positive reviews\n",
    "ax = plt.subplot(1,2,1) \n",
    "sns.lineplot(x='recall',y='precision',data=pos_pre_rec,ax=ax,color='green', alpha=0.5)\n",
    "plt.fill_between(pos_pre_rec.recall.values, pos_pre_rec.precision.values, color='green', alpha=0.3)\n",
    "plt.title('Positive Class')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "#Negative reviews\n",
    "ax = plt.subplot(1,2,2) \n",
    "sns.lineplot(x='recall',y='precision',data=neg_pre_rec,ax=ax,color='red', alpha=0.5)\n",
    "plt.fill_between(neg_pre_rec.recall.values, neg_pre_rec.precision.values, color='red', alpha=0.3)\n",
    "plt.title('Negative Class')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check how the average probability of being positive and the percentage of predicted positive reviews varies \n",
    "with the number of stars of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stars</th>\n",
       "      <th>Avg Prob</th>\n",
       "      <th>Pos Revs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.095203</td>\n",
       "      <td>5.814417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.132628</td>\n",
       "      <td>8.427454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.181406</td>\n",
       "      <td>13.419913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.256082</td>\n",
       "      <td>21.252372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.767350</td>\n",
       "      <td>80.234070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.844414</td>\n",
       "      <td>88.350877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>0.878519</td>\n",
       "      <td>91.808874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>0.890121</td>\n",
       "      <td>92.638528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stars  Avg Prob   Pos Revs\n",
       "0      1  0.095203   5.814417\n",
       "1      2  0.132628   8.427454\n",
       "2      3  0.181406  13.419913\n",
       "3      4  0.256082  21.252372\n",
       "4      7  0.767350  80.234070\n",
       "5      8  0.844414  88.350877\n",
       "6      9  0.878519  91.808874\n",
       "7     10  0.890121  92.638528"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stars = np.unique(test_stars)\n",
    "stars.sort()\n",
    "my_df={'Stars': stars, \n",
    "       'Avg Prob':[test_probas[test_stars==i].mean() for i in stars],\n",
    "      'Pos Revs': [(test_probas[test_stars==i]>=0.5).mean()*100 for i in stars]}\n",
    "\n",
    "my_df = pd.DataFrame(my_df)\n",
    "my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(num=2,figsize=(5,5))\n",
    "sns.barplot(x='Stars',y='Avg Prob',data=my_df)\n",
    "plt.title('Average Probability of Being Positive')\n",
    "plt.ylim(0,1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the higher the stars awarded to the review the higher the probability of it being positive based on our model (at least on average) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(num=3,figsize=(5,5))\n",
    "sns.barplot(x='Stars',y='Pos Revs',data=my_df)\n",
    "plt.title('Percentage of Predicted Positive Reviews')\n",
    "plt.ylim(0,100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the higher the stars awarded to the review the higher the probability of it being positive based on our model (at least on average) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on a test review\n",
    "\n",
    "Let's write a predict function which will output if a provided review is positive or negative, as well as the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, test_review, sequence_length=200, alignment='right'):\n",
    "    ''' Prints out whether a give review is predicted to be \n",
    "        positive or negative in sentiment, using a trained model.\n",
    "        \n",
    "        params:\n",
    "        net - A trained net \n",
    "        test_review - a review made of normal text and punctuation\n",
    "        sequence_length - the padded length of a review\n",
    "        '''\n",
    "    #Default sequence aligment: the one provided by the model if any\n",
    "    if sequence_length is None:\n",
    "        sequence_length = net.seq_length    \n",
    "    #get lower case review\n",
    "    test_review = test_review.lower()\n",
    "    #remove some contractions\n",
    "    test_review = remove_contractions(test_review)\n",
    "    #remove punctuation\n",
    "    test_review = ''.join([c for c in test_review if c not in punctuation])\n",
    "    #split in words\n",
    "    test_review = test_review.split()\n",
    "    #Encode the words\n",
    "    unk = net.word_to_int.get('<unk>')\n",
    "    test_review = [[net.word_to_int.get(word,unk) for word in test_review[:sequence_length]]]\n",
    "    #Padding\n",
    "    test_review = pad_features(test_review, sequence_length, alignment=alignment)\n",
    "    #Convert into pytorch tensor\n",
    "    test_review = torch.from_numpy(test_review).type(torch.LongTensor).to(device)\n",
    "    \n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    pred = net(test_review).squeeze().item()\n",
    "    if pred >= 0.5:\n",
    "        pred = ('Positive', pred)\n",
    "    else:\n",
    "        pred = ('Negative', pred)\n",
    "    return pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative test review\n",
    "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive test review\n",
    "test_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Negative', 0.005146815907210112)\n",
      "('Positive', 0.977432906627655)\n"
     ]
    }
   ],
   "source": [
    "# call function for positive and negative reviews\n",
    "print(predict(net, test_review_neg))\n",
    "print(predict(net, test_review_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dependency of the Predictions on the Length of the Sequence\n",
    "\n",
    "Although we have used a seqyence length of 350 words, our RNN  can actually use sequences of diffetrent lengths without modifying the net. Out of curiosity, we will test how the size of the sequences modifies the predictions for the longest misclassified review and for the longest review correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load our best model\n",
    "epoch, train_loss, valid_loss = load_checkpoint(my_path, net, optimizer)\n",
    "net.to(device)\n",
    "#Load test_probas\n",
    "with open('GloVe_GRU_probas.pkl','rb') as f:\n",
    "    test_probas = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the corretly classified review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the review: 2328\n",
      "Stars: 10\tLabel: 1\tPrediction:0.9749\n"
     ]
    }
   ],
   "source": [
    "test_goodies_mask = ((test_y) == (test_probas>0.5))\n",
    "test_goodies_idx = np.arange(0,len(test_y),dtype=np.int)[test_goodies_mask]\n",
    "test_goodies_lens = np.array([len(test_reviews_int[idx]) for idx in test_goodies_idx])\n",
    "argmax = test_goodies_lens.argmax()\n",
    "my_idx = test_goodies_idx[argmax]\n",
    "print(f\"Length of the review: {len(test_reviews_int[my_idx])}\")\n",
    "print(f\"Stars: {test_stars[my_idx]}\\tLabel: {test_y[my_idx]}\\tPrediction:{test_probas[my_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The review:\n",
      "There's a sign on The Lost Highway that says:<br /><br />*MAJOR SPOILERS AHEAD*<br /><br />(but you already knew that, didn't you?)<br /><br />Since there's a great deal of people that apparently did not get the point of this movie, I'd like to contribute my interpretation of why the plot makes perfect sense. As others have pointed out, one single viewing of this movie is not sufficient. If you have the DVD of MD, you can \"cheat\" by looking at David Lynch's \"Top 10 Hints to Unlocking MD\" (but only upon second or third viewing, please.) ;)<br /><br />First of all, Mulholland Drive is downright brilliant. A masterpiece. This is the kind of movie that refuse to leave your head. Not often are the comments on the DVDs very accurate, but Vogue's \"It gets inside your head and stays there\" really hit the mark.<br /><br />David Lynch deserves praise for creating a movie that not only has a beautifully stylish look to it - cinematography-wise, has great acting (esp. Naomi Watts), a haunting soundtrack by Badalamenti, and a very dream-like quality to it -- but on top of it all it also manages to involve the viewer in such a way that few movies have before. (After all, when is the last time you saw a movie that just wouldn't leave your mind and that everyone felt compelled to talk and write about, regardless of whether they liked it or hated it?)<br /><br />Allright, enough about all that, it's time to justify those statements.<br /><br />Most people that have gone through some effort to try to piece the plot together will have come to the conclusion that the first half of the picture is an illusion/a dream sequence.<br /><br />Of course, that's too bad for all those trying to make sense of the movie by expecting \"traditional\" methods in which the story is laid out in a timely, logic and linear manner for the viewer. But for those expecting that, I urge you to check the name of the director and come back again. ;)<br /><br />MD is the story of the sad demise of Diane Selwyn, a wannabe-actor who is hopelessly in love with another actor, Camilla Rowles. Due to Diane's lack of talent, she is constantly struggling to advance her career, and feels she failed to deliver on her own and her parents' expectations. Upon realizing that Camilla will never be hers (C. becomes engaged with Adam Kesher, the director), she hires a hitman to get rid of her, and subsequently has to deal with the guilt that it produces.<br /><br />The movie first starts off with what may seem as a strange opening for this kind of thriller; which is some 50s dance/jitterbug contest, in which we can see the main character Betty giving a great performance. We also see an elderly couple (which we will see twice more throughout the movie) together with her, and applauding her.<br /><br />No, wait. This is what most people see the first time they view it. There's actually another very significant fact that is given before the credits - the camera moving into an object (although blurry) and the scene quickly fading out. If you look closely, the object is actually a pillow, revealing that what follows is a dream.<br /><br />The main characters seen in the first half of the movie:<br /><br />Betty: Diane Selwyn's imaginary self, used in the first half of the movie that constitutes the \"dream-sequence\" - a positive portrayal of a successful, aspiring young actor (the complete opposite of Diane). 'Betty' was chosen as the name as that is the real name of the waitress at Winkies. Notice that in the dream version, the waitresses' name is 'Diane'.<br /><br />Rita: The fantasy version of Camilla Rhodes that, through Diane's dream, and with the help of an imaginary car-accident, is turned into an amnesiac. This makes her vulnerable and dependent on Diane's love. She is then conveniently placed in Betty/Diane's aunt's luxurious home which Betty has been allowed to stay in.<br /><br />Coco: In real life, Adam's mother. In the dream part, the woman in charge of the apartment complex that Betty stays in. She's mainly a strong authority figure, as can be witnessed in both parts of the film.<br /><br />Adam: The director. We know from the second half that he gets engaged with Camilla. His sole purpose for being in the first half of the movie is only to serve as a punching bag for Betty/Diane, since she develops such hatred towards him.<br /><br />Aunt Ruth: Diane's real aunt, but instead of being out of town, she is actually dead. Diane inherited the money left by her aunt and used that to pay for Camilla's murder.<br /><br />Mr. Roach: A typical Lynchian character. Not real; appears only in Diane's dream sequence. He's a mysterious, influential person that controls the chain of events in the dream from his wheelchair. He serves much of the same function as the backwards-talking dwarf (which he also plays) in Twin Peaks.<br /><br />The hitman: The person that murders Camilla. This character is basically the same in both parts of the movie, although rendered in a slightly more goofy fashion in the dream sequence (more on that below).<br /><br />Now, having established the various versions of the characters in the movie, we can begin to delve into the plot. Of course I will not go into every little detail (neither will I lay it out chronologically), but I will try to explain some of the important scenes, in relation to Lynch' \"hint-sheet\".<br /><br />As I mentioned above, Camilla was re-produced as an amnesiac through her improbable survival of a car-accident in the first 10 minutes of the movie, which left her completely vulnerable. What I found very intriguing with MD, is that Lynch constantly gives hints on what is real and what isn't. I've already mentioned the camera moving into the pillow, but notice how there's two cars riding in each lane approaching the limo.<br /><br />Only one of the cars actually hit the limo; what about the other? Even if they stayed clear of the accident themselves, wouldn't they try to help the others, or at least call for help? My theory is that, since this is a dream, the presence of the other car is just set aside, and forgotten about. Since, as Rogert Ebert so eloquently puts it \"Like real dreams, it does not explain, does not complete its sequences, lingers over what it finds fascinating, dismisses unpromising plotlines.\"<br /><br />Shortly after Rita crawls down from the crash site at Mulholland Dr., and makes her way down the hillside and sneaks into Aunt Ruth's apartment, Betty arrives and we see this creepy old couple driving away, staring ghoulishly at each other and grinning at themselves and the camera. This is the first indication that what we're seeing is a nightmare.<br /><br />Although the old couple seem to be unfamiliar to Betty, I think they're actually her parents (since they were applauding her at the jitterbug contest). Perhaps she didn't know them all that well, and didn't really have as good a relationship with them as she wanted, so the couple is shown as very pleasant and helpful to her in the dream. They also represent her feelings of guilt from the murder, and Diane's sense of unfulfillment regarding her unachieved goals in her life.<br /><br />A rather long and hilarious scene is the one involving the hitman. Diane apparently sees him as the major force behind the campaign trying to pressure the director to accept Camilla's part in the movie (from Adam's party in the second half of the movie), and he therefore occupies a major part of her dream. Because of her feelings of guilt and remorse towards the murder of Camilla, a part of her wants him to miss, so she turns him into a dumb criminal.<br /><br />This scene, I think, is also Lynch's attempt at totally screwing his audience over, since they're given a false pretence in which to view the movie.<br /><br />Gotta love that 'Something just bit me bad' line, though. :)<br /><br />The next interesting scene is the one with the two persons at Twinkies, who are having a conversation about how one of them keep having this recurring nightmare involving a man which is seen by him through a wall outside of the diner that they're sitting in. After a little talk, they head outside and keep walking toward the corner of a fence, accompanied of course by excellent music matching the mood of the scene.<br /><br />When reaching the corner, a bum-like character with a disfigured face appears out from behind the corner, scaring the living crap out of the man having the nightmare. This nightmare exists only in Diane's mind; she saw that guy in the diner when paying for the murder. So, in short, her obessions translate into that poor guy's nightmares. The bum also signifies Diane's evil side, as can be witnessed later in the movie.<br /><br />The Cowboy constitutes (along with the dwarf) one of the strange characters that are always present in the Lynchian landscape -- Diane only saw him for a short while at Adam's party, but just like our own dreams can award insignificant persons that we hardly know a major part in our dreams, so can he be awarded an important part in her dream. We are also given further clues during his scenes that what we're seeing is not real (his sudden disappearance, etc.)<br /><br />The Cowboy is also used as a tool to mock the Director, when he meets up with him at the odd location (the lights here give a clear indication that this is part of a dream). Also notice how he says that he will appear one more time if he (Adam) does good, or two more times if he does bad. Throughout the movie he appears two more times, indicating to Diane that she did bad. He is also the one to wake her up to reality (that scene is probably an illusion made to fit into her requirements of him appearing twice), and shortly thereafter she commits suicide.<br /><br />The espresso-scene with the Castigliane brothers (where we can see Badalamenti, the composer, as Luigi) is probably a result of the fact that Diane was having an espresso just before Camilla and Adam made their announcement at Adam's party in the second half. It could at the same time also be a statement from Lynch.<br /><br />During the scene in which they enter Diane's apartment, the body lying in the bed is Camilla, but notice how she's assumed Diane's sleeping position; Diane is seeing herself in her own dream, but the face is not hers, although it had the same wounds on the face as Diane would have after shooting herself. This scene is also filled with some genuine Lynchian creepiness. Since Diane did not know where (or when) the hitman would get to Camilla and finish her off, she just put her into her own home.<br /><br />In real life, Diane's audition for the movie part was bad. In her dream, she delivers a perfect audition - leaving the whole crew ecstatic about her performance.<br /><br />Also interesting is the fact that the money that in real-life was used to pay for Camilla's murder now appears in Rita/Camilla's purse. This is part of Diane's undoing of her terrible act by effectively being given the money back, as the murder now hasn't taken place.<br /><br />When her neighbor arrives to get her piano-shaped ashtray, another hint is given; she takes the ashtray from her table and leaves, yet later when Camilla and Betty have their encounter on the couch, we see the ashtray appear again when the camera pans over the table, suggesting that Betty's encounter with the neighbor was a fantasy.<br /><br />The catch phrase of the movie Adam is auditioning actresses for is \"She is the girl\"; which are the exact same words that Diane uses when giving the hitman Camilla's photo resume.<br /><br />The blue box and the key represent the major turning point in the movie, and is where the true identities of the characters are revealed. There's much symbolism going on here; the box may represent Diane's future (it's empty), or it may be a sort of a Pandora's box (the hitman laughs when she asks him what the key will open). Either way, it is connected to the murder by means of the blue key (which is placed next to her after the murder has taken place). The box is also seen at the end of the movie in the hands of the disfigured bum.<br /><br />Club Silencio is a neat little addition to further remind the viewer that what s/he is viewing is not real. It also signifies that Diane is about to wake up to her reality (her reality being a nightmare that she is unable to escape from, even in her dreams).<br /><br />During the chilling scene at the end where the creepy old couple reappear, Diane is tormented in such a way that she sees suicide as the only way out in order to escape the screams and to avoid being haunted by her fears.<br /><br />Anyway, that is my $0.02. Hope this could help people from bashing out at this movie and calling it 'the worst movie ever' or something to that effect, without realizing the plot.<br /><br />As usual, Lynch is all about creating irrational fears, and he certainly achieves that with this picture as well.<br /><br />10 out of 10.\n"
     ]
    }
   ],
   "source": [
    "print('The review:')\n",
    "with open(test_pos_dir+test_pos_rev_files[my_idx], 'r',encoding=\"utf8\") as f:\n",
    "    my_review=f.read()\n",
    "    print(my_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:200\t('Positive', 0.9933426976203918)\n",
      "Length:250\t('Positive', 0.9831362366676331)\n",
      "Length:300\t('Positive', 0.9500445127487183)\n",
      "Length:350\t('Positive', 0.9749488234519958)\n",
      "Length:400\t('Positive', 0.9154666066169739)\n",
      "Length:450\t('Positive', 0.8916581273078918)\n",
      "Length:500\t('Positive', 0.9290438294410706)\n",
      "Length:550\t('Positive', 0.9631395936012268)\n",
      "Length:600\t('Positive', 0.9144543409347534)\n",
      "Length:650\t('Positive', 0.963345468044281)\n",
      "Length:700\t('Positive', 0.9733172655105591)\n",
      "Length:750\t('Positive', 0.9343097805976868)\n",
      "Length:800\t('Positive', 0.9166458249092102)\n",
      "Length:850\t('Positive', 0.9519183039665222)\n",
      "Length:900\t('Positive', 0.963681697845459)\n",
      "Length:950\t('Positive', 0.9747524857521057)\n",
      "Length:1000\t('Positive', 0.9775786399841309)\n",
      "Length:1050\t('Positive', 0.971552312374115)\n",
      "Length:1100\t('Positive', 0.9829080104827881)\n",
      "Length:1150\t('Positive', 0.9704629778862)\n",
      "Length:1200\t('Positive', 0.9230346083641052)\n",
      "Length:1250\t('Positive', 0.982987642288208)\n",
      "Length:1300\t('Positive', 0.9733783602714539)\n",
      "Length:1350\t('Positive', 0.9806826114654541)\n",
      "Length:1400\t('Positive', 0.9493199586868286)\n",
      "Length:1450\t('Positive', 0.9164522290229797)\n",
      "Length:1500\t('Positive', 0.7341063022613525)\n",
      "Length:1550\t('Positive', 0.9177587032318115)\n",
      "Length:1600\t('Positive', 0.9184864163398743)\n",
      "Length:1650\t('Positive', 0.8389952778816223)\n",
      "Length:1700\t('Positive', 0.9314764738082886)\n",
      "Length:1750\t('Positive', 0.9059884548187256)\n",
      "Length:1800\t('Positive', 0.7587016820907593)\n",
      "Length:1850\t('Positive', 0.8749746680259705)\n",
      "Length:1900\t('Positive', 0.9250564575195312)\n",
      "Length:1950\t('Positive', 0.9145045876502991)\n",
      "Length:2000\t('Positive', 0.9153710603713989)\n",
      "Length:2050\t('Positive', 0.8152310848236084)\n",
      "Length:2100\t('Positive', 0.8375375866889954)\n",
      "Length:2150\t('Positive', 0.9008423686027527)\n",
      "Length:2200\t('Positive', 0.962159276008606)\n",
      "Length:2250\t('Positive', 0.9356907606124878)\n",
      "Length:2300\t('Negative', 0.46198779344558716)\n"
     ]
    }
   ],
   "source": [
    "for sl in range(200,len(test_reviews_int[my_idx]),50):\n",
    "    print(f\"Length:{sl}\\t{predict(net, my_review, sequence_length=sl)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we exclude the result with 2300 words, the network always predicts a possitive sentiment (the correct result) without much variation of the probability(0.73-0.95). It is very interesting how the probability drops to 0.46 for the las length considered, misclasifying the review as negative. That maybe due to this sentence:\n",
    "*\"Hope this could help people from bashing out at this movie and calling it **'the worst movie ever'** or something to that effect\"*\n",
    "It does not get much more negative than \"the worst movie ever\". \n",
    "\n",
    "Two factors are complicating the prediction:\n",
    "\n",
    "1) The net was trained using sequences of 350 words. So it probably is more forgetful than it should when dealing with a 2300 word sequences and does not propely remember all the positive things said before *\"the worst movie ever\"*.\n",
    "\n",
    "2) Although the test is between quotations and *'the worst movie ever'* could potentialy provide a different sentiment than *the worst movie ever*, we deleted the punctuation as part of our data preparation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it affects the misclassified review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the review: 2147\n",
      "Stars: 7\tLabel: 1\tPrediction:0.0122\n"
     ]
    }
   ],
   "source": [
    "test_error_mask = ((test_y) == (test_probas<0.5))\n",
    "test_erorr_idx = np.arange(0,len(test_y),dtype=np.int)[test_error_mask]\n",
    "test_error_lens = np.array([len(test_reviews_int[idx]) for idx in test_erorr_idx])\n",
    "argmax = test_error_lens.argmax()\n",
    "my_idx = test_erorr_idx[argmax]\n",
    "print(f\"Length of the review: {len(test_reviews_int[my_idx])}\")\n",
    "print(f\"Stars: {test_stars[my_idx]}\\tLabel: {test_y[my_idx]}\\tPrediction:{test_probas[my_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The review:\n",
      "(Some spoilers included:)<br /><br />Although, many commentators have called this film surreal, the term fits poorly here. To quote from Encyclopedia Britannica's, surreal means:<br /><br />\"Fantastic or incongruous imagery\": One needn't explain to the unimaginative how many ways a plucky ten-year-old boy at large and seeking his fortune in the driver's seat of a red Mustang could be fantastic: those curious might read James Kincaid; but if you asked said lad how he were incongruous behind the wheel of a sports car, he'd surely protest, \"NO way!\" What fantasies and incongruities the film offers mostly appear within the first fifteen minutes. Thereafter we get more iterations of the same, in an ever-cruder and more squalid progression that, far from incongruous, soon proves predictable. Not that it were, on the other hand, literally believable-- but it were unfair to tax Motorama in particular with this flaw, any plausible suspension of disbelief having fallen precipitously on the typical film-maker's and viewer's scale of values ever since \"Raiders of the Lost Ark\" became a blockbuster.<br /><br />\"Hallucinatory\": How do we know what a hallucination is if part of having one is not knowing that we are having one? At any rate, some people know that they enjoy \"hallucinogenic drugs\"-- but if Motorama typifies the result of doing so, then I'm at a loss as to why anyone would take them more than once. There is, of course, the occasional bad trip. The movie must be one of those, pun and all.<br /><br />\"Juxtaposition of words that was startling\": How many times can a ten-year-old startle you by uttering \"Oh, my God!\" when he likes something, or \"Damn!\" when he doesn't? These two interjections are about par for the course with this script. Sadly, any sense of the surreal in what passes for dialogue could only reveal, in direct proportion, one's naivete regarding the speech patterns of the rising American generation.<br /><br />\"A world completely defined and minutely depicted but that makes no rational sense:\" Motorama's world indeed makes no sense, but it is about as completely defined as a cartoon in an elementary school newspaper. The numerous guest stars in the cast all have cameo roles even less intelligent than our little hero who exclaims \"Damn!\" in the blink of an eyelash but needs several seconds to concoct the lamest lie. And even *his* character, despite appearing in nearly every scene, gets no significant development. Here's scant reward for any viewer who sympathizes, as I must, enough to wish to know him better and understand 'where he's coming from.' One vaguely senses a far better story and protagonist struggling to get out.<br /><br />\"Fully recognizable, realistically painted images are removed from their normal contexts and reassembled within an ambiguous, paradoxical, or shocking framework.\" No, we see a succession of stereotypical and ever more dilapidated billboards, filling stations, greasy-spoon eateries, cheap hotels, and their lowlife habitues along country highways, exactly where they stereotypically belong.<br /><br />\"Largely responsible for perpetuating... the traditional emphasis on content.\" There is little content, moment-to-moment, in Motorama.<br /><br />To sum up: Picture British millionaires dressed as clowns or pirates on the way to a posh costume party, sitting serene and mute as cautious chauffeurs inch their Rolls-Royces like fragile skiffs through a roiling sea of desperate humanity, Chinese who implore them through the windows and smear the glass with blood. Or imagine a stadium full of abandoned antiques, limousines like those above now rusting, and white pianos tinkled by ghosts. Into this detritus wander an exhausted boy and an ailing woman to whom he clings as mother-figure becoming girl-friend, who fall asleep side by side on the grass. He is awakened-- on the Feast of the Transfiguration, \"white and glistering\" day 1945-- by a brilliant flash on the horizon that is not the rising sun. Finding that his consort has become a corpse, he first believes that he has witnessed her soul going up to heaven. Later he explains only a little less innocently, 'I learned a new word today: atom-bomb. It's like God taking a photograph.' Now, *there* are just two samples of cinematic surrealism, surrealism whose ironies ripple out far enough to invade its film's very title: Empire of the Sun. If you seek surreal, *please* don't miss it. Alas, however hard he treads on the accelerator to race his chariot through and beyond the desert, no scenes so exquisitely strange, rich, subtle, or gorgeous await Motorama's poor little Gus in his quest.<br /><br />None of the above necessarily constitutes a thumbs-down on this film. Though somewhat disappointed, I can't dismiss it, in view of the respectability of another genre that it does exemplify-- one influenced, to be sure, by surrealism, but also by expressionism, existentialism, and Franz Kafka's pessimism amidst omnipotent power structures. Let's try on for size: Theater of the Absurd.<br /><br />Turning to E.B.'s article on this style, I am amazed by how, to the extent that Theater of the Absurd is a valid artistic style, the above objections to Motorama vanish like a puff of smoke. I'm tempted to quote the entire text as support of the identification.<br /><br />Theater of the Absurd attempts to show \"that the human situation is essentially absurd, devoid of purpose... humankind is left feeling hopeless, bewildered, and anxious.\": Having instantaneously achieved his purpose of getting away from a depressing home life among bickering parents, Gus finds himself purposeless until he drives past a glittering billboard reading \"Motorama\" and decides to win the lottery that it promises. As others have already revealed, this ambition proves illusory: although the game \"never expires\", the sponsoring corporation has no intention that anyone should ever win, and has ways to trick, confuse, and leave crestfallen any aspirant to the reward. He, like others, is ultimately disappointed in his dream.<br /><br />\"Absurdist playwrights, therefore, did away with most of the logical structure of traditional theatre. There is little dramatic action as conventionally understood; however frantically the characters perform, their busyness serves to underscore the fact that nothing happens to change their existence... a timeless, circular quality emerges.\" \"Language in an absurdist play is full of... repetitions... repeating the obvious until it sounds like nonsense.\" Underneath a sometimes \"dazzling comic surface,\" we find \"an underlying message of metaphysical distress.\" Gus's obsession with a silly game, his inane language, the plot device wherein he divines a bleak future and/or returns to an earlier moment and takes a different but still bleak turn-- so much fits now. While an admirer of the surreal would do better with some films, anyway, of Spielberg, admirers of Motorama as it really is should find fellow-travelers-- not instead but addition-- in the works of Beckett, Ionesco, and Genet.<br /><br />But one can't quite stop here. After his disillusionment with the game, Gus returns to \"Phil\" (i.e., Love), the first attendant he had met and the one person who had treated him decently, although he had also scolded him-- at a service station advertising \"Be full-filled!\". Under Phil's tutelage he learns a life of waiting for cars. We might note here that the absurdist playwright Beckett had entitled his most famous play \"Waiting for Godot,\" and that for Godot we should read \"God.\" God is one of Phil's preoccupations, too. Furthermore, as the indirect result of his previous encounter with Gus, Phil is badly maimed and goes about in a cast with his arms straight out horizontally. In the last scene, Gus, now Phil's protege, says that he wants to hear music. We hear none, but we see Phil wiggling his fingers at the end of his outstretched arm, beckoning Gus closer, and Gus responds. The End.<br /><br />Finally, on to an author whom I happen to be reading currently, the Anglican theologian William Stringfellow. If this rebel-lawyer is not acknowledged as an architect or undergirder of Liberation Theology, which is more a Roman Catholic than an Anglican movement, perhaps he should be. Police brutality and corporate greed are a cliche in cinema and literature, including Motorama, but Stringfellow supports and illuminates such sentiments with impressive warrants from scripture, tradition, and reason.<br /><br />His most significant work is an expose of the earthly activities of those fallen angels whom the Bible refers to as principalities and powers. Principalities, wrote Stringfellow, are behind all of our popular three I's: Images, Institutions, and Ideologies. All of these commend themselves to our worship by making false promises. The more deeply involved with an image, an institution, or an ideology any person becomes, the more his own personhood becomes \"depleted\" and be becomes a slave to them. Promising power, control, and immortality, they inexorably deliver helplessness, chaos, and death. As essentially fallen, defeated powers, they can do no more than that. Yet they beguile humans with that \"dominion over the earth\" promised by God in the book of Genesis, while in fact no one of us controls an image, an institution, or an ideology bent inevitably on its own hegemony and self-preservation. They take on lives of their own. \"Dominion\" happens to be a mistranslation: a more accurate rendering of the Hebrew would be \"stewardship.\" But this is a quibble beside a more fundamental problem: Most of us neglect to notice that God had delegated this power to Adam *before* the fall. We have no reason to assume that we, his descendents, still exercise it now: on the contrary, it should be obvious that demonic forces have stolen it from us.<br /><br />One might add two observations of C.S. Lewis: First, that \"man's conquest of nature\" is a mere illusion, and a ruse to cover the fact that one is really talking about the conquest of some men by other men with nature as the instrument; and secondly, contrary to popular belief, Satan is no kind of good-time Charlie. He may dangle out pleasures at first, but he is very niggardly with them and will withdraw them from any human firmly in his thrall, perhaps leaving his prey sitting in front of the fire feeling miserably sorry for himself and seething with resentment.<br /><br />Now, applying these insights to Motorama, we seem them mirrored remarkably in Gus's experience. He is, if not nice, at least a pretty little boy prior to falling victim to the Motorama game. The first signs advertising it glisten glamorously. The longer he continues, however, and the deeper he journeys towards the sponsoring corporation's headquarters, the more shabby they become. He's lonely, meeting no one else who plays the game. The stations giving out the cards have either fallen into ruins or are staffed by zombies. The people he does meet along the way are more and more ugly, deceitful, and hostile. (The fact that the principalities answer to a common dictator does not mean that they can abide one another). Gus's humanity is leached out of him as he becomes not only totally self-centered and oblivious to the needs of others but partially blinded... disfigured... prematurely aged while infantile in the literal sense of linguistically challenged. Eventually even his precious Mustang is taken from him in a crash, and he must continue in a dead man's wreck. Yet at long last, having done everything he thought was expected, he presents himself to the principality in its proud tower to receive his prize. Using the biblical power to confuse wielded by those who have built such monuments to their own vanity, its agents evade him, disappoint, insult, and finally throw him from the top floor. He FALLS long and hard, landing, finally in a body of water. In other words, in classic symbolism, he DIES. He has met the inevitable bad end of anyone who has put his faith in such a deceiver.<br /><br />But this fate proves to be only a warning look into a mutable future. He repents and returns to Phil, and upon seeing him performs the very first generous, selfless act we have seen from him for almost an hour and a half: noting that Phil is now handicapped and hardly able to insert a hose into a gas tank, he asks, \"Can I help you with that?\" Then, seeing the \"help wanted\" sign, he decides to apply for the job, explaining to the motorist with whom he was hitch-hiking that he reckons he'll get out here, because it doesn't look like too bad a place to work.<br /><br />This interpretation is conjectural, of course, and it may surprise or even outrage the film's \"cult classic\" aficionados who see quite different points in it.<br /><br />If Motorama isn't quite my cup of tea, I'm at least convinced now that it's hardly the worst film ever made.\n"
     ]
    }
   ],
   "source": [
    "print('The review:')\n",
    "with open(test_pos_dir+test_pos_rev_files[my_idx], 'r',encoding=\"utf8\") as f:\n",
    "    my_review=f.read()\n",
    "    print(my_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:200\t('Negative', 0.018453430384397507)\n",
      "Length:250\t('Negative', 0.019829707220196724)\n",
      "Length:300\t('Negative', 0.0029593247454613447)\n",
      "Length:350\t('Negative', 0.012203792110085487)\n",
      "Length:400\t('Negative', 0.011524662375450134)\n",
      "Length:450\t('Negative', 0.020036334171891212)\n",
      "Length:500\t('Negative', 0.017699336633086205)\n",
      "Length:550\t('Negative', 0.018588325008749962)\n",
      "Length:600\t('Negative', 0.02527276612818241)\n",
      "Length:650\t('Negative', 0.0337168388068676)\n",
      "Length:700\t('Negative', 0.20352429151535034)\n",
      "Length:750\t('Negative', 0.278317928314209)\n",
      "Length:800\t('Negative', 0.19187146425247192)\n",
      "Length:850\t('Negative', 0.08950316905975342)\n",
      "Length:900\t('Negative', 0.008395679295063019)\n",
      "Length:950\t('Negative', 0.020181169733405113)\n",
      "Length:1000\t('Negative', 0.013488088734447956)\n",
      "Length:1050\t('Negative', 0.06002632528543472)\n",
      "Length:1100\t('Negative', 0.04306826740503311)\n",
      "Length:1150\t('Negative', 0.05303666740655899)\n",
      "Length:1200\t('Negative', 0.1857983022928238)\n",
      "Length:1250\t('Negative', 0.042468901723623276)\n",
      "Length:1300\t('Negative', 0.10404867678880692)\n",
      "Length:1350\t('Negative', 0.14756305515766144)\n",
      "Length:1400\t('Negative', 0.3663439452648163)\n",
      "Length:1450\t('Negative', 0.33004996180534363)\n",
      "Length:1500\t('Negative', 0.13831304013729095)\n",
      "Length:1550\t('Negative', 0.17215216159820557)\n",
      "Length:1600\t('Negative', 0.06306783109903336)\n",
      "Length:1650\t('Negative', 0.059726160019636154)\n",
      "Length:1700\t('Negative', 0.005151549819856882)\n",
      "Length:1750\t('Negative', 0.06821532547473907)\n",
      "Length:1800\t('Negative', 0.030032744631171227)\n",
      "Length:1850\t('Negative', 0.016401516273617744)\n",
      "Length:1900\t('Negative', 0.03858301043510437)\n",
      "Length:1950\t('Negative', 0.07528596371412277)\n",
      "Length:2000\t('Negative', 0.041525889188051224)\n",
      "Length:2050\t('Negative', 0.07364508509635925)\n",
      "Length:2100\t('Negative', 0.024467667564749718)\n"
     ]
    }
   ],
   "source": [
    "for sl in range(200,len(test_reviews_int[my_idx]),50):\n",
    "    print(f\"Length:{sl}\\t{predict(net, my_review, sequence_length=sl)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probabilities vary with the sequence length (0.01-0.37) but always remained clearly below 0.5, and the review is never predicted to be positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
