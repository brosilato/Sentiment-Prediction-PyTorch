{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with an RNN\n",
    "\n",
    "In this notebook, I implement a recursive neural network (RNN) composed of layers of GRU cells. This RNN will be trained using the traning set of the IMDB reviews database and it will be tested on its corresponding testing set. These databases has been downloaded from:\n",
    "__[http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)__\n",
    "\n",
    "The model is implemented using pyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# IMPORTS\n",
    "Let's import the modules we will use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPUs?\n",
    "Let's check if a GPU is available and select the device use for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if it's available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load in, tokenize and visualize the data\n",
    "\n",
    "The download data is already divied into train and test data. Each folder is further divided into positive (7-10/10 stars reviews) and negative reviews (1-4/10 stars reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SET\n",
      "Number of positive reviews: 12500 Number of negative reviews: 12500\n"
     ]
    }
   ],
   "source": [
    "#training positive reviews directory\n",
    "train_pos_dir = r'aclImdb/train/pos/'\n",
    "#negative positive reviews directory\n",
    "train_neg_dir = r'aclImdb/train/neg/'\n",
    "\n",
    "#List of files with training positive review\n",
    "train_pos_rev_files = os.listdir(train_pos_dir)\n",
    "#List of files with training negative review\n",
    "train_neg_rev_files = os.listdir(train_neg_dir)\n",
    "print('TRAIN SET')\n",
    "print('Number of positive reviews:',len(train_pos_rev_files),'Number of negative reviews:',len(train_neg_rev_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep track of the number of stars (1-10) awarded to each positive and negative review in the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_stars = []\n",
    "for file in train_pos_rev_files:\n",
    "    full_train_stars.append(int(file.split('_')[1].split('.')[0]))\n",
    "for file in train_neg_rev_files:\n",
    "    full_train_stars.append(int(file.split('_')[1].split('.')[0]))    \n",
    "full_train_stars = np.array(full_train_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST SET\n",
      "Number of positive reviews: 12500 Number of negative reviews: 12500\n"
     ]
    }
   ],
   "source": [
    "#training positive reviews directory\n",
    "test_pos_dir = r'aclImdb/test/pos/'\n",
    "#negative positive reviews directory\n",
    "test_neg_dir = r'aclImdb/test/neg/'\n",
    "\n",
    "#List of files with training positive review\n",
    "test_pos_rev_files = os.listdir(test_pos_dir)\n",
    "#List of files with training negative review\n",
    "test_neg_rev_files = os.listdir(test_neg_dir)\n",
    "print('TEST SET')\n",
    "print('Number of positive reviews:',len(test_pos_rev_files),'Number of negative reviews:',len(test_neg_rev_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep track of the number of stars (1-10) awarded to each positive and negative review in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stars = []\n",
    "for file in test_pos_rev_files:\n",
    "    test_stars.append(int(file.split('_')[1].split('.')[0]))\n",
    "for file in test_neg_rev_files:\n",
    "    test_stars.append(int(file.split('_')[1].split('.')[0]))    \n",
    "test_stars = np.array(test_stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that 5- and 6-star reviews are not included in the train or test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  7  8  9 10]\n",
      "[ 1  2  3  4  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(full_train_stars))\n",
    "print(np.unique(test_stars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Matrices\n",
    "Let's create numpy arrays that hold the train and test labels. 1 stands for positive and 0 for negative. Since we will stack first the positive reviews and the the negative ones, the first 12500 elements are ones and the next 12500 are zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train target array\n",
    "full_train_target = np.zeros(len(train_neg_rev_files)+len(train_neg_rev_files), dtype=int)\n",
    "full_train_target[:len(train_neg_rev_files)] = 1\n",
    "#Test target array\n",
    "test_target = np.zeros(len(test_neg_rev_files)+len(test_neg_rev_files), dtype=int)\n",
    "test_target[:len(test_neg_rev_files)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,)\n",
      "0.5 0.5\n"
     ]
    }
   ],
   "source": [
    "print(full_train_target.shape, test_target.shape)\n",
    "print(full_train_target.mean(), test_target.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Contractions\n",
    "The following function expands common english contractions. There is obviously  plenty of room for improvement. Some \n",
    "actions are 100% justified (can't and cannot into can not), while others are rather arbitrary ('s into is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_contractions(text):\n",
    "    # Turn ain't into am not (it could be many other oprions such us is not,...)\n",
    "    text = text.replace(\"ain't\", \"am not\")\n",
    "    #Turn can't and annot into can not\n",
    "    text = text.replace(\"can't\", \"can not\").replace(\"cannot\", \"can not\")\n",
    "    #Turn shan't into shall not\n",
    "    text = text.replace(\"shan't\", \"shall not\")\n",
    "    #Turn won't into will not\n",
    "    text = text.replace(\"won't\", \"will not\")\n",
    "    #Turn y'all into you all\n",
    "    text = text.replace(\"y'all\", \"you all\")\n",
    "    #Turn n't into not\n",
    "    text = text.replace(\"n't\", \" not\")\n",
    "    #Turn 'd into would (it might be had too)\n",
    "    text = text.replace(\"'d\", \" would\")\n",
    "    #Turn 'll into will\n",
    "    text = text.replace(\"'ll\", \" will\")\n",
    "    #Turn 're into are\n",
    "    text = text.replace(\"'re\", \" are\")\n",
    "    #Turn 'm into am\n",
    "    text = text.replace(\"'m\", \" am\")\n",
    "    #Turn 's into is (it could also be has or the posssesive)\n",
    "    text = text.replace(\"'s\", \" is\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviews as List of Words\n",
    "For each review we convert every character to lower case, remove english contructions, then remove punctuation and finally split it into words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_reviews=[]\n",
    "#Read and tokenize positive reviews\n",
    "for file_name in train_pos_rev_files:\n",
    "    with open(train_pos_dir+file_name, 'r',encoding=\"utf8\") as f:\n",
    "        review = f.read().lower()\n",
    "        #remove some contractions\n",
    "        review = remove_contractions(review)\n",
    "        #remove punctuation\n",
    "        review = ''.join([c for c in review if c not in punctuation])\n",
    "        #split in words\n",
    "        review = review.split()  \n",
    "    full_train_reviews.append(review)\n",
    "\n",
    "#Read and tokenize negative reviews\n",
    "for file_name in train_neg_rev_files:\n",
    "    with open(train_neg_dir+file_name, 'r',encoding=\"utf8\") as f:\n",
    "        review = f.read().lower()\n",
    "        #remove some contractions\n",
    "        review = remove_contractions(review)\n",
    "        #remove punctuation\n",
    "        review = ''.join([c for c in review if c not in punctuation])\n",
    "        #split in words\n",
    "        review = review.split()  \n",
    "    full_train_reviews.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "['if', 'mulder', 'was', 'looking', 'for', 'his', 'real', 'father', 'here', 'he', 'is', 'darren', 'mcgavin', 'the', 'first', 'x', 'files', 'pity', 'it', 'was', 'only', 'one', 'season', 'long', 'the', 'producers', 'of', 'this', 'show', 'did', 'not', 'know', 'that', 'they', 'had', 'the', 'makings', 'of', 'a', 'classic', 'on', 'their', 'hands', 'and', 'in', '1993', 'along', 'came', 'chris', 'carter', 'with', 'what', 'i', 'call', 'the', 'follow', 'up', 'to', 'the', 'night', 'stalker', 'the', 'x', 'files', 'both', 'will', 'go', 'down', 'as', 'classics', 'is', 'my', 'opinion', 'the', 'two', 'shows', 'taking', 'the', 'viewers', 'to', 'a', 'level', 'of', 'experience', 'that', 'only', 'comes', 'along', 'once', 'in', 'a', 'while', 'and', 'who', 'should', 'appear', 'in', 'the', 'x', 'files', 'years', 'later', 'darren', 'mcgavin', 'as', 'agent', 'arthur', 'dales', 'helping', 'our', 'two', 'favorite', 'heros', 'solving', 'cases', 'paying', 'homage', 'to', 'the', 'man', 'i', 'think', 'so', 'well', 'done', 'chris', 'carter', 'bringing', 'back', 'a', 'forgotten', 'tv', 'show', 'in', 'the', 'form', 'of', 'david', 'duchovny', 'as', 'darren', 'mcgavin', 'if', 'it', 'was', 'not', 'for', 'watching', 'the', 'x', 'files', 'and', 'that', 'particular', 'show', 'i', 'would', 'have', 'never', 'known', 'about', 'the', 'night', 'stalker']\n"
     ]
    }
   ],
   "source": [
    "print(len(full_train_reviews))\n",
    "print(full_train_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reviews=[]\n",
    "#Read and tokenize positive reviews\n",
    "for file_name in test_pos_rev_files:\n",
    "    with open(test_pos_dir+file_name, 'r',encoding=\"utf8\") as f:\n",
    "        review = f.read().lower()\n",
    "        #remove some contractions\n",
    "        review = remove_contractions(review)\n",
    "        #remove punctuation\n",
    "        review = ''.join([c for c in review if c not in punctuation])\n",
    "        #split in words\n",
    "        review = review.split()  \n",
    "    test_reviews.append(review)\n",
    "\n",
    "#Read and tokenize negative reviews\n",
    "for file_name in test_neg_rev_files:\n",
    "    with open(test_neg_dir+file_name, 'r',encoding=\"utf8\") as f:\n",
    "        review = f.read().lower()\n",
    "        #remove some contractions\n",
    "        review = remove_contractions(review)\n",
    "        #remove punctuation\n",
    "        review = ''.join([c for c in review if c not in punctuation])\n",
    "        #split in words\n",
    "        review = review.split()  \n",
    "    test_reviews.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "['i', 'saw', 'this', 'film', 'knowing', 'absolutely', 'nothing', 'about', 'both', 'it', 'and', 'its', 'stars', 'chris', 'farley', 'and', 'david', 'spade', 'and', 'i', 'have', 'to', 'say', 'that', 'this', 'film', 'is', 'a', 'comic', 'classic', 'it', 'is', 'so', 'stupid', 'at', 'times', 'that', 'it', 'can', 'only', 'be', 'hilarious', 'farley', 'is', 'brilliant', 'as', 'the', 'bumbling', 'idiot', 'who', 'takes', 'to', 'the', 'road', 'with', 'his', 'dad', 'is', 'right', 'hand', 'man', 'the', 'equally', 'excellent', 'spade', 'to', 'find', 'the', 'funding', 'to', 'save', 'the', 'families', 'auto', 'parts', 'business', 'relax', 'put', 'your', 'brain', 'on', 'autopilot', 'and', 'soak', 'up', 'the', 'fun', 'a', 'great', 'supporting', 'cast', 'features', 'film', 'favourites', 'such', 'as', 'brian', 'dennehy', 'cocoon', 'rob', 'lowe', 'wayne', 'is', 'world', 'and', 'bo', 'derek', '10', 'highly', 'recommended', 'for', 'a', 'good', 'laugh']\n"
     ]
    }
   ],
   "source": [
    "print(len(test_reviews))\n",
    "print(test_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Sets\n",
    "\n",
    "We will divide our full train set into a train set and a validation set that will help us to control how our model generalize.\n",
    "The validation set will be just 10% of the original train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#90% will remain as training data\n",
    "train_size = 0.9; test_size = 1-train_size\n",
    "(train_reviews, valid_reviews, train_y, valid_y, \n",
    " train_stars, valid_stars) = train_test_split(full_train_reviews, full_train_target, full_train_stars,\n",
    "                                              random_state=123, shuffle=True,\n",
    "                                              train_size=train_size, test_size=test_size,stratify=full_train_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22500 (22500,) (22500,)\n",
      "2500 (2500,) (2500,)\n",
      "0.5 0.5\n"
     ]
    }
   ],
   "source": [
    "#Print out the shapes of your resultant feature data\n",
    "print(len(train_reviews), train_y.shape, train_stars.shape)\n",
    "print(len(valid_reviews), valid_y.shape, valid_stars.shape)\n",
    "print(train_y.mean(), valid_y.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To keep it simple let's create a test_y variable\n",
    "test_y = test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing the size of the reviews\n",
    "It is important to xhoose now the word size of our review before we build our vocabulary. That will prevent us from including\n",
    "words that may appear frequently in our reviews if we keep all the words but not more than five times if we keep, \n",
    "let's say, the first 50 words only. Of course, the larger the sequence considered the less important this would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Sequence length #########\n",
    "seq_length = 350"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "\n",
    "Let's get our data into the proper form to feed into the network. Since we're using embedding layers, we'll need to encode each word with an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4556152\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "for review in train_reviews:\n",
    "    all_words.extend(review[:seq_length]) #Only words that are going to be used in the model\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first',\n",
       " 'of',\n",
       " 'all',\n",
       " 'really',\n",
       " 'kim',\n",
       " 'basinger',\n",
       " 'your',\n",
       " 'rich',\n",
       " 'banker',\n",
       " 'husband',\n",
       " 'leaves',\n",
       " 'you',\n",
       " 'alone',\n",
       " 'in',\n",
       " 'your',\n",
       " 'beautiful',\n",
       " 'most',\n",
       " 'likely',\n",
       " 'paid',\n",
       " 'in',\n",
       " 'cash',\n",
       " 'for',\n",
       " 'home',\n",
       " 'and',\n",
       " 'you',\n",
       " 'can',\n",
       " 'not',\n",
       " 'even',\n",
       " 'put',\n",
       " 'on']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the words\n",
    "\n",
    "The embedding lookup requires that we pass in integers to our network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Then we can convert each of our reviews into integers so they can be passed into the network.Also we are going to keep the value zero to represent the padding and the value 1 to represent every word\n",
    "that appears least than five times in our train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "## Build a dictionary that maps words to integers\n",
    "counts = Counter(all_words)\n",
    "# Let's reorder counts by word fewuency so the least common words are at the end\n",
    "words = sorted(counts, key=counts.get, reverse=True)\n",
    "\n",
    "#Remove words that appear less than 5 times\n",
    "i = 0\n",
    "for val in counts.values():\n",
    "    if val<5:\n",
    "        i+=1\n",
    "#Let's keep the most common words and add '<pad>' and '<unk>'        \n",
    "words = ['<pad>','<unk>'] + words[:-i]\n",
    "#Create a dictionary, value 1 will indicate unseen or seen less than 5 times word (<unk>). Zero is for padding <pad>\n",
    "word_to_int = {word: i for i,word in enumerate(words)}\n",
    "\n",
    "# Let's tokenize the reviews into integers\n",
    "# train set\n",
    "train_reviews_int = []\n",
    "for review in train_reviews:\n",
    "    train_reviews_int.append([word_to_int.get(word,1) for word in review])\n",
    "# validation set\n",
    "valid_reviews_int = []\n",
    "for review in valid_reviews:\n",
    "    valid_reviews_int.append([word_to_int.get(word,1) for word in review])\n",
    "# test set\n",
    "test_reviews_int = []\n",
    "for review in test_reviews:\n",
    "    test_reviews_int.append([word_to_int.get(word,1) for word in review])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove: 75280 \t Kept: 27057\n"
     ]
    }
   ],
   "source": [
    "print(f'Remove: {i} \\t Kept: {len(words)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test your code**\n",
    "\n",
    "As a text that you've implemented the dictionary correctly, print out the number of unique words in your vocabulary and the contents of the first, tokenized review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:  27057\n",
      "\n",
      "Tokenized review: \n",
      " [84, 6, 31, 64, 2139, 4624, 123, 927, 13693, 568, 850, 24, 599, 9, 123, 284, 90, 1382, 1425, 9, 2161, 18, 327, 5, 24, 50, 13, 59, 262, 23, 4, 512, 4744, 10, 93, 4, 230, 5, 388, 10, 93, 164, 7, 210, 206, 47, 128, 1, 142, 1703, 19, 123, 1153, 5, 134, 24, 22, 30, 8, 8, 3, 909, 3932, 727, 123, 322, 49, 1, 30, 214, 4, 909, 2953, 43, 13, 1456, 1519, 221, 7, 2, 9390, 4562, 1434, 123, 491, 429, 1872, 240, 5, 1347, 42, 8, 2, 213, 1128, 9, 5, 313, 727, 15043, 2396, 87, 2, 366, 63, 24, 1129, 7, 552, 266, 4, 1529, 899, 43, 13, 1863, 123, 380, 10, 6577, 7, 225, 741, 18, 8715, 501, 649, 7, 2, 184, 12, 2139, 4624, 3, 11, 966, 3, 3142, 1390, 56, 462, 24, 7, 225, 78, 18, 2, 318, 433, 4431, 230, 74, 82, 8, 4525, 4863, 3, 4, 3485, 20, 592, 43, 13, 2316, 465, 110, 100, 123, 201, 120, 87, 24, 22, 9, 24, 4192, 24, 389, 51, 13, 25, 1134, 312, 155, 4745, 5, 144, 1326, 4481, 5, 15818, 98, 7, 265, 123, 322, 2328, 61, 19, 102, 512, 531, 6, 121, 1, 524, 8, 3, 87, 172, 706, 7, 814, 2, 104, 9, 2, 21, 1368, 632, 41, 284, 44, 5951, 7, 41, 16, 4, 1, 1518, 4624, 66, 49, 128, 42, 48, 254, 9, 2, 212, 8, 3, 87, 172, 1051, 89, 575, 6, 2, 2727, 50, 536, 143, 639, 8715, 3, 2383, 4563, 1816, 22, 1135, 7, 1478, 41, 1833, 1816, 22, 172, 8521, 99, 11, 21, 3, 37, 1558, 8, 4286, 1, 33, 8313, 123, 380, 347, 23, 2, 3512, 3578, 44, 123, 568, 262, 4, 3307, 9, 2, 1656, 20, 31, 24, 50, 103, 42, 3, 2467, 5098, 6463, 54, 24, 22, 30, 2, 4562, 298, 6, 280, 49, 1, 5, 2497, 44, 2, 230, 24, 405, 7, 309, 363, 19, 1118, 2, 9162, 24, 69, 278, 30, 1, 2, 184, 12, 56, 3026, 7, 186, 129, 12, 1973, 17856, 6827, 134, 617, 141, 2, 1396, 1908, 5, 2579, 268, 9, 41, 2508, 1488, 57, 1, 230, 3, 4, 6511, 35, 1241, 7, 4319, 2, 13441, 47, 6, 2, 491, 20, 13, 70, 9921, 368, 6, 17118, 12499, 135, 16, 123, 5639, 10, 25, 117, 438, 2, 78, 478, 7, 3561, 16, 76, 16, 54, 10, 253, 11, 21, 5, 72, 237, 320, 565, 7, 1195, 2, 2726, 304, 4744, 2, 314, 217, 15, 8163, 442, 388, 9810, 5, 5862, 123, 520, 23, 376, 2, 90, 16325, 2663, 8040, 9, 4, 21, 243, 43, 13, 386, 5, 523, 5, 2197, 221, 6, 123, 120, 298, 137, 43, 48, 8715, 79, 13, 776, 47, 89, 7, 1, 434, 6, 123, 322, 5, 280, 9112, 123, 1153, 12, 1030, 15002, 30, 2, 136, 151, 15, 172, 1157, 1113, 10, 58, 28, 327, 18, 909, 9, 2, 2433, 134, 123, 1666, 3324, 1, 442, 37, 10269, 7, 123, 15043, 2396, 3, 42, 16, 915, 16, 8, 204, 1232, 8715]\n",
      "22500\n"
     ]
    }
   ],
   "source": [
    "# stats about vocabulary\n",
    "print('Unique words: ', len((word_to_int)))  \n",
    "print()\n",
    "\n",
    "# print tokens in first review\n",
    "print('Tokenized review: \\n', train_reviews_int[0])\n",
    "print(len(train_reviews_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of the Reviews \n",
    "\n",
    "We have already set for a value of 500 words but it is convinient to check some statistic about the review lenght (mean,\n",
    "median, std). We may even decide to run the whole notebook with a different sequence lenght."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum review length: 10\n",
      "Maximum review length: 2501\n",
      "Mean and Median review length: 237.38422222222223\t177.0\n",
      "Std review length: 176.13607837867087\n"
     ]
    }
   ],
   "source": [
    "review_lens = np.array([len(x) for x in train_reviews_int])\n",
    "\n",
    "print(f\"Minimum review length: {review_lens.min()}\")\n",
    "print(f\"Maximum review length: {review_lens.max()}\")\n",
    "print(f'Mean and Median review length: {review_lens.mean()}\\t{np.median(review_lens)}')\n",
    "print(f'Std review length: {review_lens.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Padding sequences\n",
    "\n",
    "To deal with both short and very long reviews, we'll pad or truncate all our reviews to a specific length. For reviews shorter than some `seq_length`, we'll pad with 0s. For reviews longer than `seq_length`, we can truncate them to the first `seq_length` words. We will work with a sewuence lenght of 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length, alignment='right'):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    features=np.zeros((len(reviews_ints),seq_length),dtype=int)\n",
    "    \n",
    "    if alignment.lower() == 'right':\n",
    "        my_idxs = lambda length: ((seq_length-length),seq_length) \n",
    "    elif alignment.lower() == 'left':\n",
    "        my_idxs = lambda length: (0,seq_length-(seq_length-length)) \n",
    "    elif alignment.lower() == 'center':\n",
    "        my_idxs = lambda length: ((seq_length-length)//2,seq_length-((seq_length-length)-(seq_length-length)//2))\n",
    "    else:\n",
    "        print(alignment, 'is not a valid option for alignment')\n",
    "        pritn('options: right, left, center')\n",
    "        return None\n",
    "    \n",
    "    for i,review in enumerate(reviews_ints):\n",
    "        text = review[:seq_length]\n",
    "        (a,b) = my_idxs(len(text))\n",
    "        features[i,a:b] = text\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   84     6    31    64  2139  4624   123   927 13693   568   850    24\n",
      "    599     9   123   284    90  1382  1425     9  2161    18   327     5\n",
      "     24    50    13    59   262    23     4   512  4744    10    93     4\n",
      "    230     5   388    10    93   164     7   210   206    47   128     1\n",
      "    142  1703    19   123  1153     5   134    24    22    30     8     8\n",
      "      3   909  3932   727   123   322    49     1    30   214     4   909\n",
      "   2953    43    13  1456  1519   221     7     2  9390  4562  1434   123\n",
      "    491   429  1872   240     5  1347    42     8     2   213  1128     9\n",
      "      5   313   727 15043  2396    87     2   366    63    24  1129     7\n",
      "    552   266     4  1529   899    43    13  1863   123   380    10  6577\n",
      "      7   225   741    18  8715   501   649     7     2   184    12  2139\n",
      "   4624     3    11   966     3  3142  1390    56   462    24     7   225\n",
      "     78    18     2   318   433  4431   230    74    82     8  4525  4863\n",
      "      3     4  3485    20   592    43    13  2316   465   110   100   123\n",
      "    201   120    87    24    22     9    24]]\n",
      "[[ 4192    24   389    51    13    25  1134   312   155  4745     5   144\n",
      "   1326  4481     5 15818    98     7   265   123   322  2328    61    19\n",
      "    102   512   531     6   121     1   524     8     3    87   172   706\n",
      "      7   814     2   104     9     2    21  1368   632    41   284    44\n",
      "   5951     7    41    16     4     1  1518  4624    66    49   128    42\n",
      "     48   254     9     2   212     8     3    87   172  1051    89   575\n",
      "      6     2  2727    50   536   143   639  8715     3  2383  4563  1816\n",
      "     22  1135     7  1478    41  1833  1816    22   172  8521    99    11\n",
      "     21     3    37  1558     8  4286     1    33  8313   123   380   347\n",
      "     23     2  3512  3578    44   123   568   262     4  3307     9     2\n",
      "   1656    20    31    24    50   103    42     3  2467  5098  6463    54\n",
      "     24    22    30     2  4562   298     6   280    49     1     5  2497\n",
      "     44     2   230    24   405     7   309   363    19  1118     2  9162\n",
      "     24    69   278    30     1     2   184    12    56  3026     7   186\n",
      "    129    12  1973 17856  6827   134   617]]\n"
     ]
    }
   ],
   "source": [
    "train_x = pad_features(train_reviews_int, seq_length=seq_length)\n",
    "valid_x = pad_features(valid_reviews_int, seq_length=seq_length)\n",
    "test_x = pad_features(test_reviews_int, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(train_x)==len(train_reviews_int), \"Your features should have as many rows as reviews.\"\n",
    "assert len(train_x[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# Print the first review\n",
    "print(train_x[:1,:seq_length//2])\n",
    "print(train_x[:1,seq_length//2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders and Batching\n",
    "\n",
    "After creating training, test, and validation data, we can create DataLoaders for batching our data into the net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x).long(), torch.from_numpy(train_y).long())\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_x).long(), torch.from_numpy(valid_y).long())\n",
    "test_data = TensorDataset(torch.from_numpy(test_x).long(), torch.from_numpy(test_y).long())\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader_50 = DataLoader(train_data, shuffle=True, batch_size=50)\n",
    "train_loader_100 = DataLoader(train_data, shuffle=True, batch_size=100)\n",
    "train_loader_150 = DataLoader(train_data, shuffle=True, batch_size=150)\n",
    "valid_loader = DataLoader(valid_data, shuffle=False, batch_size=100) #No need to shuffle\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=100) #No need to shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 350])\n",
      "Sample input: \n",
      " tensor([[   0,    0,    0,  ...,    1, 1417,   21],\n",
      "        [   0,    0,    0,  ...,   43,   24,  340],\n",
      "        [   0,    0,    0,  ...,  111,   95,  486],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,    3,  224,   20],\n",
      "        [   0,    0,    0,  ..., 2015,  694,    8],\n",
      "        [   0,    0,    0,  ...,   38,    8, 1444]])\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
      "        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        1, 1])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader_50)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Sentiment Network with PyTorch\n",
    "\n",
    "We are going to use a Recurent Neural Network (RNN) to tackle the sentiment prediction problem. The layers are as follows:\n",
    "1. An embedding layer that converts our word tokens (integers) into embeddings of a specific size and whose coefficients will be learnt during training.\n",
    "2. An GRU cell defined by a hidden_state size and number of layers\n",
    "3. A fully-connected output layer that maps the GRU layer outputs to a desired output_size\n",
    "4. A sigmoid activation layer which turns all outputs into a value 0-1; it returns **only the last sigmoid output** as the output of this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, \n",
    "                 drop_prob=0.5, seq_length=None):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        \n",
    "        #keeps the hidden state\n",
    "        self.hidden = None\n",
    "        \n",
    "        # define all layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden=None, return_hidden=False):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        if hidden:\n",
    "            gru_out, hidden = self.gru(embeds, hidden)\n",
    "        else:\n",
    "            #Get hidden state all zeros\n",
    "            hidden = self.init_hidden(batch_size)\n",
    "            #Ensures hidden is in the same device that x is\n",
    "            hidden = x.new_tensor(data=hidden, dtype=hidden.dtype) \n",
    "            gru_out, hidden = self.gru(embeds, hidden)\n",
    "         # stack up gru outputs\n",
    "        gru_out = gru_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(gru_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        if return_hidden:\n",
    "            # return last sigmoid output and hidden state\n",
    "            return sig_out, hidden\n",
    "        \n",
    "        return sig_out\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        #weight = next(self.parameters()).data\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden\n",
    "    \n",
    "    def get_params(self):\n",
    "        params = {'vocab_size': self.vocab_size, 'output_size': self.output_size, 'hidden_dim': self.hidden_dim,\n",
    "                  'embedding_dim': self.embedding_dim, 'seq_length': self.seq_length, \n",
    "                  'drop_prob': self.drop_prob, 'n_layers': self.n_layers}\n",
    "        return params\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the network\n",
    "\n",
    "Here, we'll instantiate the network. First up, defining the hyperparameters.\n",
    "\n",
    "* `vocab_size`: Size of our vocabulary or the range of values for our input, word tokens.\n",
    "* `output_size`: Size of our desired output; the number of class scores we want to output (pos/neg).\n",
    "* `embedding_dim`: Number of columns in the embedding lookup table; size of our embeddings.\n",
    "* `hidden_dim`: Number of units in the hidden layers of our GRU cells.\n",
    "* `n_layers`: Number of GRU layers in the network. Typically between 1-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(27057, 128, padding_idx=0)\n",
       "  (gru): GRU(128, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
       "  (dropout): Dropout(p=0.3)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "net_params = {'vocab_size': len(word_to_int), 'output_size': 1, 'embedding_dim': 128, \n",
    "              'hidden_dim': 128,'n_layers': 2, 'drop_prob': 0.3}\n",
    "\n",
    "net = SentimentRNN(**net_params)\n",
    "\n",
    "#Save the encoding dictionary and the list of words\n",
    "net.words = words\n",
    "net.word_to_int = word_to_int\n",
    "#Move to gpu or cpu device\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training\n",
    "\n",
    "Let's train our net. We will use the Adam optimizer and we evaluate the model every few steps and after each epoch. We will save the best model based on the evaluation loss\n",
    "\n",
    "Training hyparameters:\n",
    "\n",
    "* `lr`: Learning rate for our optimizer.\n",
    "* `L2`: L2 regularization used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr = 0.001\n",
    "L2 = 1e-3\n",
    "criterion = nn.BCELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=L2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function saves the model, optimizer and other parameters into a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(path=None, model=None, optimizer=None, params=None, \n",
    "                    epoch=None, train_loss=None, valid_loss=None, word_to_int=None, words=None):\n",
    "    if path:\n",
    "        my_path=path\n",
    "        print('Using', my_path, 'to save')\n",
    "    else:\n",
    "        my_path='my_model.pt'\n",
    "        print('Using', my_path, 'to save')\n",
    "        \n",
    "    checkpoint = {}\n",
    "    \n",
    "    if model:\n",
    "        checkpoint['model_state_dict']= model.state_dict()\n",
    "    else:\n",
    "        print('No model dictionary saved')\n",
    "    \n",
    "    if params:\n",
    "        checkpoint['params'] = params\n",
    "    else:\n",
    "        print('No model parameters saved')\n",
    "        \n",
    "    if optimizer:\n",
    "        checkpoint['optimizer_state_dict']= optimizer.state_dict()\n",
    "    else:\n",
    "        print('NNo optimizer dictionary saved')\n",
    "        \n",
    "    if epoch:\n",
    "        checkpoint['epoch'] = epoch\n",
    "    else:\n",
    "        print('No current epoch value saved')\n",
    "        \n",
    "    if train_loss:\n",
    "        checkpoint['train_loss'] = train_loss\n",
    "    else:\n",
    "        print('No value of the training loss saved')\n",
    "        \n",
    "    if valid_loss:\n",
    "        checkpoint['valid_loss'] = valid_loss\n",
    "    else:\n",
    "        print('No value of the validation loss saved')\n",
    "\n",
    "    if word_to_int:\n",
    "        checkpoint['word_to_int'] = word_to_int\n",
    "    else:\n",
    "        print('No dictionary for encoding words saved')\n",
    "        \n",
    "    if words:\n",
    "        checkpoint['words'] = words\n",
    "    else:\n",
    "        print('No list of words saved')\n",
    "    \n",
    "    torch.save(checkpoint, my_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function loads the model, optimizer and other parameters into a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path=None, model=None, optimizer=None):\n",
    "    '''\n",
    "    It overrrides the model and optimizer provided with the saved parameters \n",
    "    and returns the saved epoch, train loss and validation loss'''\n",
    "    if path:\n",
    "        checkpoint = torch.load(path, map_location='cpu') \n",
    "    else:\n",
    "        print('Nothing loaded. Plese provide a file')\n",
    "        return None\n",
    "        \n",
    "    #Load the model state dictionary\n",
    "    my_dict = checkpoint.get('model_state_dict', None)\n",
    "    if my_dict:\n",
    "        model.load_state_dict(my_dict)\n",
    "        model.word_to_int = checkpoint.get('word_to_int', None)\n",
    "        model.words = checkpoint.get('words', None)\n",
    "    else:\n",
    "        print('No model dictionary found')\n",
    "    \n",
    "    #Load the optimizer state dictionary\n",
    "    my_dict = checkpoint.get('optimizer_state_dict', None)\n",
    "    if my_dict:\n",
    "        optimizer.load_state_dict(my_dict)\n",
    "    else:\n",
    "        print('No optimizer dictionary found')    \n",
    "    \n",
    "    #Load the epoch value, train loss and validation loss\n",
    "    epoch = checkpoint.get('epoch', None)\n",
    "    train_loss = checkpoint.get('train_loss', None)\n",
    "    valid_loss = checkpoint.get('valid_loss', None)\n",
    "    \n",
    "    return epoch, train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE THE MODEL HERE ###\n",
    "my_path = 'my_RNN_0117.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing the number of intermediate evals at epoch 1\n",
      "225\n",
      "[112]\n",
      "Step: 112\n",
      "Epoch: 1/20... Train Loss: 0.667155... Val Loss: 0.607035\n",
      "Validation loss decreased (inf --> 0.607035).  Saving model ...\n",
      "Using my_RNN_0117.pt to save\n",
      "Step: 225\n",
      "Epoch: 1/20... Train Loss: 0.629971... Val Loss: 0.570323\n",
      "Validation loss decreased (0.607035 --> 0.570323).  Saving model ...\n",
      "Using my_RNN_0117.pt to save\n",
      "Step: 112\n",
      "Epoch: 2/20... Train Loss: 0.538161... Val Loss: 0.538077\n",
      "Validation loss decreased (0.570323 --> 0.538077).  Saving model ...\n",
      "Using my_RNN_0117.pt to save\n",
      "Step: 225\n",
      "Epoch: 2/20... Train Loss: 0.531259... Val Loss: 0.482202\n",
      "Validation loss decreased (0.538077 --> 0.482202).  Saving model ...\n",
      "Using my_RNN_0117.pt to save\n",
      "Step: 112\n",
      "Epoch: 3/20... Train Loss: 0.437109... Val Loss: 0.436803\n",
      "Validation loss decreased (0.482202 --> 0.436803).  Saving model ...\n",
      "Using my_RNN_0117.pt to save\n",
      "Step: 225\n",
      "Epoch: 3/20... Train Loss: 0.432271... Val Loss: 0.404911\n",
      "Validation loss decreased (0.436803 --> 0.404911).  Saving model ...\n",
      "Using my_RNN_0117.pt to save\n",
      "Changing the number of intermediate evals at epoch 4\n",
      "225\n",
      "[37, 75, 112, 150, 187]\n",
      "Step: 37\n",
      "Epoch: 4/20... Train Loss: 0.360027... Val Loss: 0.410805\n",
      "Step: 75\n",
      "Epoch: 4/20... Train Loss: 0.370358... Val Loss: 0.409808\n",
      "Step: 112\n",
      "Epoch: 4/20... Train Loss: 0.366308... Val Loss: 0.378850\n",
      "Validation loss decreased (0.404911 --> 0.378850).  Saving model ...\n",
      "Using my_RNN_0117.pt to save\n",
      "Step: 150\n",
      "Epoch: 4/20... Train Loss: 0.362900... Val Loss: 0.384580\n",
      "Step: 187\n",
      "Epoch: 4/20... Train Loss: 0.363887... Val Loss: 0.373325\n",
      "Validation loss decreased (0.378850 --> 0.373325).  Saving model ...\n",
      "Using my_RNN_0117.pt to save\n",
      "Step: 225\n",
      "Epoch: 4/20... Train Loss: 0.361255... Val Loss: 0.385064\n",
      "Step: 37\n",
      "Epoch: 5/20... Train Loss: 0.320023... Val Loss: 0.371724\n",
      "Validation loss decreased (0.373325 --> 0.371724).  Saving model ...\n",
      "Using my_RNN_0117.pt to save\n",
      "Step: 75\n",
      "Epoch: 5/20... Train Loss: 0.320819... Val Loss: 0.400556\n",
      "Step: 112\n",
      "Epoch: 5/20... Train Loss: 0.327509... Val Loss: 0.353735\n",
      "Validation loss decreased (0.371724 --> 0.353735).  Saving model ...\n",
      "Using my_RNN_0117.pt to save\n",
      "Step: 150\n",
      "Epoch: 5/20... Train Loss: 0.321980... Val Loss: 0.346903\n",
      "Validation loss decreased (0.353735 --> 0.346903).  Saving model ...\n",
      "Using my_RNN_0117.pt to save\n",
      "Step: 187\n",
      "Epoch: 5/20... Train Loss: 0.320504... Val Loss: 0.359602\n",
      "Step: 225\n",
      "Epoch: 5/20... Train Loss: 0.325206... Val Loss: 0.381330\n",
      "Step: 37\n",
      "Epoch: 6/20... Train Loss: 0.304347... Val Loss: 0.391144\n",
      "Step: 75\n",
      "Epoch: 6/20... Train Loss: 0.298015... Val Loss: 0.351636\n",
      "Step: 112\n",
      "Epoch: 6/20... Train Loss: 0.292703... Val Loss: 0.351322\n",
      "Step: 150\n",
      "Epoch: 6/20... Train Loss: 0.297157... Val Loss: 0.365091\n",
      "Step: 187\n",
      "Epoch: 6/20... Train Loss: 0.298918... Val Loss: 0.331469\n",
      "Validation loss decreased (0.346903 --> 0.331469).  Saving model ...\n",
      "Using my_RNN_0117.pt to save\n",
      "Step: 225\n",
      "Epoch: 6/20... Train Loss: 0.296693... Val Loss: 0.328203\n",
      "Validation loss decreased (0.331469 --> 0.328203).  Saving model ...\n",
      "Using my_RNN_0117.pt to save\n",
      "Changing the number of intermediate evals at epoch 7\n",
      "225\n",
      "[22, 45, 67, 90, 112, 135, 157, 180, 202]\n",
      "Step: 22\n",
      "Epoch: 7/20... Train Loss: 0.264392... Val Loss: 0.328812\n",
      "Step: 45\n",
      "Epoch: 7/20... Train Loss: 0.246611... Val Loss: 0.350414\n",
      "Step: 67\n",
      "Epoch: 7/20... Train Loss: 0.245479... Val Loss: 0.356137\n",
      "Step: 90\n",
      "Epoch: 7/20... Train Loss: 0.246641... Val Loss: 0.330871\n",
      "Step: 112\n",
      "Epoch: 7/20... Train Loss: 0.250933... Val Loss: 0.335584\n",
      "Step: 135\n",
      "Epoch: 7/20... Train Loss: 0.254479... Val Loss: 0.337920\n",
      "Step: 157\n",
      "Epoch: 7/20... Train Loss: 0.255753... Val Loss: 0.322037\n",
      "Validation loss decreased (0.328203 --> 0.322037).  Saving model ...\n",
      "Using my_RNN_0117.pt to save\n",
      "Step: 180\n",
      "Epoch: 7/20... Train Loss: 0.256646... Val Loss: 0.334712\n",
      "Step: 202\n",
      "Epoch: 7/20... Train Loss: 0.257585... Val Loss: 0.331143\n",
      "Step: 225\n",
      "Epoch: 7/20... Train Loss: 0.260042... Val Loss: 0.332307\n",
      "Step: 22\n",
      "Epoch: 8/20... Train Loss: 0.206496... Val Loss: 0.358098\n",
      "Step: 45\n",
      "Epoch: 8/20... Train Loss: 0.220725... Val Loss: 0.336538\n",
      "Step: 67\n",
      "Epoch: 8/20... Train Loss: 0.215955... Val Loss: 0.366157\n",
      "Step: 90\n",
      "Epoch: 8/20... Train Loss: 0.222116... Val Loss: 0.369188\n",
      "Step: 112\n",
      "Epoch: 8/20... Train Loss: 0.225693... Val Loss: 0.360442\n",
      "Step: 135\n",
      "Epoch: 8/20... Train Loss: 0.230769... Val Loss: 0.339554\n",
      "Step: 157\n",
      "Epoch: 8/20... Train Loss: 0.233653... Val Loss: 0.325990\n",
      "Step: 180\n",
      "Epoch: 8/20... Train Loss: 0.236211... Val Loss: 0.304077\n",
      "Validation loss decreased (0.322037 --> 0.304077).  Saving model ...\n",
      "Using my_RNN_0117.pt to save\n",
      "Step: 202\n",
      "Epoch: 8/20... Train Loss: 0.235395... Val Loss: 0.330077\n",
      "Step: 225\n",
      "Epoch: 8/20... Train Loss: 0.239375... Val Loss: 0.299983\n",
      "Validation loss decreased (0.304077 --> 0.299983).  Saving model ...\n",
      "Using my_RNN_0117.pt to save\n",
      "Step: 22\n",
      "Epoch: 9/20... Train Loss: 0.192604... Val Loss: 0.315887\n",
      "Step: 45\n",
      "Epoch: 9/20... Train Loss: 0.195774... Val Loss: 0.339866\n",
      "Step: 67\n",
      "Epoch: 9/20... Train Loss: 0.203813... Val Loss: 0.322637\n",
      "Step: 90\n",
      "Epoch: 9/20... Train Loss: 0.211507... Val Loss: 0.321678\n",
      "Step: 112\n",
      "Epoch: 9/20... Train Loss: 0.213545... Val Loss: 0.311871\n",
      "Step: 135\n",
      "Epoch: 9/20... Train Loss: 0.212577... Val Loss: 0.372569\n",
      "Step: 157\n",
      "Epoch: 9/20... Train Loss: 0.218379... Val Loss: 0.363139\n",
      "Step: 180\n",
      "Epoch: 9/20... Train Loss: 0.218924... Val Loss: 0.336508\n",
      "Step: 202\n",
      "Epoch: 9/20... Train Loss: 0.217615... Val Loss: 0.319689\n",
      "Step: 225\n",
      "Epoch: 9/20... Train Loss: 0.216732... Val Loss: 0.331937\n",
      "Step: 22\n",
      "Epoch: 10/20... Train Loss: 0.157431... Val Loss: 0.373830\n",
      "Step: 45\n",
      "Epoch: 10/20... Train Loss: 0.163846... Val Loss: 0.351642\n",
      "Step: 67\n",
      "Epoch: 10/20... Train Loss: 0.172951... Val Loss: 0.324194\n",
      "Step: 90\n",
      "Epoch: 10/20... Train Loss: 0.178112... Val Loss: 0.319868\n",
      "Step: 112\n",
      "Epoch: 10/20... Train Loss: 0.186110... Val Loss: 0.359203\n",
      "Step: 135\n",
      "Epoch: 10/20... Train Loss: 0.189657... Val Loss: 0.351136\n",
      "Step: 157\n",
      "Epoch: 10/20... Train Loss: 0.192299... Val Loss: 0.312983\n",
      "Step: 180\n",
      "Epoch: 10/20... Train Loss: 0.190840... Val Loss: 0.385210\n",
      "Step: 202\n",
      "Epoch: 10/20... Train Loss: 0.203676... Val Loss: 0.319278\n",
      "Step: 225\n",
      "Epoch: 10/20... Train Loss: 0.208344... Val Loss: 0.355073\n",
      "Step: 22\n",
      "Epoch: 11/20... Train Loss: 0.159455... Val Loss: 0.356297\n",
      "Step: 45\n",
      "Epoch: 11/20... Train Loss: 0.157981... Val Loss: 0.367472\n",
      "Step: 67\n",
      "Epoch: 11/20... Train Loss: 0.162731... Val Loss: 0.331070\n",
      "Step: 90\n",
      "Epoch: 11/20... Train Loss: 0.169218... Val Loss: 0.356949\n",
      "Step: 112\n",
      "Epoch: 11/20... Train Loss: 0.173628... Val Loss: 0.364777\n",
      "Step: 135\n",
      "Epoch: 11/20... Train Loss: 0.173147... Val Loss: 0.357795\n",
      "Stopping optimization... DONE!\n",
      "...DONE\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "\n",
    "epochs = 20 \n",
    "patience=25\n",
    "missteps = 0\n",
    "init_epoch = 1\n",
    "\n",
    "train_loader = train_loader_100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "eval_schedule = [1,4,7]\n",
    "inter_evals_list = [1,5,9]\n",
    "inter_evals = 9\n",
    "eval_list = [int(len(train_loader)/(inter_evals+1)*ii) for ii in range(1,inter_evals+1)]\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "# train for some number of epochs\n",
    "for e in range(init_epoch, init_epoch+epochs):\n",
    "    if missteps > patience:\n",
    "        print('...DONE')\n",
    "        break\n",
    "    if e in eval_schedule:\n",
    "        print('Changing the number of intermediate evals at epoch',e)\n",
    "        idx = eval_schedule.index(e)\n",
    "        #train_loader = loader_list[idx]\n",
    "        inter_evals = inter_evals_list[idx]\n",
    "        print(len(train_loader))\n",
    "        eval_list = [int(len(train_loader)/(inter_evals+1)*ii) for ii in range(1,inter_evals+1)]\n",
    "        print(eval_list)\n",
    "        \n",
    "    net.train()\n",
    "    # batch loop\n",
    "    train_loss = 0.0\n",
    "    train_size = 0\n",
    "    step = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        net.train()\n",
    "        step+=1\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        #Number of items in the batch\n",
    "        train_size += inputs.size(0)\n",
    "        '''# initialize hidden state\n",
    "        h = net.init_hidden(inputs.size(0)).to(device)'''\n",
    "        \n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output and hidden state from the model\n",
    "        #output, h = net(inputs, h)\n",
    "        output = net(inputs)\n",
    "        \n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        if step in eval_list + [len(train_loader)]:\n",
    "            print('Step:', step)\n",
    "            \n",
    "            # Get validation loss\n",
    "            valid_loss = 0.0\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "                # move tensors to GPU if CUDA is available\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                '''# initialize hidden state\n",
    "                val_h = net.init_hidden(inputs.size(0)).to(device)'''\n",
    "                #Compute validation loss\n",
    "                #output, val_h = net(inputs, val_h)\n",
    "                output = net(inputs)\n",
    "                valid_loss += criterion(output.squeeze(), labels.float()).item()\n",
    "\n",
    "            # Print results\n",
    "            print(\"Epoch: {}/{}...\".format(e, epochs),\n",
    "                  \"Train Loss: {:.6f}...\".format(train_loss/train_size),\n",
    "                  \"Val Loss: {:.6f}\".format(valid_loss/len(valid_loader.dataset)))\n",
    "            if valid_loss <= valid_loss_min:\n",
    "                missteps = 0  \n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                valid_loss_min/len(valid_loader.dataset),\n",
    "                valid_loss/len(valid_loader.dataset)))\n",
    "                save_checkpoint(my_path, net, optimizer, epoch=e, train_loss=train_loss, valid_loss=valid_loss,\n",
    "                                words=net.words, word_to_int= net.word_to_int, params=net.get_params())\n",
    "                valid_loss_min = valid_loss\n",
    "            else:\n",
    "                missteps+=1\n",
    "                if missteps > patience:\n",
    "                    print('Stopping optimization... DONE!')\n",
    "                    break\n",
    "        \n",
    "else:\n",
    "    print('DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(27057, 128, padding_idx=0)\n",
       "  (gru): GRU(128, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
       "  (dropout): Dropout(p=0.3)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch, train_loss, valid_loss = load_checkpoint(my_path, net, optimizer)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Testing\n",
    "\n",
    "We will test or neural network in two ways\n",
    "\n",
    "* **Test data performance:** First, we'll see how our trained model performs on all of our defined test_data, above. We'll calculate the average loss and accuracy over the test data, and plot some parameters to assess the model.\n",
    "\n",
    "* **Inference on user-generated data:** Second, we'll see if we can input just one example review at a time (without a label), and see what the trained model predicts and if the results are the ones expected based on the sentiment we infer from the used sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3043\n",
      "Test accuracy: 0.8724\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_loss = 0.0 # track loss\n",
    "num_correct = 0\n",
    "test_probas = []\n",
    "net.eval()\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    '''# init hidden state\n",
    "    h = net.init_hidden(inputs.size(0)).to(device)'''\n",
    "\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output = net(inputs)\n",
    "    test_probas.extend(output.squeeze().detach().cpu().numpy())\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss += criterion(output.squeeze(), labels.float()).item()\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "# Convert test_probas to numpy array\n",
    "test_probas = np.array(test_probas)\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.4f}\".format(test_loss/len(test_loader.dataset)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's save the predictions,the test lables and the star-based rank of the reviews\n",
    "with open('Simple_GRU_probas.pkl','wb') as f:\n",
    "    pickle.dump(test_probas, f)\n",
    "with open('test_y.pkl','wb') as f:\n",
    "    pickle.dump(test_y, f)\n",
    "with open('test_stars.pkl','wb') as f:\n",
    "    pickle.dump(test_stars, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred_1</th>\n",
       "      <th>Pred_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Label_1</th>\n",
       "      <td>11051</td>\n",
       "      <td>1449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label_0</th>\n",
       "      <td>1742</td>\n",
       "      <td>10758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Pred_1  Pred_0\n",
       "Label_1   11051    1449\n",
       "Label_0    1742   10758"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cm = pd.DataFrame(cm(test_y, test_probas>=0.5, labels=[1,0]), index=['Label_1', 'Label_0'], columns=['Pred_1','Pred_0'] )\n",
    "test_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the errors are due to negative reviews misclassified as positive reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's provide some metrics for the classification focused on the possitive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 0.8724\n",
      "Precision of the model for positive reviews: 0.8638\n",
      "Recall of the model for positive reviews: 0.8841\n",
      "f1-score of the model for positive reviews: 0.8738\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy of the model: {accuracy_score(test_y, test_probas>=0.5):.4f}')\n",
    "print(f'Precision of the model for positive reviews: {precision_score(test_y, test_probas>=0.5, pos_label=1):.4f}')\n",
    "print(f'Recall of the model for positive reviews: {recall_score(test_y, test_probas>=0.5, pos_label=1):.4f}')\n",
    "print(f'f1-score of the model for positive reviews: {f1_score(test_y, test_probas>=0.5, pos_label=1):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's provide some metrics for the classification focused on the negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 0.8724\n",
      "Precision of the model for negative reviews: 0.8813\n",
      "Recall of the model for negative reviews: 0.8606\n",
      "f1-score of the model for negative reviews: 0.8708\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy of the model: {accuracy_score(test_y, test_probas>=0.5):.4f}')\n",
    "print(f'Precision of the model for negative reviews: {precision_score(test_y, test_probas>=0.5, pos_label=0):.4f}')\n",
    "print(f'Recall of the model for negative reviews: {recall_score(test_y, test_probas>=0.5, pos_label=0):.4f}')\n",
    "print(f'f1-score of the model for negative reviews: {f1_score(test_y, test_probas>=0.5, pos_label=0):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the precision-recall curves for positive and negative reviews look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positive reviews\n",
    "pos_pre_rec = pd.DataFrame({name: values for values, name in \n",
    "                            zip(precision_recall_curve(test_y, test_probas, pos_label=1)[:2],['precision','recall'])})\n",
    "#Negative reviews\n",
    "neg_pre_rec = pd.DataFrame({name: values for values, name in \n",
    "                            zip(precision_recall_curve(test_y, 1-test_probas, pos_label=0)[:2],['precision','recall'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmYAAAFMCAYAAABh83BHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzs3XmcVNWd///XXWrrru7qhW4aGhoQWQKKCO6YURFE1O8kMzEDuCVOHjpJnMQtM5P4UNFEjJOoSdyyGJeMoKL+UGOiYGLcoo0I4kIjsoqsTe/Qa3XVvb8/CloIAkV3V9et4v18PHhIbfd+DtiHd51z7rmG67ouIiIiIpJ2ZroLEBEREZEEBTMRERERj1AwExEREfEIBTMRERERj1AwExEREfEIBTMRERERj1Awk32MGjWKqVOncu655zJt2jS+9rWvUVlZ2e3j3XXXXTzxxBMAvPnmm2zdunW/53vD66+/zowZM5g2bRpnn3023/nOd1i3bh0ACxYs4Jvf/GavnUtEvGXUqFHccMMN+zz3zjvvcOmll6bkfNFolOeeew6A6upqLrjggl479q5du/jxj3/MOeecw7Rp0zjvvPN4+OGH2bOz1eTJk1m6dGmvnU+8x053AeI9jz32GGVlZQAsW7aM73znOyxcuJCioqLDPtb111/f9ftHH32U73znOwwcOHCf53vqtdde48Ybb+RXv/oVEydOxHVdnnrqKS666CJefPHFXjuPiHjXu+++y8qVKxkzZkzKz7Vy5Uqee+45vvrVr9K/f3/+9Kc/9cpxHcfhiiuuYPjw4bzwwgsEAgG2b9/OVVddRVNTE9dee22vnEe8TcFMDmrixIlUVFSwfPlyzj77bF566SXuv/9+YrEYpaWl3HbbbVRUVLB69Wpuuukmmpub6ezs5LLLLuOSSy7hhz/8IRUVFUSjURYvXsz69ev5r//6L9544w0qKipobm6mo6ODm266CYD6+nomT57Mm2++SXV1Nbfccgs1NTX4/X5uv/12jj322P1qvPfee/ne977HxIkTATAMgxkzZtC/f38CgcA+762treV//ud/2LJlC9FolEsvvZTLL78cgLlz5zJv3jxc1yUcDvPTn/6UESNGHPB5EfGO6667jttvv525c+fu95rrutx///288MILRKNRzj77bH70ox9hWRZVVVVdgeef//mfWbRoETfeeCMnn3wyTz/9NA8//DDxeJySkhJ+9rOfEQgE+M///E+am5u56KKL+NnPfsY555zD4sWLOf3003nttde6vsTOmTOHQCDA9ddff8Dz7+2NN96gurqaxx57DJ/PB0BZWRm/+MUvaGxs3K9dX1RfeXk51dXV/Pd//zc1NTVEo1HOP/98rr322gM+L96iqUw5pFgsht/vZ+vWrdx0003cf//9LFy4kDPPPJObb74ZgPvuu4+ZM2fy5z//mSeffJK3336baDTadYxrrrmG/v378/Of/5zzzjuv6/lzzz2XV199tevxq6++yimnnEJubi5XXXUVX/nKV1i0aBG33HIL3/3ud4nFYvvU1traSlVVFWeeeeZ+dZ955pmEw+F9nvv1r3/NoEGDWLhwIX/4wx+466672LZtG83NzfzqV7/i6aefZuHChXzrW9/itddeO+DzIuIt06dPx3VdFi5cuN9rzz//PAsXLuSZZ57hL3/5C5s2bepaSnHTTTfxzW9+k5dffplwOMynn34KQF1dHT/+8Y955JFHePnll6moqOCBBx6gX79+XHfddYwfP57HH3+86xz5+fmcfPLJ+/Rnr7zyCtOnTz/o+fe2ZMkSJk2a1BXK9qioqGDcuHH7PHeg+iAxO3HiiSfy4osv8sILL7Bp0yZ27NhxwOfFWxTM5KBef/11amtrmTBhAm+99RYnn3wyQ4YMAeDrX/8677zzDrFYjOLiYhYtWkRVVRWFhYU88MAD+P3+Qx5/3LhxuK7LqlWrAPjLX/7C9OnTWb9+PXV1dVx44YVAYuSuqKiI5cuX7/P5nTt34rouxcXFSbXnxhtv7BqdGzx4MCUlJWzevJlAIIBhGDzzzDPU1tYyffp0rrjiigM+LyLec8MNN3DnnXfS0dGxz/OvvvoqX/va18jLy8O2bb7+9a/z8ssv097eTlVVVdcasYsvvrhrLVdxcTHLli3rWtZxwgknsGnTpoOef9q0afztb38DoKqqCtu2GTt27AHP/4+ampqS7ssOVl9xcTF///vfWbp0KX6/n7vvvpvS0tIDPi/eoqlM2c+ll16KZVm4rkt5eTkPPvggubm5NDQ0kJ+f3/W+vLw8XNeloaGBH/zgB/z2t7/lmmuuoaOjg//4j//g4osvTup855xzDq+88goVFRW899573HnnnaxevZr29namT5/e9b7m5ub9hvMjkQimaVJdXU15efkhz/XRRx91jZKZpklNTQ2O4+Dz+Xj00Uf5zW9+w7333suoUaOYPXs2o0aNOuDzIuItY8eO5cQTT+SRRx7h+OOP73p+165dPPTQQ8yfPx+AeDxOUVERTU1NGIbR1a/5fL6uYBSPx7nnnnv429/+Rjwep6WlhWHDhh30/FOmTOGOO+6go6ODv/71r13914HO/48KCwuTHsE6WH3f/OY3cRyHW2+9lR07dnDxxRfzve9974DPG4aR1DmlbyiYyX72Xvy/t+Li4n1GrJqamjBNk8LCQmzb5rrrruO6667jww8/5IorruC0005L6nzTpk1jzpw5jBgxghNPPJFwOExpaSm5ublfOC2xt1AoxLhx43j55Ze71ort8eijjzJ58uR9nvuv//ovvvGNbzBr1iwMw+DLX/5y12tjxozhnnvuIRqN8vvf/57Zs2fz5JNPHvB5EfGea6+9ln/9139l0KBBXc+VlpYyefJkLrnkkn3e29LSguu6tLW1EQqFiMVi1NfXA/Diiy/yt7/9jblz51JUVMRTTz3FCy+8cNBzFxQUMG7cOCorK/nrX//Kz3/+84Oe/x+dfPLJ/PCHP6S9vZ1gMNj1/GeffcYrr7yyTx93sPps2+bKK6/kyiuvZMOGDVxxxRVMnDiRSZMmHfB58Q5NZUrSJk2axNKlS7uGy5988kkmTZqEbdt8+9vfZs2aNQCMHDmScDi837cw27bZtWvXfsc9/vjjqaurY8GCBV3fMMvLyykrK+sKZvX19Vx33XW0trbu9/mrr76a3/zmN7zxxhtAYqHv448/zh/+8Afy8vL2eW9dXR3HHHMMhmHw7LPP0tbWRmtrK5988gnf//73iUaj+P3+rvcc6HkR8abS0lIuvvhi7r333q7nzj77bJ5//nna2tqARN/17LPPkpuby/Dhw3nppZcAmD9/ftfPd11dHeXl5RQVFdHQ0MBLL71ES0sLkOjLmpubu6Y99zZt2jSeeuopOjs7GT169EHP/49OP/10jjrqKP77v/+b5uZmALZv384111yz3/rag9V3880389ZbbwGJ9Wn9+vXDMIwDPi/eohEzSVpZWRm33XYb3/3ud+ns7GTQoEH85Cc/AeCSSy7h+uuvp7OzE4CLLrqIoUOH7vP5adOmcd111/H9739/n+cNw2DKlCk8/fTT3HXXXV3P3X333dxyyy388pe/xDRNLr/8cnJycvar67TTTuPuu+/mnnvu4Sc/+QmWZTF27FjmzZtHYWHhPu+9+uqrueqqqygoKGDmzJnMmDGDm266iccff5xBgwZxwQUX4PP5yM3N5eabb2bkyJFf+LyIeNe///u/8/TTT3c9njJlCmvWrOFf/uVfgEQomTNnDgCzZ8/mpptu4qGHHura/sIwDC644AL+/Oc/M3XqVAYPHsw111zDd77zHe644w4uvfRS7rzzTr785S/vcwEAwNSpU7n11lu58sorkzr/3gzD4De/+Q2/+MUv+OpXv4pt24RCIS6++OKu9bZ7HKy+mTNncvPNN/OTn/wE13WZPHkyp556KgUFBV/4vHiL4X5R5BcRETlCuK7bNXJ0yimn8Oijj3aNdon0NU1liojIEev73/8+Dz74IACVlZW4rrvfaL9IX0ppMFu9ejVTpkz5wg3/3n77bS688EJmzJjB/fffn8oyREQOm/qvI8PVV1/NX//6166LkH72s5/ts/BepK+lbI1Za2srP/nJTw44f33bbbfx0EMP0b9/fy655BKmTZvG0UcfnapyRESSpv7ryDF8+HCeeuqpdJch0iVlI2Z+v58HH3zwCzev27RpE5FIhAEDBmCaJmeccUaPbpQtItKb1H+JSLqkLJjZtn3A4eCampp9NtcrKiqipqYmVaWIiBwW9V8iki4Zs13G2vq1HF10NK7rsr5hPSt2rGDTzk00tDUQd+NA4sqauBvHwOCy4y4jx5dDY3sjOzt2siu6i4pIBf1z+9PpdNLW2UZZuIyAve9Nrl3XpSPeQVtnG51OJ3EnjuM6+CwfjusQd+IE7SAFwQIs0/qiUkVE9uP+539iWBa0tkJHBwwbBqEQtLfvfoMLjgPRKBgG/GMwNE3w+RKv730xfW4uxGLQ2Zn4PCQ+H4mAbX9+HJ8vcQzH+fxXbi6Ew4nfu+7nn8vN/fy9ppk4Rk4OWFbiuXj881+mmTiPZSXeZxiJY7lu4vm9b8229+uWlfjV9Qe0+/wiR7i0BLPS0lJqa2u7HldXVx/yfl31bfVs2lbDH9c9y6dNG2jtbKUj3k7QDuIz/diGhWnaVNV+iG36+OXO+8jx5eC4Dm2xNqLxKJFABMMwMPj8h980TAJ2kBw7h7ZYG+2xNhzX2efcjutgGonBRZdEh+g3/RSFisn15RJzYnQ6nTiuQ8gOUZrTn4AVwGf68Jm+ruMZhoFlWBQVhWloaCXHl0PICmGbPhwc8v352IZN3I0Td+P4TT8+a9+b2XpNSUkeNTX7bxqbabKlHZB9bfGa7vRfAMYZZ9DU4WKtX4e99B3cz7bghkK4Pj+ubYNlgu3DrGuEWAw3Ly8ReqKdmLU1uIWFuJYFhgW2idnQAJaNGwzgGkbiecvAiMWgrQ1sG6OtDdeywXEwXCcRlHaHJhcwHAc3GNz9HOA6GBi4Ph/khKAjCq6TeGxaEAiQm+Onpbn98/AFieN2h2nucxw3GEoEyD0h1XEAF/x7/owSQc/1+RL/DeXghsOJNkLiz9BM/HLz8hN/vpEIbihn3xC4W7b9rGRDW7KlHdD9/istwWzQoEE0NzezefNmysrKePXVV7nzzjsP+blFn77I+sZ1xN04R0WGU5Jb2hWY9hgaGUpVbRVxNwauS8AKkOfPp6mjkfr2OkJ2iKAVoq69lrbOVizTxjRMgnYI27CwTB+2aWIbvt372rjEHAeIYxkmLi7t8Xa2tG9iQ9N6AlYAx3UwDAPHjeNi4DNtfKYfx43jM324JMKdZVjEnBiRvFw62mIYhknACnxRU7sUh/pRFCzCNm0igQIi/ggAATtIwAoQtIL4LD8FgQJsM2MGQEUyVnf7rz3iQ4fhRAoSwWvv0aQ9r485pjfL/VwslhiR2vMLMFpaEuHHMHDNRKgzWloSwc51d4+axTHa2yHa2TWq5dp24himAbE4GAbG7hE71+8DB3BdjGhH4hiWmRhdiyVmN7AtjGgUt+uLsosbdzBraxLBLFFd4j+OC87uXe/jcXBcDNfBtX2JTwb8YPs+D3GGCfFYIrwFQ11tdYPBRFjLzcXNj+CGwzByKIadm3iPZSUCnM/bX4Yl+6XsX/IVK1bwv//7v2zZsgXbtlm0aBGTJ09m0KBBTJ06lVtuuYXrr78egPPOO++QN4etb6vnk/pVRJ1OTuh/4gGnEfP8+ZwyMPmdjPfsr9ud21JE41FMw8QyrK7Pt8XaaGxvxMUhGo8Sc2KYhpk4jwGOGyc3J0BTvIUOp4Oo04mJQdSJEot3Jkb/LBtwaY4289nOzwjaga5glx/IpyPWgWGY+Ewflmklzo9Brj+M48YpCBRSGCzEcZ3ENKzpJ8eXk/gmjEuOnYOxO2T6TJvCYBFFgUT4C1hBbNPWbTrkiNbb/dc+TBN3942y+9QXjGq54fC+jwE3UnDw40RCxJvaerGwbojFIBbDaGvFaG39fBp0zyhbLIbR2pIIlLEYOA5mQz3GhvWJz9t2YpQxL4eQf/fdRPb0eXtG3Hz+xHO2lRjVLC7Gyc/HKemPG4nglA/aHU61Haj0rozZ+f93y37Hx1vXUBAoZHhhZl+WHskP0bQzuY7NcRMBr9PppL6tjk63E8M1aI+10eF0EHNixN04zdFdBO0QBtDU0UjQDnWFThcXy7ASAREX07Cwd4fJuBvHZ/gJ+UJd5/RbfgJWEHP31Ktl2lhGIgjvGYE0DIOA5WdI/4GY7SHy/Pnk+/PxW/uPAGSCbBs+z6a2ZI2nn6apIyO624OKREI0pTuY9UQ0itHejtHWSl7ddppbowAYrgvRjsTvHQfiTmL6Mx6HWGdiRND24fr9iVG6UCLQOaWlOMUluEVFuLYvMbWam4vTrySxfq+PZMvPfba0AzJsKrM7qluq2RXdyZjiFA3ze1RimjVIkCB5/uT+kjviHThOHMMwMQ2TaDxKNN6BYRi4sNc6OoNOJ0pjWwNt8TZMTDrjUTriHVimjbP7ooqYE9/9ZdIg7sR2hzUL13XJqQ7gd3K6gluuL0yuL5dIIEIkUIC7e32dufszeb48IsECcndPH+TaOfgsPz7Tt9+0tIhIr/P7E+EqPx9GDktu9M91oa0No60Vs64WY0c1NDdjdEaxt2xOjEb6fLiOA6aVOLZtJ6ZPc3Nxi4oT6wNNC/w+3HAeTkEhTlkZbl6+pk9lHxkTzJo7mnFw9xnZkS8WsAKw10xvYhTrIN/cIskfuzPeSdTpwHWh04kSs9vYXLedjlg7rbFW6tpqExcvOPGuK14d18XaHboSo3U2uf7c/WouDvWjX6iEsD9Mjp3DyMJR5PrCmlYVkfQyDMjJwc3JIV7cD0budR9Nx0lMp7Y0Y3R2YuzaBR1tmI2NGI6D0daaCG2GmViLt+fii5wc3FAILAunoBC3tBTX9kEoiBsM4RQW4RT3S4S8kP7dO5JkTjDrbMZvZuY0WTbxWb59rhSN5Icoscv3e19ztJmoE/38atbdU7LN0V3siu5MrL0zLdpjbeBCdct2Nuxcj+tArj+HjliUPH8ekUCEkpzSrnV6QTtIYbCQ8vBgCoNFBCw/tunbvW5OAU5E+phpJtbqhcPsPVEd3/Mbx0lsZeK6EI9jxOMYzbswamswWpohFsfevOnzLUcsC+IOrmUljhsI4OblJa4wDeclAlvZgMRIXG4Yt1+/NDRaUiljglk0FiVk9d18vfRM2P/Ff1f9c8sO+BnHdWjqaCTmxGhob2Bby1a2NG9m465PMTGxzMRVrY7jkOsLEwlGurY+KQwWkufPJ2Qn1ruV5JRSGiqlNKe/ApuIpI9pQuDzq+9dSIyCDdzrC+2e/evi8cSoW3sbRkMDxs4mjNraRIjr6OjaA84N5eAWFCQubPAHiA8bBl86GrvTwCkqxs3Px82P6MKEDJUxwQzA0BqkrGYaJoXBxI7qJTmljCwaBSSunN0Trtpj7eyMNlHdvI3WeBsGJh2xdjbu/JSgldhI0zItfKYfv5W4GjXHziXsD1MULCbXl8PQyFGU5vQn7FPQFxEP2LOJL7uDGwVQNmDf97gudHZiNDZi1tVARwdmWxs01OPbuAE+/pBARywxHZqTA7ad2JYlHN69n1sB8YohOIMGa02bx2VWMEvdHaTEw/Ye8QraQYJ2kNKc/vu9z3EdOuIdtHS2UN9WR3XLNpriUerdelpjLdiGDwzI3R3U8vz5DAwPJN8fYURsCB3NLgNyy8nx5fRl80REDs0wEhculJYS372hcRwS+8XtbCJku8S312I0Nia2EWlpwdq8GQM3cdGBE09MfYZCiX3cIhHi5YNwSvvjDBiIW1SkOy94REYFMwv9TyMHZhomITtEyA7RL9Sva8QNIObEiMaj7Iw2sa15K00dif+uqv+YgBXgte0+4lGDsC/MgNwBFIWK8Zl+fKZNaW4ZEX+EXF+YgoBuxSUiHmIYib3nIiGc3ML9X4/FoDOKWV+PuW0LRmNDYo3bmk+wlyz+/O4JBQU4Q4bSecppOP0PvOREUi+jgpmmMqW7bNPGNm1yfDmU5SamCFzXpTXWSluslQ6zmR2N9dS31bFp12cErMDujYNNbNMmz5+HgUHIDlGRP4RBeYPJ90c4KjLc87fNEpEjmG0npjXLcxKb4u4Ri2E0NmDW1mBWV2OuXYO1sgr7vWXEKypwSkpx+5UQHzQYZ8BATX/2oYwJZo7rYOmWQ9KLDMMg15eb2Hctv4LyQGI/oz33Pk2MsnVQ01pDW6yFtlgr21q2sr5pHZZhYxsWhaEiBudVMDQyrOuWWZFAhDx/vvZlExHvsu1E8OpXQnz0GHBdzO3bsD5Zhf3+8t1BzMDZfcVpfHAFTsWQxBWi4TDx/gMSN7uXXpcxSSexe73+oZPU2zO6lpBHcWjfy9Gbo82JNWyt26lrq2XTzs/4YMdyAlYQv+UnaAexDIuy3AEE7SBDI8MYW3wsQTvY940REUmGYeAMGJgYHdu9oW5iNG07xo5q/J9txF2yGDcQxM0JQSBIbOyxxIcfTXxwRZ/e5SDbZU4wc11MrTETDwj7w4T9YSoiQ4DEnRYa2xto7GikObqT1lgrzR272LjzUwBe/+xVBoQHMiQylDHFY+mfU0ZhsEhBTUS8afeGuk7FEJyKRD9HeztmYwM0N2M278LcthX/hnW4Rf1wioqITjuP+NhjtEVHL8icYIZL5t9lTrJRwArQP7dsvz3aHNehpbOFT5vWs2XXZjbu/JSl25ZQHOqHbdkEzADDC0cwNH8oYX8excFi8gOHcRsGEZG+Egzi7N7CwwE47niM2hqszz7FXrkCc/t2nEGDcAaWEztmHPGRoxLr2+SwZcyfmouLqSQuGcQ0TPL8eRxbchzHlhxHa2crm3ZtZGfHTho7GmnvbGNl7QoKQkUUBApwXZdhBUcxof8JjCocrY1xRcS7DAO3pJRYSSmxLx2DXfUR5pbNWKs+xl6yGGdwBfGRo4kfPYL40GG6eOAwZEwwSwyX6R8qyVw5vhxGFX2p67HrujR1NLKjdQc7o400d7bw+qZXWbptCRPLTmRc6XiGRY7SRrgi4m05OcROPBkAY9dOzHVrsVZ9jLVubeLWUcX96Dx1Ep0nnLTPXRDki2VMMHNcB8vQ/lGSPQzDoCBYSEHw872HmjoaWbb9XV7f9CrLq5eRHyhgYtkJHNtvHIPyButKTxHxNDcvn/j4CcTHjcdoasRavw7z0w0E1q3FfutNYpO+TOeEE3Rj9oPImGDm4mq8TLJeJFDAWRVT2BltYmvzVtY2rOaFtc/xztZK+uf255SBp3Fsv+PwW/50lyoicmCmiVtYRGxiERx7HNbqT7A+fB9z82Z8lW8TO+ZYOo+fiNt//7u4HOkyJpgBmBoxkyOAYRiJPdECBYwqGs2O1mo2NK1nefV7rK5fTXm4nOP7T2Bk0Wgq8oZoLZqIeJvfT/yYY4mP/lIioG1Yh3/NJ9iVbxE/fkJi241hR+ligd0y5k/BdV1N48gRxzRMynIHUJY7gLbSNtY1rGFd4xo+3bmBSKCAgeFyxvYby4TSE/aZEhUR8RzbJj5mLPHRX8LcvAn7w/cxX/wT9lt/xykfRPzYY+Er56W7yrTLnGCGq7X/ckQL2SGOKRnHmH7HUN2ynQ1N6/i4roqP66p4Y9PrHFc6nvGlEygqPibdpYqIHJhp4lQMITq4AqOxAWv9OqzVq7BWrYSq9wmUDCR23PHEvzTmiLyxeuYEM9fFRCNmIqZhMiA8kAHhgcSdOFuaN7OybgUvrHme5dXvUVl7NBMLT2Nk4ShNc4qIdxnG5+vQxk/AWvMJbPkM38ersRdX4owYQecJJxMbfzwEj5wNuTMmmAGayhT5B5ZpUZE/hMF5FTS017Oy9iNe3fAqyza+z6D8wRxdOJIJpRMZGC5XSBMR77KsxD07T55I5+YdWCs/wn5vKdaaNcQr/07nqacTmzAR/Nl/4VNGBTP9wyLyxQzDoChUzOmDzyTma2XpxvdZ37SelbVVLN76NkPzh/HlwWcwsnCUvuCIiKe5eXnETj6N2LHjsdavxf64CmvNJ8Tfe5eOc8/HOWp4uktMqYwJZontMvQPisihFIeKOWXgabiuS3XrdtY0rObtrX9nXeMahhUcxXlH/T8G51Wku0wRkYPLySF+zDjiI0djf/QB9rKlmGvW0Dl1Gp2nTcItyM4LnjInmLkupkbMRJJmGEbXFZ1NHU18XLeCt7b8nU/qPuGsIZM5ZcAkSnJK0l2miMjB+f3EJp5I/Kjh+Crfwv/sM9jvvkPHrEuIjxiZdRcIZNgQVHb94Yv0lUggwikDJzFt2HnE3BgL17/I3Ut/xt+3vEFnvDPd5YmIHJJbWET03POJjz8ec+OnhH59H8GHfou1ZnW6S+tVmTNihkbMRHqqX6gf5x11AVtbtrB4SyWPr3yMtza9yamDJnHygFMJWLqPnYh4mGkSP3okTmkZ1ooPsN9ZjPXBB3R+9V+IfvnMrLg4QCNmIkcYwzAoDw/igqP/mWEFR7GuaS3zqv6P+5ffQ01rTbrLExE5JDc/n9hpX6ZzyjkYJvjnPUbo/l9hraxKd2k9ljEjZri6KlOkNwWsAONKxjO6aAwr61awdPsSGtrqOP/or3By2Sn6eRMRz3MLComecx7WJ6uwP/oAa8N62r/5LWITTkh3ad2WMSNmLm66SxDJSn7Lz/jSCZwz9Fxq2mt46IPf8sK652jtbE13aSIih2bbxMceQ/TcCzB27SL48IPYiyvTXVW3ZUwwE5HUGhAeyLSh55Hry+H5tQt48MPfsLKuCtfVlyIR8T43EiF6znSIxQg+/Dvs5cvSXVK3KJiJSJccXw7TjjqfLxWP5f0d7/Hr9+7l2TXP0NjekO7SREQOyS0qonPKNACCDz+I79VXIB5Pc1WHJ6OCmVa8iKSeaZgcW3IcFwz/CpZl8ce1z3LPsl+waddn6S5NROSQ3Lw8Os8+B9raCPzfwwSeegI6M2dboIwKZiLSd8L+MNOGnceJA05hVcNKfv/Bb9jRuiPdZYmIHJJbUED0K/+KWz4qskgTAAAgAElEQVQY/x+fI/jYoxhNjekuKykKZiJyQKZhcnThCP5p8Fmsb1rPA8vv4dOmDekuS0Tk0Px+Ok87nfjYY/C98jKBPzwM7e3pruqQFMxE5JCGRY7ijMFnsbZxDQ+8fx91bXXpLklE5NAsi9iJJxM/Zhy+t98i8Oc/gscvaFIwE5GkDM6vYFL56WzZtYmHPvytNqMVkYwRGz8BZ8hQ/C88j/+lP3s6nCmYiUjShkWGM77/8SzfsYxfv38PzdFd6S5JROTQTJPOUyfhFhYSmD8P3zve3ecsY4KZi4uh6zJF0sowDI7tdxwnDzyVFbUf8dBHv6OpIzMW1IrIES4UInrm2bg5OQQe+T3Wqo/TXdEXyphgJiLeYBgGo4vGcELZSSzeVsm8lY/RFmtLd1kiIocWDNJ51hSMaJTgfb/E3LY13RXtR8FMRA6bYRiMKxnPuJLjeH3T33h2zTM4rpPuskREDsmNFNB5xlmYDfUE7/0lRrO3lmQomIlItxiGwfjSCYwoHMmL617gna3eXbMhIrI3Z8BAYieegr12Nb6FL3rqYgAFMxHpNtMwOXHAKeT6w8xd+QeqW7anuyQRkaTER40mPmQYgT8+h7V2TbrL6aJgJiI9ErSDnFZ+OvXt9TxW9ahuei4imcE0iZ18CjgO/mfme2bz2cwKZoauyhTxorLcAXypeAzLa95jWfW76S5HRCQpbl4+8fETsJcvI/Ds/+eJKc3MCmYi4lnHlBxLyArx6IqHaWivT3c5IiJJiY09BndwBf6Ff8Z+/710l6NgJiK9I9cX5pTy06hp3cEf1z6rKU0RyQymSXTSP+FaFoFHfo/RkN4vlikNZrfffjszZsxg5syZfPjhh/u8Nm/ePGbMmMGsWbOYM2dOKssQkT4yKDyYowqO4uUNC/mo9oN0l9Mj6r9EjiC5ucROOx1zR3ViSjONUhbMlixZwsaNG5k/fz5z5szZp/Nqbm7moYceYt68eTzxxBOsW7eO999/P1WliEgfMQyDkwacit8O8PCHv6exvSHdJXWL+i+RI48zaDDOUcPxvfoK1rr0XaWZsmBWWVnJlClTABg+fDhNTU00NzcD4PP58Pl8tLa2EovFaGtrIxKJpKoUEelDOb4cTi2fxNaWLfxx3XMZOaWp/kvkCGQYxI47HuIx/POfgGg0LWXYqTpwbW0tY8eO7XpcVFRETU0N4XCYQCDAVVddxZQpUwgEApx//vkMGzbskMfMzwsSCYVSVXKfiuRnRzsge9qSLe2A9Lfl2LzRbG3fyN82L2JM+Qimj5ie1noOVyr6L4BIJDv+H8uWdoDa4kVpbUckBKefBpWVhN95Hf71X/u8hJQFs3+097fm5uZmfvvb37Jw4ULC4TDf+MY3WLVqFaNHjz7oMXbtasfuzPx78kXyQzTtzPx2QPa0JVvaAd5py8TiU6nd2ci9bz9AvlPCyKJRh32MkpK8FFR2+Hqj/wJoakr/30tPRSKhrGgHqC1e5Il2DB2Jf91GjIcfpWXw0ThDk/vi9Y+623+lbCqztLSU2trarsc7duygpKQEgHXr1jF48GCKiorw+/2ccMIJrFixIlWliEga+C0/Zww+i7gb5+lPnsioKU31XyJHMMui86SToa2dwGOPQmdnn54+ZcFs0qRJLFq0CICqqipKS0sJh8MAlJeXs27dOtp377K7YsUKhg4dmqpSRCRN8gP5fKl4DB/XrWRV/cfpLidp6r9EjmxuQSHxiSdgr6zCXta3m2anbCpzwoQJjB07lpkzZ2IYBrNnz2bBggXk5eUxdepUvvWtb3HZZZdhWRbHH388J5xwQqpKEZE0Gl00htX1q3is6lFunTQHn+VLd0mHpP5LRGKjx2B9vJLg3P+jtWxAt6c0D5fhZsj8wtfmf42zBp5DQbAw3aX0mFfWAPWGbGlLtrQDvNmWj2o+ZOn2JVwz8XpOGXha0p/zyhqzXvH00zR1ZER3e1CeWAPUS9QW7/FaO8wtm/H9ZSGxiSfRds31YCc/nuW5NWYiInsMLziaoB3kyVXz2LJrc7rLERFJilM+iPjYY7GXL8Vav65PzqlgJiIpl+PL4czBk9nevI3ndbsmEckg8RGjMOLxPltrpmAmIn1iQHggwwuPpnLr39mwc326yxERSYobieCU9sf39zf6ZNNZBTMR6TNj+h1D3Inz7rZ30l2KiEhyDIP4kKGYDfWYmzel/HQKZiLSZ4qCxYR8OXyw4wNNZ4pIxnBKy8Bx8L39VsrPlVnBzDDSXYGI9IBpmBxVMJyNOzewaddn6S5HRCQpbkkJ8cEV+P/2MkZ1dUrPlVnBTEQy3tDIMMDl7a1/T3cpIiLJMQzixx6H0dqK/+UXU3oqBTMR6VPFwX4UhYp4a8sbROOpX0grItIbnP5lOMX9sJcuhRQuxVAwE5E+ZRgGRxUcTW1rHe/vWJ7uckREkuYMHIRVV4NZvT1l51AwE5E+d1TkaEJ2kGc+eRLHddJdjohIUpz+/SEWw/x4ZcrOoWAmIn0uaAcZVTyGrS1b+HTnhnSXIyKSFKekFNfvx/9OZcrOkVHBTNdkimSPQXmDcF2X97YvS3cpIiLJCQZxhg7DqlqBuXVLSk6RUcFMRLJHUbCYHF8uS6vf0Z5mIpIx4sOGY3RGsRe/nZLjK5iJSFqYhkl5eBDVzdupb69PdzkiIklxygbgRAoSt2iKx3v9+ApmIpI2A8IDaI+3saoudQtpRUR6lWniHHU0VvX2lExnKpiJSNoUBosxTZu1jWvSXYqISNKcfv0gHsfcsL7Xj51hwUzL/0WySZ4/j4AVYF3j2nSXIiKSNKe4GCwb34qPev3YGRbMRCSbmIZJ/5z+bNr1Ge2x9nSXIyKSnGAINzcX89P14PTuXowKZiKSViU5/emId7B516Z0lyIikhzDID50GNaWzZjbtvbqoRXMRCStBuQOwHEdlmxfnO5SRESS5pQNABesj6t69bgKZiKSVoXBIvIDET6seT/dpYiIJM0pKcW1bXxL3+3V4yqYiUhaGYZBaU4p1S3VtHS2pLscEZHk+P045YOwVq/C2NnUa4fNqGBm6KpMkazUL1RCR7ydbc29u1ZDRCSVnIoKjNZWrNWf9NoxMyaYueiWLSLZKhKI4OLqhuYiklGcomIwTcwjMZiJSPYqChZjGRYraj9MdykiIklz8yO4wSD2Jx/32jEVzEQk7YJ2kHx/Ppt3fZbuUkREkmeaOBVDsT/dgLFrZ+8csleOIiLSQ4XBYhraG+iId6S7FBGRpDlFxRCNYm7e3CvHUzATEU8oCBbQHmunrq023aWIiCTNKSrCdcFas7pXjpdRwczQRZkiWSvsy8PBYcuu3vnWKSLSF9zCIvD5sNb0zgUAGRXMRCR75QfysQyL1Q2r0l2KiEjybBu3Xz+sdWvB7fkOEgpmIuIJef58DAxq2+rSXYqIyGFxivth7NrVKxvNKpiJiCcErAA5vjBbm7ekuxQRkcPi5uZiOA5GfX2Pj6VgJiKeURwqZkfrdjrjnekuRUQkaW6kAOIxrFU9389MwUxEPCPfn09HPEp9R8+/dYqI9BWnfxmuP4D94fs9PlaGBTNdlimSzfIC+bg4bG/elu5SRESSt+cCgA0bwHF6dKgMC2Yiks3y/flYWKxrXJPuUkREDotbUIjR2oLR2tKj4yiYiYhn5PnysCyLTbo1k4hkGDcUwnDiGDt7dmsmBTMR8YygHSJk57Bx58Z0lyIicljccBjicYyamh4dR8FMRDzDMAwKg4XUtdcRd+LpLkdEJGluTi5gYFb3bI1sRgUzQ4v/RbJe2JeP48RpijamuxQRkaS5Oblgmpi1Pbvfb0YFMxHJfiE7iINDa2druksREUmam5MDhoFZvb1Hx1EwExFP8VsBHNehubM53aWIiCTPtnHDYcxtR9BUpohkvxw7B9d12NFSne5SREQOixOJYNbW9Ohm5gpmIuIpQTuIbfrY1rI13aWIiByeUA5GZye0tXX7EApmIuIpASuIATR1NKW7FBGRw+IGgxCPYbR0f5NZuxfr2c/tt9/OBx98gGEY3HDDDYwbN67rtW3btnHdddfR2dnJmDFj+PGPf3zI4xm6KFMk6/ktP6Zp0dCe3vtl9nb/JSLZzw2GwHUxmrp/VXnKRsyWLFnCxo0bmT9/PnPmzGHOnDn7vH7HHXfw7//+7zzzzDNYlsXWrZq2EBHwmT58po+d0fSNmKn/EpHucENBXMPAaGjo9jFSFswqKyuZMmUKAMOHD6epqYnm5sRVVo7jsGzZMiZPngzA7NmzGThwYKpKEZEMYhgGAStAW6z7azR6Sv2XiHRLMIRhGJg13b94KWXBrLa2lsLCwq7HRUVF1Oy+TUF9fT25ubn89Kc/ZdasWdx1112pKkNEMlDIzknrdhnqv0SkO9xgCAwTqwe3ZUrpGrO9uXtdOuq6LtXV1Vx22WWUl5dz5ZVX8tprr3HmmWce9Bj5eSHC/lCKK+0bkfzsaAdkT1uypR2Q+W0pzovQ1FiX7jK69Eb/BRCJZPbfyx7Z0g5QW7woo9vhB4I+AnR2+xApC2alpaXU7nVbgh07dlBSUgJAYWEhAwcOpKKiAoBTTz2VNWvWHLJj27mrjbjPSlXJfSaSH6JpZ/qmaXpTtrQlW9oB2dGWeNSgvaP7HVtPpaL/Amhqyuy/F0j8o5kN7QC1xYsyvh2uS6DTIbZpG+FuHiJlU5mTJk1i0aJFAFRVVVFaWko4nCjTtm0GDx7Mp59+2vX6sGHDkjiqLssUORLYpo3rOmk7f2r6LxHJeoYBOTmYO7t/8VLKRswmTJjA2LFjmTlzJoZhMHv2bBYsWEBeXh5Tp07lhhtu4Ic//CGu6zJy5MiuhbQiIqZhErLTN52h/ktEussN5Xh3H7Mf/OAH+zwePXp01++HDBnCE088kcrTi0iGMg2Ljng0rTWo/xKR7nBzczAbur8Po3b+FxHPMQ2TmBNLdxkiIofNDeboXpkikl0s0wRcnDSuMxMR6Ra/DyPW/S+WSU1l1tTU8OKLL9LU1LTPZeNXX311t0/cHYYW/4scEczd3xkd18E0evb90Sv9l4gcGdxAAFwHHAfMw++/kvrEf/zHf7Bq1SpM08SyrK5fIiKpYJoWLhB34j0+lvovEelLrj+QCGXdvAAgqRGznJwcfvrTn3brBCIih8syEsHJpfvrNPZQ/yUifcrvT4yUtbZCXt5hfzypEbPjjjuOdevWHfbBRUS6IzF96fbKiJn6LxHpS67fn9jPrK17G+UmNWL25ptv8uijj1JYWIht27iui2EYvPbaa906qYjIwZiGiYFBp9Pz3f/Vf4lIn/IHEsGss3v9V1LB7Ne//nW3Di4i0h17Fvy7PbjkfA/1XyLSl1x79xrWbl6ZmVQwKysr44UXXmDFihUAjB8/ngsuuKBbJ+wJXZMpcmSwDAsDg454R4+P5ZX+S0SOENbuaBXv3lKMpILZbbfdRl1dHSeffDKu6/LSSy/x/vvvc+ONN3brpCIiB5MYMTN6ZZNZ9V8i0qf2bJGRymC2Zs0a5s6d2/X4kksu4aKLLurWCUVEDsXAwDQMorGe35ZJ/ZeI9CnbBsOEaPf6r6Suyuzs7MRxPt+BOx6PE+9mEhQROZQ9a8x6Y+d/9V8i0qcMA7cbG8vukdSI2RlnnMGFF17IiSeeCMA777zDeeed1+2TiogcTG8GM/VfItKnDAN8vm6PmCUVzL773e9y2mmn8cEHH2AYBj/+8Y8ZN25ct04oInIoXVdl9sIGs+q/RKTPWVa3b2R+0LG2lStXAlBZWUlbWxsjR45kxIgRtLS0UFlZ2a0T9oih6zJFjgQGPR8x81z/JSJHDrv7t3076IjZ888/z5gxY3jggQf2e80wDE499dRun1hE5EB6Y8RM/ZeIpItr2d0eMTtoMPvRj34EwGOPPbbP847jYPZgYZuIyMH0xhoz9V8ikja23e3tMpLqnRYsWMC8efOIx+PMmjWLs88+m8cff7xbJxQROZQ9waw3qP8Skb7WkxGzpHq/+fPn8/Wvf52//OUvjBgxgldeeYWXXnqpWycUETkUY/d60t64JZP6LxHpc7aV2hGzQCCA3+/n9ddfZ/r06ZoGEJGUMjAwDKNXrspU/yUifc5KcTADuPXWW3nvvfc46aSTWL58OdFu7s/RE4bulilyRDANk5Cd02s/817ov0TkCGKYqZ3KvPPOOxkyZAi//vWvsSyLLVu2cOutt3brhCIih2KbNkcXjuiVDWbVf4lIn+vByHxS+5itW7eOUaNGUV1dTWVlJcXFxTQ0NHT7pN0xJDKEgBXo03OKSPqMKxnfo897qf8SkSNLfMjQ1GyX4aV9gCYOnAgdmsoUkeR4qf8SkSOLM7C8259Neh+zXbt2kZeXB0BNTQ0lJSXdPqmISKqp/xKRTJTUJOi8efP4n//5n67H119/PXPnzk1ZUSIivUX9l4hkkqSC2R//+EfuueeerscPP/wwf/rTn1JWlIhIb1H/JSKZJKlgFo/Hse3PZz0Nw+iVjR9FRFJN/ZeIZJKDrjHbY/LkycycOZOJEyfiOA6LFy/mnHPOSXVtIiI9pv5LRDJJUsHsu9/9LieddBIffvghhmEwe/Zsxo/v2aXsIiJ9Qf2XiGSSpHdAa25uxu/3c/nll1NUVKSpABHJGOq/RCRTJBXMfv7zn/PMM8+wYMECAF544QVuu+22lBYmItIb1H+JSCZJKpi9++673HfffeTm5gJw1VVXUVVVldLCRER6g/ovEckkSQWzQCBxKyTDSOy8H4/HiXfzrukiIn1J/ZeIZJKkFv9PmDCBH/3oR+zYsYNHHnmEl19+mZNOOinVtYmI9Jj6LxHJJEkFs2uvvZaFCxcSDAbZvn07l19+uS43F5GMoP5LRDJJUsHsd7/7HVdeeSXnnntuqusREelV6r9EJJMktcZs9erVbNy4MdW1iIj0OvVfIpJJkhox++STTzj//POJRCL4fL6u51977bVU1SUi0ivUf4lIJkkqmN15550sWbKE119/HcMwOPvssznhhBNSXZuISI+p/xKRTJJUMLv77rspKChgypQpuK7L0qVLeeONN3jggQdSXZ+ISI+o/xKRTJJUMGtqauK3v/1t1+NZs2Zx0UUXpawoEZHeov5LRDJJUov/Bw0aRE1NTdfj2tpahgwZkrKiRER6i/ovEckkSY2Ybd26lalTp3L00UfjOA4bNmxg+PDhXHzxxQDMmzcvpUWKiHSX+i8RySRJBbNrrrkm1XWIiKSE+i8RySRJBbPu3r7k9ttv54MPPsAwDG644QbGjRu333vuuusu3n//fR577LFunUNE5GDUf4lIJklqjVl3LFmyhI0bNzJ//nzmzJnDnDlz9nvP2rVreffdd1NVgohIt6j/EpF0SVkwq6ysZMqUKQAMHz6cpqYmmpub93nPHXfcwbXXXpuqEkREukX9l4ikS8qCWW1tLYWFhV2Pi4qK9rkyasGCBZx00kmUl5enqgQRkW5R/yUi6ZLUGrPe4Lpu1+8bGxtZsGABjzzyCNXV1UkfI5IfSkVpaaG2eE+2tAOyqy1e0Bv9F0Akkh1/L9nSDlBbvChb2tFdKQtmpaWl1NbWdj3esWMHJSUlACxevJj6+nouvvhiotEon332Gbfffjs33HDDQY/ZtLMtVeX2qUh+SG3xmGxpB2RXW4ik57Sp6L8Ampoy/+8lEgllRTtAbfGibGkHQKQ4r1ufS9lU5qRJk1i0aBEAVVVVlJaWEg6HATj33HN58cUXeeqpp7jvvvsYO3ZsUp2aiEhfUP8lIumSshGzCRMmMHbsWGbOnIlhGMyePZsFCxaQl5fH1KlTU3VaEZEeU/8lIuliuHsvnvCwuR/OhQ5/usvoFdk01ZQtbcmWdkB2teXYwaM4ruy4dJfRO55+mqaOjOhuDyqrpprUFs/JlnbA7qnM6dMP+3Mpm8oUERERkcOjYCYiIiLiEQpmIiIiIh6hYCYiIiLiEQpmIiIiIh6hYCYiIiLiEQpmIiIiIh6hYCYiIiLiEQpmIiIiIh6hYCYiIiLiEQpmIiIiIh6hYCYiIiLiEQpmIiIiIh6hYCYiIiLiEQpmIiIiIh6hYCYiIiLiEQpmIiIiIh6hYCYiIiLiEQpmIiIiIh6hYCYiIiLiEQpmIiIiIh6hYCYiIiLiEQpmIiIiIh6RMcHMdd10lyAiIiKSUhkTzERERESynYKZiIiIiEcomImIiIh4hIKZiIiIiEcomImIiIh4hIKZiIiIiEcomImIiIh4hIKZiIiIiEcomImIiIh4hIKZiIiIiEcomImIiIh4hIKZiIiIiEcomImIiIh4hIKZiIiIiEcomImIiIh4hIKZiIiIiEcomImIiIh4hIKZiIiIiEcomImIiIh4hIKZiIiIiEcomImIiIh4hIKZiIiIiEfYqTz47bffzgcffIBhGNxwww2MGzeu67XFixdz9913Y5omw4YNY86cOZimcqKIeIP6LxFJh5T1JEuWLGHjxo3Mnz+fOXPmMGfOnH1ev/nmm7nnnnt48sknaWlp4c0330xVKSIih0X9l4ikS8qCWWVlJVOmTAFg+PDhNDU10dzc3PX6ggULKCsrA6CoqIiGhoZUlSIicljUf4lIuqRsKrO2tpaxY8d2PS4qKqKmpoZwOAzQ9d8dO3bw1ltvcfXVVx/ymJH8UGqKTQO1xXuypR2QXW1Jh1T0XwCRSHb8vWRLO0Bt8aJsaUd3pXSN2d5c193vubq6Or797W8ze/ZsCgsLD3mMpp1tqSitz0XyQ2qLx2RLOyC72kIk3QUk9Eb/BdDUlPl/L5FIKCvaAWqLF2VLOwAixXnd+lzKpjJLS0upra3terxjxw5KSkq6Hjc3N3PFFVdwzTXXcPrpp6eqDBGRw6b+S0TSJWXBbNKkSSxatAiAqqoqSktLu4b/Ae644w6+8Y1v8E//9E+pKkFEpFvUf4lIuqRsKnPChAmMHTuWmTNnYhgGs2fPZsGCBeTl5XH66afz3HPPsXHjRp555hkALrjgAmbMmJGqckREkqb+S0TSJaVrzH7wgx/s83j06NFdv1+xYkUqTy0i0iPqv0QkHbQjooiIiIhHKJiJiIiIeISCmYiIiIhHKJiJiIiIeISCmYiIiIhHKJiJiIiIeISCmYiIiIhHKJiJiIiIeISCmYiIiIhHKJiJiIiIeISCmYiIiIhHKJiJiIiIeISCmYiIiIhHKJiJiIiIeISCmYiIiIhHKJiJiIiIeISCmYiIiIhHKJiJiIiIeISCmYiIiIhHKJiJiIiIeISCmYiIiIhHKJiJiIiIeISCmYiIiIhHKJiJiIiIeISCmYiIiIhHKJiJiIiIeISCmYiIiIhHKJiJiIiIeISCmYiIiIhHKJiJiIiIeISCmYiIiIhHKJiJiIiIeISCmYiIiIhHKJiJiIiIeISCmYiIiIhHKJiJiIiIeISCmYiIiIhHKJiJiIiIeISCmYiIiIhHKJiJiIiIeISCmYiIiIhHKJiJiIiIeISCmYiIiIhHKJiJiIiIeISCmYiIiIhHpDSY3X777cyYMYOZM2fy4Ycf7vPa22+/zYUXXsiMGTO4//77U1mGiMhhU/8lIumQsmC2ZMkSNm7cyPz585kzZw5z5szZ5/XbbruNe++9lyeeeIK33nqLtWvXpqoUEZHDov5LRNIlZcGssrKSKVOmADB8+HCamppobm4GYNOmTUQiEQYMGIBpmpxxxhlUVlamqhQRkcOi/ktE0iVlway2tpbCwsKux0VFRdTU1ABQU1NDUVHRF74mIpJu6r9EJF3svjqR67o9+vwpg06hI9bRS9V4QEG6C+hF2dKWbGkHZE1bSnJL0l0C0PP+C4CRI4lYVs+P4wGRdBfQi9QW78mWduD3d+tjKQtmpaWl1NbWdj3esWMHJSUlX/hadXU1paWlBz3eiOIRqSlUROQf9Hb/BcBxx/V6nSKSfVI2lTlp0iQWLVoEQFVVFaWlpYTDYQAGDRpEc3MzmzdvJhaL8eqrrzJp0qRUlSIicljUf4lIuhhur4zRf7E777yTpUuXYhgGs2fPZuXKleTl5TF16lTeffdd7rzzTgDOOeccvvWtb6WqDBGRw6b+S0TSIaXBTERERESSp53/RURERDxCwUxERETEIzwZzLLlVigHa8fixYv5t3/7N2bOnMmPfvQjHMdJU5XJOVhb9rjrrru49NJL+7iyw3ewtmzbto1Zs2Zx4YUXcvPNN6epwuQdrC3z5s1jxowZzJo1a7+d671o9erVTJkyhblz5+73Wrb83GdSOyB7+jD1X96k/usAXI9555133CuvvNJ1Xdddu3at+2//9m/7vD59+nR369atbjwed2fNmuWuWbMmHWUe0qHaMXXqVHfbtm2u67ru9773Pfe1117r8xqTdai2uK7rrlmzxp0xY4Z7ySWX9HV5h+VQbfn+97/vvvzyy67ruu4tt9zibtmypc9rTNbB2rJr1y73rLPOcjs7O13Xdd3LL7/cXb58eVrqTEZLS4t7ySWXuDfeeKP72GOP7fd6tvzcZ0o7XDd7+jD1X96k/uvAPDdili23QjlYOwAWLFhAWVkZkNg5vKGhIS11JuNQbQG44447uPbaa9NR3mE5WFscx2HZsmVMnjwZgNmzZzNw4MC01XooB2uLz+fD5/PR2tpKLBajra2NSMS72zb6/X4efPDBL9wPLFt+7jOpHZA9fZj6L29S/3Vgngtm2XIrlIO1A+jaE2nHjh289dZbnHHGGX1eY7IO1ZYFCxZw0kknUV5eno7yDsvB2lJfX09ubi4//elPmTVrFnfddVe6ykzKwdoSCAS46qqrmDJlCmeddRbHHXccw4YNS1eph2TbNsFg8Atfy5af+0xqB2RPH6b+y5vUfx2Y54LZP3KzZDePL2pHXV0d3/72t5k9e/Y+/4N63d5taWxsZMGCBVx++eVprKj79m6L67pUV1dz2WWXMXfuXFau/ASMOL8AAAQbSURBVP/bu59Q6PY4juOfyxg7+RflX2xsrCYkJWRFzZKQJGTFwsq/lYWIjY0UzW4oUZTyp6SUkkkpOm5KClGKJI3yp5m5i6frPupxuD3Nc46Z92tpLL7fzPn0mXM0v7+1tbVl3XD/08+7+P1+TU1NaX19XZubmzo4ONDx8bGF00WnSMkvKXIyjPyyJ/LrP7YrZmE5CsUCZntIP954HR0d6u7uVllZmRUjfpnZLru7u7q7u1NTU5O6urp0dHSk4eFhq0b9lNkuSUlJysjIUE5OjmJjY1VaWqqTkxOrRv2U2S6np6fKzs5WcnKynE6nioqKZBiGVaP+lki57r/THlLkZBj5ZU/k18dsV8wi5SgUsz2kH//T0NLSovLycqtG/DKzXaqrq7W6uqr5+XlNTEyooKBAAwMDVo5rymwXh8Oh7OxsnZ2dvb1u59vnZrtkZmbq9PRUT09PkiTDMJSbm2vVqL8lUq7777SHFDkZRn7ZE/n1MVt+83+kHIXy0R5lZWUqLi6Wy+V6+1232636+noLpzVn9jf51+Xlpfr7++X1ei2c9HNmu5yfn6uvr0+hUEj5+fkaHBxUTIztPr+8Mdtlbm5Oi4uLio2NlcvlUk9Pj9XjfsgwDI2Ojurq6koOh0Pp6emqqqpSVlZWxFz3320PKXIyjPyyJ/Lr12xZzAAAAKKRfas0AABAlKGYAQAA2ATFDAAAwCYoZgAAADZBMQMAALAJihkiQnNzs3Z2duTz+dTY2Gj1OADwZeQXfkYxAwAAsAmH1QMg+vh8Pk1OTio+Pl5VVVUyDEPn5+d6fHyU2+1WW1ubgsGghoaG3o7haG1tVU1NjTY2NuTxeOR0OhUIBDQ2NqasrCyLNwIQLcgvhBt3zGAJwzA0NjYmv9+vtLQ0eb1eLSwsaGVlRcfHx1peXtbt7a3m5+fl8Xi0tLSkQCCgh4cHjY+Py+v1qqKiQrOzs1avAiDKkF8IJ+6YwRJ5eXlKTEyUz+fT9fW19vb2JEkvLy+6uLjQ4eGhSkpKJEkJCQmanp6WJKWmpqq3t1ehUEg3NzfvjoQBgD+B/EI4Ucxgibi4OEmS0+lUZ2enqqur373u8/kUDAbf/ez19VXd3d1aWlpSbm6uZmZm3h4VAMCfQn4hnHiUCUsVFhZqbW1NkhQMBjUyMqL7+3u5XC5tb29Lkvx+v+rq6vTw8KCYmBhlZmbq+flZm5ubenl5sXJ8AFGM/EI4cMcMlmpqatLJyYnq6+sVCARUWVmpxMRE1dTUaH9/Xw0NDQoEAmptbVVKSorcbrdqa2uVkZGh9vZ29fT0vAUjAPxJ5BfC4a9QKBSyeggAAADwKBMAAMA2KGYAAAA2QTEDAACwCYoZAACATVDMAAAAbIJiBgAAYBMUMwAAAJugmAEAANjEP7u9gSOX6RVSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(num=1,figsize=(10,5))\n",
    "#Positive reviews\n",
    "ax = plt.subplot(1,2,1) \n",
    "sns.lineplot(x='recall',y='precision',data=pos_pre_rec,ax=ax,color='green', alpha=0.5)\n",
    "plt.fill_between(pos_pre_rec.recall.values, pos_pre_rec.precision.values, color='green', alpha=0.3)\n",
    "plt.title('Positive Class')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "#Negative reviews\n",
    "ax = plt.subplot(1,2,2) \n",
    "sns.lineplot(x='recall',y='precision',data=neg_pre_rec,ax=ax,color='red', alpha=0.5)\n",
    "plt.fill_between(neg_pre_rec.recall.values, neg_pre_rec.precision.values, color='red', alpha=0.3)\n",
    "plt.title('Negative Class')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check how the average probability of being positive and the percentage of predicted positive reviews varies \n",
    "with the number of stars of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stars</th>\n",
       "      <th>Avg Prob</th>\n",
       "      <th>Pos Revs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.123413</td>\n",
       "      <td>7.666268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.160246</td>\n",
       "      <td>11.468288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.213923</td>\n",
       "      <td>16.725699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.294694</td>\n",
       "      <td>25.351044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.753100</td>\n",
       "      <td>81.014304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.822766</td>\n",
       "      <td>87.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>0.846828</td>\n",
       "      <td>89.889078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>0.865724</td>\n",
       "      <td>91.598320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stars  Avg Prob   Pos Revs\n",
       "0      1  0.123413   7.666268\n",
       "1      2  0.160246  11.468288\n",
       "2      3  0.213923  16.725699\n",
       "3      4  0.294694  25.351044\n",
       "4      7  0.753100  81.014304\n",
       "5      8  0.822766  87.578947\n",
       "6      9  0.846828  89.889078\n",
       "7     10  0.865724  91.598320"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stars = np.unique(test_stars)\n",
    "stars.sort()\n",
    "my_df={'Stars': stars, \n",
    "       'Avg Prob':[test_probas[test_stars==i].mean() for i in stars],\n",
    "      'Pos Revs': [(test_probas[test_stars==i]>=0.5).mean()*100 for i in stars]}\n",
    "\n",
    "my_df = pd.DataFrame(my_df)\n",
    "my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(num=2,figsize=(5,5))\n",
    "sns.barplot(x='Stars',y='Avg Prob',data=my_df)\n",
    "plt.title('Average Probability of Being Positive')\n",
    "plt.ylim(0,1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the higher the stars awarded to the review the higher the probability of it being positive based on our model (at least on average) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(num=3,figsize=(5,5))\n",
    "sns.barplot(x='Stars',y='Pos Revs',data=my_df)\n",
    "plt.title('Percentage of Predicted Positive Reviews')\n",
    "plt.ylim(0,100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the higher the stars awarded to the review the higher the probability of it being positive based on our model (at least on average) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on a test review\n",
    "\n",
    "Let's write a predict function which will output if a provided review is positive or negative, as well as the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, test_review, sequence_length=200, alignment='right'):\n",
    "    ''' Prints out whether a give review is predicted to be \n",
    "        positive or negative in sentiment, using a trained model.\n",
    "        \n",
    "        params:\n",
    "        net - A trained net \n",
    "        test_review - a review made of normal text and punctuation\n",
    "        sequence_length - the padded length of a review\n",
    "        '''\n",
    "    #Default sequence aligment: the one provided by the model if any\n",
    "    if sequence_length is None:\n",
    "        sequence_length = net.seq_length\n",
    "    #get lower case review\n",
    "    test_review = test_review.lower()\n",
    "    #remove some contractions\n",
    "    test_review = remove_contractions(test_review)\n",
    "    #remove punctuation\n",
    "    test_review = ''.join([c for c in test_review if c not in punctuation])\n",
    "    #split in words\n",
    "    test_review = test_review.split()\n",
    "    #Encode the words\n",
    "    unk = net.word_to_int.get('<unk>')\n",
    "    test_review = [[net.word_to_int.get(word,unk) for word in test_review[:sequence_length]]]\n",
    "    #Padding\n",
    "    test_review = pad_features(test_review, sequence_length, alignment=alignment)\n",
    "    #Convert into pytorch tensor\n",
    "    test_review = torch.from_numpy(test_review).type(torch.LongTensor).to(device)\n",
    "    \n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    pred = net(test_review).squeeze().item()\n",
    "    if pred >= 0.5:\n",
    "        pred = ('Positive', pred)\n",
    "    else:\n",
    "        pred = ('Negative', pred)\n",
    "    return pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative test review\n",
    "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive test review\n",
    "test_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Negative', 0.0012513089459389448)\n",
      "('Positive', 0.9868086576461792)\n"
     ]
    }
   ],
   "source": [
    "# call function for positive and negative reviews\n",
    "print(predict(net, test_review_neg))\n",
    "print(predict(net, test_review_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dependency of the Predictions on the Length of the Sequence\n",
    "\n",
    "Although we have used a seqyence length of 350 words, our RNN  can actually use sequences of diffetrent lengths without modifying the net. Out of curiosity, we will test how the size of the sequences modifies the predictions for the longest misclassified review and for the longest review correctly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the corretly classified review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the review: 2328\n",
      "Stars: 10\tLabel: 1\tPrediction:0.9146\n"
     ]
    }
   ],
   "source": [
    "test_goodies_mask = ((test_y) == (test_probas>0.5))\n",
    "test_goodies_idx = np.arange(0,len(test_y),dtype=np.int)[test_goodies_mask]\n",
    "test_goodies_lens = np.array([len(test_reviews_int[idx]) for idx in test_goodies_idx])\n",
    "argmax = test_goodies_lens.argmax()\n",
    "my_idx = test_goodies_idx[argmax]\n",
    "print(f\"Length of the review: {len(test_reviews_int[my_idx])}\")\n",
    "print(f\"Stars: {test_stars[my_idx]}\\tLabel: {test_y[my_idx]}\\tPrediction:{test_probas[my_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The review:\n",
      "There's a sign on The Lost Highway that says:<br /><br />*MAJOR SPOILERS AHEAD*<br /><br />(but you already knew that, didn't you?)<br /><br />Since there's a great deal of people that apparently did not get the point of this movie, I'd like to contribute my interpretation of why the plot makes perfect sense. As others have pointed out, one single viewing of this movie is not sufficient. If you have the DVD of MD, you can \"cheat\" by looking at David Lynch's \"Top 10 Hints to Unlocking MD\" (but only upon second or third viewing, please.) ;)<br /><br />First of all, Mulholland Drive is downright brilliant. A masterpiece. This is the kind of movie that refuse to leave your head. Not often are the comments on the DVDs very accurate, but Vogue's \"It gets inside your head and stays there\" really hit the mark.<br /><br />David Lynch deserves praise for creating a movie that not only has a beautifully stylish look to it - cinematography-wise, has great acting (esp. Naomi Watts), a haunting soundtrack by Badalamenti, and a very dream-like quality to it -- but on top of it all it also manages to involve the viewer in such a way that few movies have before. (After all, when is the last time you saw a movie that just wouldn't leave your mind and that everyone felt compelled to talk and write about, regardless of whether they liked it or hated it?)<br /><br />Allright, enough about all that, it's time to justify those statements.<br /><br />Most people that have gone through some effort to try to piece the plot together will have come to the conclusion that the first half of the picture is an illusion/a dream sequence.<br /><br />Of course, that's too bad for all those trying to make sense of the movie by expecting \"traditional\" methods in which the story is laid out in a timely, logic and linear manner for the viewer. But for those expecting that, I urge you to check the name of the director and come back again. ;)<br /><br />MD is the story of the sad demise of Diane Selwyn, a wannabe-actor who is hopelessly in love with another actor, Camilla Rowles. Due to Diane's lack of talent, she is constantly struggling to advance her career, and feels she failed to deliver on her own and her parents' expectations. Upon realizing that Camilla will never be hers (C. becomes engaged with Adam Kesher, the director), she hires a hitman to get rid of her, and subsequently has to deal with the guilt that it produces.<br /><br />The movie first starts off with what may seem as a strange opening for this kind of thriller; which is some 50s dance/jitterbug contest, in which we can see the main character Betty giving a great performance. We also see an elderly couple (which we will see twice more throughout the movie) together with her, and applauding her.<br /><br />No, wait. This is what most people see the first time they view it. There's actually another very significant fact that is given before the credits - the camera moving into an object (although blurry) and the scene quickly fading out. If you look closely, the object is actually a pillow, revealing that what follows is a dream.<br /><br />The main characters seen in the first half of the movie:<br /><br />Betty: Diane Selwyn's imaginary self, used in the first half of the movie that constitutes the \"dream-sequence\" - a positive portrayal of a successful, aspiring young actor (the complete opposite of Diane). 'Betty' was chosen as the name as that is the real name of the waitress at Winkies. Notice that in the dream version, the waitresses' name is 'Diane'.<br /><br />Rita: The fantasy version of Camilla Rhodes that, through Diane's dream, and with the help of an imaginary car-accident, is turned into an amnesiac. This makes her vulnerable and dependent on Diane's love. She is then conveniently placed in Betty/Diane's aunt's luxurious home which Betty has been allowed to stay in.<br /><br />Coco: In real life, Adam's mother. In the dream part, the woman in charge of the apartment complex that Betty stays in. She's mainly a strong authority figure, as can be witnessed in both parts of the film.<br /><br />Adam: The director. We know from the second half that he gets engaged with Camilla. His sole purpose for being in the first half of the movie is only to serve as a punching bag for Betty/Diane, since she develops such hatred towards him.<br /><br />Aunt Ruth: Diane's real aunt, but instead of being out of town, she is actually dead. Diane inherited the money left by her aunt and used that to pay for Camilla's murder.<br /><br />Mr. Roach: A typical Lynchian character. Not real; appears only in Diane's dream sequence. He's a mysterious, influential person that controls the chain of events in the dream from his wheelchair. He serves much of the same function as the backwards-talking dwarf (which he also plays) in Twin Peaks.<br /><br />The hitman: The person that murders Camilla. This character is basically the same in both parts of the movie, although rendered in a slightly more goofy fashion in the dream sequence (more on that below).<br /><br />Now, having established the various versions of the characters in the movie, we can begin to delve into the plot. Of course I will not go into every little detail (neither will I lay it out chronologically), but I will try to explain some of the important scenes, in relation to Lynch' \"hint-sheet\".<br /><br />As I mentioned above, Camilla was re-produced as an amnesiac through her improbable survival of a car-accident in the first 10 minutes of the movie, which left her completely vulnerable. What I found very intriguing with MD, is that Lynch constantly gives hints on what is real and what isn't. I've already mentioned the camera moving into the pillow, but notice how there's two cars riding in each lane approaching the limo.<br /><br />Only one of the cars actually hit the limo; what about the other? Even if they stayed clear of the accident themselves, wouldn't they try to help the others, or at least call for help? My theory is that, since this is a dream, the presence of the other car is just set aside, and forgotten about. Since, as Rogert Ebert so eloquently puts it \"Like real dreams, it does not explain, does not complete its sequences, lingers over what it finds fascinating, dismisses unpromising plotlines.\"<br /><br />Shortly after Rita crawls down from the crash site at Mulholland Dr., and makes her way down the hillside and sneaks into Aunt Ruth's apartment, Betty arrives and we see this creepy old couple driving away, staring ghoulishly at each other and grinning at themselves and the camera. This is the first indication that what we're seeing is a nightmare.<br /><br />Although the old couple seem to be unfamiliar to Betty, I think they're actually her parents (since they were applauding her at the jitterbug contest). Perhaps she didn't know them all that well, and didn't really have as good a relationship with them as she wanted, so the couple is shown as very pleasant and helpful to her in the dream. They also represent her feelings of guilt from the murder, and Diane's sense of unfulfillment regarding her unachieved goals in her life.<br /><br />A rather long and hilarious scene is the one involving the hitman. Diane apparently sees him as the major force behind the campaign trying to pressure the director to accept Camilla's part in the movie (from Adam's party in the second half of the movie), and he therefore occupies a major part of her dream. Because of her feelings of guilt and remorse towards the murder of Camilla, a part of her wants him to miss, so she turns him into a dumb criminal.<br /><br />This scene, I think, is also Lynch's attempt at totally screwing his audience over, since they're given a false pretence in which to view the movie.<br /><br />Gotta love that 'Something just bit me bad' line, though. :)<br /><br />The next interesting scene is the one with the two persons at Twinkies, who are having a conversation about how one of them keep having this recurring nightmare involving a man which is seen by him through a wall outside of the diner that they're sitting in. After a little talk, they head outside and keep walking toward the corner of a fence, accompanied of course by excellent music matching the mood of the scene.<br /><br />When reaching the corner, a bum-like character with a disfigured face appears out from behind the corner, scaring the living crap out of the man having the nightmare. This nightmare exists only in Diane's mind; she saw that guy in the diner when paying for the murder. So, in short, her obessions translate into that poor guy's nightmares. The bum also signifies Diane's evil side, as can be witnessed later in the movie.<br /><br />The Cowboy constitutes (along with the dwarf) one of the strange characters that are always present in the Lynchian landscape -- Diane only saw him for a short while at Adam's party, but just like our own dreams can award insignificant persons that we hardly know a major part in our dreams, so can he be awarded an important part in her dream. We are also given further clues during his scenes that what we're seeing is not real (his sudden disappearance, etc.)<br /><br />The Cowboy is also used as a tool to mock the Director, when he meets up with him at the odd location (the lights here give a clear indication that this is part of a dream). Also notice how he says that he will appear one more time if he (Adam) does good, or two more times if he does bad. Throughout the movie he appears two more times, indicating to Diane that she did bad. He is also the one to wake her up to reality (that scene is probably an illusion made to fit into her requirements of him appearing twice), and shortly thereafter she commits suicide.<br /><br />The espresso-scene with the Castigliane brothers (where we can see Badalamenti, the composer, as Luigi) is probably a result of the fact that Diane was having an espresso just before Camilla and Adam made their announcement at Adam's party in the second half. It could at the same time also be a statement from Lynch.<br /><br />During the scene in which they enter Diane's apartment, the body lying in the bed is Camilla, but notice how she's assumed Diane's sleeping position; Diane is seeing herself in her own dream, but the face is not hers, although it had the same wounds on the face as Diane would have after shooting herself. This scene is also filled with some genuine Lynchian creepiness. Since Diane did not know where (or when) the hitman would get to Camilla and finish her off, she just put her into her own home.<br /><br />In real life, Diane's audition for the movie part was bad. In her dream, she delivers a perfect audition - leaving the whole crew ecstatic about her performance.<br /><br />Also interesting is the fact that the money that in real-life was used to pay for Camilla's murder now appears in Rita/Camilla's purse. This is part of Diane's undoing of her terrible act by effectively being given the money back, as the murder now hasn't taken place.<br /><br />When her neighbor arrives to get her piano-shaped ashtray, another hint is given; she takes the ashtray from her table and leaves, yet later when Camilla and Betty have their encounter on the couch, we see the ashtray appear again when the camera pans over the table, suggesting that Betty's encounter with the neighbor was a fantasy.<br /><br />The catch phrase of the movie Adam is auditioning actresses for is \"She is the girl\"; which are the exact same words that Diane uses when giving the hitman Camilla's photo resume.<br /><br />The blue box and the key represent the major turning point in the movie, and is where the true identities of the characters are revealed. There's much symbolism going on here; the box may represent Diane's future (it's empty), or it may be a sort of a Pandora's box (the hitman laughs when she asks him what the key will open). Either way, it is connected to the murder by means of the blue key (which is placed next to her after the murder has taken place). The box is also seen at the end of the movie in the hands of the disfigured bum.<br /><br />Club Silencio is a neat little addition to further remind the viewer that what s/he is viewing is not real. It also signifies that Diane is about to wake up to her reality (her reality being a nightmare that she is unable to escape from, even in her dreams).<br /><br />During the chilling scene at the end where the creepy old couple reappear, Diane is tormented in such a way that she sees suicide as the only way out in order to escape the screams and to avoid being haunted by her fears.<br /><br />Anyway, that is my $0.02. Hope this could help people from bashing out at this movie and calling it 'the worst movie ever' or something to that effect, without realizing the plot.<br /><br />As usual, Lynch is all about creating irrational fears, and he certainly achieves that with this picture as well.<br /><br />10 out of 10.\n"
     ]
    }
   ],
   "source": [
    "print('The review:')\n",
    "with open(test_pos_dir+test_pos_rev_files[my_idx], 'r',encoding=\"utf8\") as f:\n",
    "    my_review=f.read()\n",
    "    print(my_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:200\t('Positive', 0.9913550019264221)\n",
      "Length:250\t('Positive', 0.9481842517852783)\n",
      "Length:300\t('Positive', 0.822350800037384)\n",
      "Length:350\t('Positive', 0.9146257638931274)\n",
      "Length:400\t('Positive', 0.8450926542282104)\n",
      "Length:450\t('Positive', 0.9030171036720276)\n",
      "Length:500\t('Positive', 0.9454030990600586)\n",
      "Length:550\t('Positive', 0.8161596655845642)\n",
      "Length:600\t('Positive', 0.6993175745010376)\n",
      "Length:650\t('Positive', 0.7223625183105469)\n",
      "Length:700\t('Positive', 0.657808244228363)\n",
      "Length:750\t('Positive', 0.8400471806526184)\n",
      "Length:800\t('Negative', 0.2388102114200592)\n",
      "Length:850\t('Positive', 0.6818288564682007)\n",
      "Length:900\t('Positive', 0.913536787033081)\n",
      "Length:950\t('Positive', 0.6888061761856079)\n",
      "Length:1000\t('Negative', 0.3520359396934509)\n",
      "Length:1050\t('Positive', 0.5967943072319031)\n",
      "Length:1100\t('Positive', 0.6685435175895691)\n",
      "Length:1150\t('Positive', 0.8842641115188599)\n",
      "Length:1200\t('Positive', 0.9111968874931335)\n",
      "Length:1250\t('Positive', 0.9712346196174622)\n",
      "Length:1300\t('Positive', 0.9450238347053528)\n",
      "Length:1350\t('Positive', 0.941097617149353)\n",
      "Length:1400\t('Positive', 0.6500811576843262)\n",
      "Length:1450\t('Positive', 0.8010260462760925)\n",
      "Length:1500\t('Positive', 0.7956892251968384)\n",
      "Length:1550\t('Positive', 0.8468532562255859)\n",
      "Length:1600\t('Positive', 0.8163259625434875)\n",
      "Length:1650\t('Positive', 0.9007474780082703)\n",
      "Length:1700\t('Positive', 0.8759380578994751)\n",
      "Length:1750\t('Positive', 0.6189854741096497)\n",
      "Length:1800\t('Negative', 0.49875378608703613)\n",
      "Length:1850\t('Positive', 0.6981590986251831)\n",
      "Length:1900\t('Positive', 0.7822060585021973)\n",
      "Length:1950\t('Positive', 0.6614677309989929)\n",
      "Length:2000\t('Negative', 0.4783925712108612)\n",
      "Length:2050\t('Negative', 0.3726698160171509)\n",
      "Length:2100\t('Positive', 0.6806808710098267)\n",
      "Length:2150\t('Positive', 0.8987547159194946)\n",
      "Length:2200\t('Positive', 0.9713649749755859)\n",
      "Length:2250\t('Positive', 0.961516261100769)\n",
      "Length:2300\t('Negative', 0.16108155250549316)\n"
     ]
    }
   ],
   "source": [
    "for sl in range(200,len(test_reviews_int[my_idx]),50):\n",
    "    print(f\"Length:{sl}\\t{predict(net, my_review, sequence_length=sl)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independently of the size, the network predicts a possitive sentiment (the correct result) in most of the cases. On the other hand the probability varies widly (0.16-0.99) and the best results are obtained if the sequence length is smaller that 1100 words.\n",
    "\n",
    "It is very interesting how the probability drops to 0.16 for the las length considered, misclasifying the review as negative. That maybe due to this sentence: \"Hope this could help people from bashing out at this movie and calling it 'the worst movie ever' or something to that effect\" It does not get much more negative than \"the worst movie ever\".\n",
    "\n",
    "Two factors are complicating the prediction:\n",
    "\n",
    "1) The net was trained using sequences of 350 words. So it probably is more forgetful than it should when dealing with a 2300 word sequences and does not propely remember all the positive things said before \"the worst movie ever\".\n",
    "\n",
    "2) Although the test is between quotations and 'the worst movie ever' could potentialy provide a different sentiment than the worst movie ever, we deleted the punctuation as part of our data preparation.\n",
    "\n",
    "Let's see how it affects the misclassified review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it affects the misclassified review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the review: 2154\n",
      "Stars: 10\tLabel: 1\tPrediction:0.1770\n"
     ]
    }
   ],
   "source": [
    "test_error_mask = ((test_y) == (test_probas<0.5))\n",
    "test_erorr_idx = np.arange(0,len(test_y),dtype=np.int)[test_error_mask]\n",
    "test_error_lens = np.array([len(test_reviews_int[idx]) for idx in test_erorr_idx])\n",
    "argmax = test_error_lens.argmax()\n",
    "my_idx = test_erorr_idx[argmax]\n",
    "print(f\"Length of the review: {len(test_reviews_int[my_idx])}\")\n",
    "print(f\"Stars: {test_stars[my_idx]}\\tLabel: {test_y[my_idx]}\\tPrediction:{test_probas[my_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The review:\n",
      "Back in the mid/late 80s, an OAV anime by title of \"Bubblegum Crisis\" (which I think is a military slang term for when technical equipment goes haywire) made its debut on video, taking inspiration from \"Blade Runner\", \"The Terminator\" and maybe even \"Robocop\", with a little dash of Batman/Bruce Wayne - Iron Man/Tony Stark and Charlie's Angel's girl power thrown in for good measure. 8 episodes long, the overall story was that in 21st century Tokyo, Japan, year 2032-2033, living machines called Boomers were doing manual labor and sometimes cause problems. A special, SWAT like branch of law enforcers, the Advanced Police (AD Police for short) were formed to handle the boomers, but were mostly ineffective, prompting millionaire scientist Sylia Stingray, the daughter of the scientist who made the boomers, to create four powered combat armor (hard suits) to be worn by women to fight the boomers and fight the evil corporation that produced the boomers, GENOM. That group becomes known as the Knight Sabers, and in addition to ring leader Sylia, her rag-tag band of rebel women included Priss Asagiri, a struggling rock and roll gal with a passion for motorcycles and a disdain for cops, Linna Yamazaki, an aerobics instructor with an eye for money and a tendency to blow through boyfriends, and Nene Romanova, a young officer of the ADP and expert computer hacker (the first in a long line). GENOM, meanwhile, is represented by Quincy, a tall, gaunt old guy who happens to own the company, his younger assistant Brian J. Mason (killed in episode 3) and an annoying boomer man named Largo. Other characters included Leon McNichol and Daley Wong, two AD Police detectives (Leon appeared in a spin-off/prequel anime, \"AD Police Files\" which I heard was very dark), their balding, overweight boss Chief Todo, Sylia's younger brother Mackey, and a funny little mechanic known as Dr. Raven, who apparently helps Sylia with maintaining the suits. Aside from the overall Knight Sabers & AD Police VS GENOM storyline, there was also another storyline involving a friend of Linna's who was apparently a daughter in a big crime family, the annoying Largo trying to usurp GENOM, and various Priss-wants-revenge-for-a-minor-character story. Oh and did I mention that there were hints that Sylia herself might have been a boomer?<br /><br />Well, it was a great watch, full of chaos and mayhem and even some very nice pop songs, but it was not without its flaws, some of which, unfortunately, were due to the fact that the series was discontinued after episode 8 when it was originally planned for 13 episodes in all. So some of the storylines, like Largo's scheme (or schemes), the family of Linna's ill-fated friend, and Sylia's origins, were never resolved. Another problem with the series was that at the time Priss was the most popular character, so a good portion of the series focused on her, and unfortunately, most of the Priss oriented episodes basically focused on Priss self-righteously seeking justice/revenge for some secondary character who had never appeared before but happened to be a friend of hers, yet she rarely went out of her way for her the Knight Sabers, who were always bailing her out of trouble and for some reason cared a great deal about her well-being (just to be fair though, she did go to rescue Linna in episode 7, and her boyfriend got killed by a boomer and the ADP acted wrongly in the investigation). This meant we didn't really get to focus on the more interesting back story of Sylia, or even the day-to-day antics of Nene and Linna. Linna had two episodes oriented around her, which pertained to her friend with the mafia family, while Nene managed to snag the last episode for herself, which showed her eternal good cheer was genuinely good spirits and not ditziness. Nene also got to put her computer skills to good use quite a bit, or she sometimes just acted like a lovable goof, which put her screen time and character development a few notches above poor Linna, who was often thrust into the background with only her greed and her tendency to eat up boyfriends to get her any attention. Don't get me wrong, I like it and I love the overall concept of it all, but it did irk me a little bit. Also this is one of those runner-ups for \"worst English voice dubbing of all time\" features, meaning you'd better stick to the Japanese. Some of the voices were okay (some really did match their characters personas) but others were just flat and passionless or, in the case of Priss, really overacted.<br /><br />Well, Tokyo 2040 comes along and pretty much tosses all that out the window. Set a few years ahead, the story here is that after earthquakes shattered Tokyo, GENOM's boomers rebuilt the city into a big old paradise, except the boomers still have a tendency to fly off the handle, which prompts the AD Police to be formed followed by the Knight Sabers being formed. So the overall story is the same, but the backstories of the characters and the look and attitudes of the characters have changed a lot.<br /><br />1) Originally Sylia had short purplish black hair and brown eyes, was usually dressed like a stern, proper business woman and was distant from others. 2040 Sylia has more of a super-model look to her, dressing more provocatively and possessing white hair and blue eyes that seem to change color depending on the light (runs the gamut from blue to purple to silver and eyes occasionally looking purple or gray), and also 2040 Sylia is more of an emotionally unstable woman who flies off the handle when she's not in public, and possibly keeps even more secrets than before. Sylia also doesn't take as much risk on the battlefield, as she is more of a stay-in-the-mobile-pit type here, but she does do battle when she has to.<br /><br />2) Originally Priss was a short woman with an Afro and a really bad temper, always picking fights with people who offended her, always biting off more than she could chew, etc. 2040 Priss, however, has gone the way of the Clint Eastwood loner - very cold, very stoic and emotionally distant (more like the original Sylia you might say), so she's not really attached to anyone. Also her hair is more stingry and cat-like (a big improvement) and she is clad in leather like Trinity from \"The Matrix\" (although much less annoying than before, unfortunately, the writers screw her in the end when revealing her reasons for hating the ADP).<br /><br />3) Originally Linna had this big black hair going for her, but now her hair is shorter, browner, and, well, more 90s like. 2040 Linna is also an office lady who has bad luck with being sexually harrassed. As if to apologize for the way she was treated by the OAV writers, the 2040 writers actually dedicated the first 6 episodes to Linna, writing her as a country girl new to the city but determined to meet the Knight Sabers and win a spot with them, which she eventually does.<br /><br />4) Originally a short red haired girl who was often the victim of ridicule and ate a lot of candy, Nene is now a short blonde haired girl who likes to tease and take pot shots at ADP detective Leon McNichol (revenge for him toying with her in the OAV?) and other characters, even her surrogate big sister Linna and Mackey, Sylia's \"brother\", whom she becomes infatuated with. Cockey and arrogant, she still eats a lot of candy and is a master hacker, but she is eventually deflated and grows beyond her comic relief status.<br /><br />5) Nigel Kirkland is a new character, a tall, stoic, ruggedly handsome man with long black hair (he looks like Adrian Paul from TV's Highlander), he replaces Dr. Raven from the old series and now serves as the man who gives maitenance to Sylia's hard suits. Nigel is also Sylia's lover, but you wouldn't know it by his demeanor. He's kind of the father/big brother/mentor figure to Mackey.<br /><br />6) Leon and Daley are back, but of course differently. The original Leon was a tall pretty boy built like a baseball player with slicked back brown hair, blue eyes, a black leather jacket, tight blue jeans, and always carrying a revolver that could magically pack more whallop than a howitzer if necessary; while he wasn't really a bad guy deep down, he was kind of a jerk, but he served mostly as comic relief, as he tried to pursue Priss romantically (exactly what he saw in her is a mystery) but occasionally he and Daley served as information guides to important plot points. Also the original Daley was a fairly muscular red head who dressed in pink/purple suits as he was a flamboyantly homosexual character who was always hitting on Leon when not providing important information. In 2040, Leon is no longer a pretty boy but more your typical rugged tough guy type, with spiked black hair, brown eyes, tall and sporting big muscles, a brown leather jacket and blue dockers (he actually looks like Arnold Schwarzenegger a little bit, or maybe a pumped up Colin Farrell, or Hugh Jackman), and he still carries a revolver, a BIG one, but it's not as powerful as before. Although 2040 Leon still has a bit of an attitude probelm (especially in approaching Priss), he's not nearly as much of a jerk as he was in the old series, but he does have a bad temper and he is easily annoyed by Nene and Daley (also he drinks way too much coffee). Oh, and Leon is still after Priss, but he has a lot more luck this time around. Daley, meanwhile, is now a taller (but not as tall as Leon) more pretty boyish looking guy with red rimmed glasses, a white suit, green eyes, and light brown hair, and he carries a big machine gun (he actually looks like James Marsden from the X-Men films); Daley is a lot smarter and more assertive in 2040 than the OAV and, although it's not completely clear, his homosexual tendencies have been almost totally disappeared, save a moment of what appears to be jealousy when he hears about Leon inquiring about Priss's e-mail.<br /><br />7) Brian J. Mason (what does that \"J\" stand for?) is back, and so is Quincy, but Mason is much more the main villain here, with Quincy as co-villain, as he is no longer a towering figure of terror but a vegetable with a bunch of batteries and wires plugged into him. Mason now sports slicked back brown hair instead of black hair as he did in the OAV (he actually looks like OAV Leon in a suit) and he is very much from the Alan Rickman school of villains.<br /><br />8) Though a pervert in the first series, Mackey is no longer a pervert in 2040. Of course, there are lots of things different about Mackey in 2040, but they won't be revealed here.<br /><br />9) Sylia now has a companion, an Alfred-the-butler type named Henderson, who worries about her and the gang.<br /><br />10) In the original series, boomers were like the Replicants in \"Blade Runner\", armed with their own thoughts and feelings and ambitions, but in 2040, they're more of the dumb-monsters-on-the-rampage type. Most of the time they're just big robots who do whatever they're programmed to do (heavy labor, combat, clean up, etc) and they have this tendency to \"go rogue\", which means try to evolve and become a monster in the process.<br /><br />What does stay the same is the theme of humanity VS technology (do machines have souls?). Sadly, this series, though well animated and well written, only runs 26 episodes, so it moves by faster than one might like, especially those of us who are used to more than one season of our most beloved characters, and unfortunately it still ends on a cliff hanger with unresolved storyline bits (which I will not discuss here. What saves this show and makes it what it is, however, is the characters, a colorful cast of screwballs they are, ranging from stoic loners, psycho women, genocidal mad men, rough neck cops, sardonic intellectuals, wise old sages, and loveable innocents, much more diverse than before and with a lot more to play off of, they're enough to make you wish this show had gone longer.<br /><br />It's not great, but it's a good watch. Also the English dub (by ADV) is quite good, though not without a few flat spots, but certainly better than the dub on the original.\n"
     ]
    }
   ],
   "source": [
    "print('The review:')\n",
    "with open(test_pos_dir+test_pos_rev_files[my_idx], 'r',encoding=\"utf8\") as f:\n",
    "    my_review=f.read()\n",
    "    print(my_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:200\t('Positive', 0.545491099357605)\n",
      "Length:250\t('Positive', 0.5126422643661499)\n",
      "Length:300\t('Negative', 0.46818283200263977)\n",
      "Length:350\t('Negative', 0.17698468267917633)\n",
      "Length:400\t('Positive', 0.743892252445221)\n",
      "Length:450\t('Positive', 0.9037755727767944)\n",
      "Length:500\t('Positive', 0.8443353176116943)\n",
      "Length:550\t('Positive', 0.9468356966972351)\n",
      "Length:600\t('Positive', 0.9333352446556091)\n",
      "Length:650\t('Positive', 0.9651436805725098)\n",
      "Length:700\t('Positive', 0.8169646859169006)\n",
      "Length:750\t('Positive', 0.502206027507782)\n",
      "Length:800\t('Positive', 0.5434826612472534)\n",
      "Length:850\t('Positive', 0.6555498242378235)\n",
      "Length:900\t('Negative', 0.4059029519557953)\n",
      "Length:950\t('Positive', 0.601677417755127)\n",
      "Length:1000\t('Negative', 0.47310540080070496)\n",
      "Length:1050\t('Positive', 0.5247160196304321)\n",
      "Length:1100\t('Negative', 0.15493106842041016)\n",
      "Length:1150\t('Negative', 0.2618597745895386)\n",
      "Length:1200\t('Negative', 0.1979038417339325)\n",
      "Length:1250\t('Negative', 0.36529818177223206)\n",
      "Length:1300\t('Negative', 0.2852105498313904)\n",
      "Length:1350\t('Positive', 0.7716877460479736)\n",
      "Length:1400\t('Positive', 0.7396081686019897)\n",
      "Length:1450\t('Negative', 0.1659281700849533)\n",
      "Length:1500\t('Negative', 0.2105710804462433)\n",
      "Length:1550\t('Negative', 0.3836711645126343)\n",
      "Length:1600\t('Positive', 0.8241234421730042)\n",
      "Length:1650\t('Negative', 0.078262560069561)\n",
      "Length:1700\t('Negative', 0.12986257672309875)\n",
      "Length:1750\t('Negative', 0.12248377501964569)\n",
      "Length:1800\t('Positive', 0.5221366882324219)\n",
      "Length:1850\t('Negative', 0.48239222168922424)\n",
      "Length:1900\t('Negative', 0.46430230140686035)\n",
      "Length:1950\t('Positive', 0.7802841663360596)\n",
      "Length:2000\t('Positive', 0.595796525478363)\n",
      "Length:2050\t('Positive', 0.8408015370368958)\n",
      "Length:2100\t('Positive', 0.5056344866752625)\n",
      "Length:2150\t('Positive', 0.89396071434021)\n"
     ]
    }
   ],
   "source": [
    "for sl in range(200,len(test_reviews_int[my_idx]),50):\n",
    "    print(f\"Length:{sl}\\t{predict(net, my_review, sequence_length=sl)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again the probabilities strongly depend on the sequence length (0.08-0.95). No pattern on the variation arises.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
