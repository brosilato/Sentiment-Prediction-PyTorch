{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with an CNN\n",
    "\n",
    "In this notebook, I implement a convolutional neural network (CNN) that will be trained using the traning set of the IMDB reviews database and it will be tested on its corresponding testing set. The databases has been downloaded from:\n",
    "__[http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)__\n",
    "\n",
    "Words will be turned into numerical vector by the means of an embedding layer. The word vector sequences will be first \"analyzed\" using one (words), three (trigrams), five (pentagrams) and seven-word (heptagrams) wide filters. \n",
    "\n",
    "The model is implemented in python using pyTorch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Let's import the modules we will use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " #print(plt.style.available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPUs?\n",
    "Let's check if a GPU is available and select the device use for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if it's available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load in, tokenize and visualize the data\n",
    "\n",
    "The download data is already divied into train and test data. Each folder is further divided into positive (7-10/10 stars reviews) and negative reviews (1-4/10 stars reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SET\n",
      "Number of positive reviews: 12500 Number of negative reviews: 12500\n"
     ]
    }
   ],
   "source": [
    "#training positive reviews directory\n",
    "train_pos_dir = r'aclImdb/train/pos/'\n",
    "#negative positive reviews directory\n",
    "train_neg_dir = r'aclImdb/train/neg/'\n",
    "\n",
    "#List of files with training positive review\n",
    "train_pos_rev_files = os.listdir(train_pos_dir)\n",
    "#List of files with training negative review\n",
    "train_neg_rev_files = os.listdir(train_neg_dir)\n",
    "print('TRAIN SET')\n",
    "print('Number of positive reviews:',len(train_pos_rev_files),'Number of negative reviews:',len(train_neg_rev_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep track of the number of stars (1-10) awarded to each positive and negative review in the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_stars = []\n",
    "for file in train_pos_rev_files:\n",
    "    full_train_stars.append(int(file.split('_')[1].split('.')[0]))\n",
    "for file in train_neg_rev_files:\n",
    "    full_train_stars.append(int(file.split('_')[1].split('.')[0]))    \n",
    "full_train_stars = np.array(full_train_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST SET\n",
      "Number of positive reviews: 12500 Number of negative reviews: 12500\n"
     ]
    }
   ],
   "source": [
    "#training positive reviews directory\n",
    "test_pos_dir = r'aclImdb/test/pos/'\n",
    "#negative positive reviews directory\n",
    "test_neg_dir = r'aclImdb/test/neg/'\n",
    "\n",
    "#List of files with training positive review\n",
    "test_pos_rev_files = os.listdir(test_pos_dir)\n",
    "#List of files with training negative review\n",
    "test_neg_rev_files = os.listdir(test_neg_dir)\n",
    "print('TEST SET')\n",
    "print('Number of positive reviews:',len(test_pos_rev_files),'Number of negative reviews:',len(test_neg_rev_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep track of the number of stars (1-10) awarded to each positive and negative review in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stars = []\n",
    "for file in test_pos_rev_files:\n",
    "    test_stars.append(int(file.split('_')[1].split('.')[0]))\n",
    "for file in test_neg_rev_files:\n",
    "    test_stars.append(int(file.split('_')[1].split('.')[0]))    \n",
    "test_stars = np.array(test_stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that 5- and 6-star reviews are not included in the train or test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  7  8  9 10]\n",
      "[ 1  2  3  4  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(full_train_stars))\n",
    "print(np.unique(test_stars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Matrices\n",
    "Let's create numpy arrays that hold the train and test labels. 1 stands for positive and 0 for negative. Since we will stack first the positive reviews and the the negative ones, the first 12500 elements are ones and the next 12500 are zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train target array\n",
    "full_train_target = np.zeros(len(train_neg_rev_files)+len(train_neg_rev_files), dtype=int)\n",
    "full_train_target[:len(train_neg_rev_files)] = 1\n",
    "#Test target array\n",
    "test_target = np.zeros(len(test_neg_rev_files)+len(test_neg_rev_files), dtype=int)\n",
    "test_target[:len(test_neg_rev_files)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,)\n",
      "0.5 0.5\n"
     ]
    }
   ],
   "source": [
    "print(full_train_target.shape, test_target.shape)\n",
    "print(full_train_target.mean(), test_target.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Contractions\n",
    "The following function expands common english contractions. There is obviously  plenty of room for improvement. Some \n",
    "actions are 100% justified (can't and cannot into can not), while others are rather arbitrary ('s into is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_contractions(text):\n",
    "    # Turn ain't into am not (it could be many other oprions such us is not,...)\n",
    "    text = text.replace(\"ain't\", \"am not\")\n",
    "    #Turn can't and annot into can not\n",
    "    text = text.replace(\"can't\", \"can not\").replace(\"cannot\", \"can not\")\n",
    "    #Turn shan't into shall not\n",
    "    text = text.replace(\"shan't\", \"shall not\")\n",
    "    #Turn won't into will not\n",
    "    text = text.replace(\"won't\", \"will not\")\n",
    "    #Turn y'all into you all\n",
    "    text = text.replace(\"y'all\", \"you all\")\n",
    "    #Turn n't into not\n",
    "    text = text.replace(\"n't\", \" not\")\n",
    "    #Turn 'd into would (it might be had too)\n",
    "    text = text.replace(\"'d\", \" would\")\n",
    "    #Turn 'll into will\n",
    "    text = text.replace(\"'ll\", \" will\")\n",
    "    #Turn 're into are\n",
    "    text = text.replace(\"'re\", \" are\")\n",
    "    #Turn 'm into am\n",
    "    text = text.replace(\"'m\", \" am\")\n",
    "    #Turn 's into is (it could also be has or the posssesive)\n",
    "    text = text.replace(\"'s\", \" is\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviews as List of Words\n",
    "For each review we convert every character to lower case, remove english contructions, then remove punctuation and finally split it into words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_reviews=[]\n",
    "#Read and tokenize positive reviews\n",
    "for file_name in train_pos_rev_files:\n",
    "    with open(train_pos_dir+file_name, 'r',encoding=\"utf8\") as f:\n",
    "        review = f.read().lower()\n",
    "        #remove some contractions\n",
    "        review = remove_contractions(review)\n",
    "        #remove punctuation\n",
    "        review = ''.join([c for c in review if c not in punctuation])\n",
    "        #split in words\n",
    "        review = review.split()  \n",
    "    full_train_reviews.append(review)\n",
    "\n",
    "#Read and tokenize negative reviews\n",
    "for file_name in train_neg_rev_files:\n",
    "    with open(train_neg_dir+file_name, 'r',encoding=\"utf8\") as f:\n",
    "        review = f.read().lower()\n",
    "        #remove some contractions\n",
    "        review = remove_contractions(review)\n",
    "        #remove punctuation\n",
    "        review = ''.join([c for c in review if c not in punctuation])\n",
    "        #split in words\n",
    "        review = review.split()  \n",
    "    full_train_reviews.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "['br', 'br', 'if', 'you', 'like', 'rap', 'or', 'hiphop', 'watch', 'this', 'movie', 'although', 'it', 'is', 'funny', 'if', 'you', 'do', 'not', 'get', 'the', 'references', 'as', 'a', 'straight', 'comedybr', 'br', 'have', 'not', 'seen', 'much', 'of', 'the', 'much', 'hyped', 'cb4', 'but', 'what', 'i', 'did', 'see', 'did', 'not', 'have', 'the', 'heart', 'that', 'this', 'little', 'stormer', 'hasbr', 'br', 'have', 'not', 'heard', 'from', 'the', 'people', 'involved', 'since', 'which', 'is', 'a', 'surprise', 'the', 'film', 'is', 'very', 'similar', 'to', 'spinal', 'tap', 'which', 'is', 'no', 'bad', 'thing', 'and', 'i', 'think', 'a', 'lot', 'of', 'the', 'dialogue', 'while', 'priceless', 'in', 'tap', 'is', 'funnier', 'here', 'probably', 'because', 'i', 'am', 'more', 'into', 'rap', 'than', 'rock', 'theses', 'days', 'so', 'my', 'own', 'judgment', 'does', 'cloud', 'that', 'pointbr', 'br', 'the', 'rap', 'songs', 'are', 'funny', 'as', 'hell', 'and', 'it', 'is', 'basically', 'spot', 'the', 'reference', 'for', 'most', 'of', 'the', 'film', 'not', 'all', 'of', 'them', 'are', 'inyourface', 'which', 'means', 'the', 'physical', 'comedy', 'and', 'the', 'oneliners', 'get', 'priority', 'over', 'the', 'takeoffsbr', 'br', 'great', 'fun', 'one', 'to', 'watch', 'twice', 'if', 'there', 'ever', 'was', 'a', 'movie']\n"
     ]
    }
   ],
   "source": [
    "print(len(full_train_reviews))\n",
    "print(full_train_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reviews=[]\n",
    "#Read and tokenize positive reviews\n",
    "for file_name in test_pos_rev_files:\n",
    "    with open(test_pos_dir+file_name, 'r',encoding=\"utf8\") as f:\n",
    "        review = f.read().lower()\n",
    "        #remove some contractions\n",
    "        review = remove_contractions(review)\n",
    "        #remove punctuation\n",
    "        review = ''.join([c for c in review if c not in punctuation])\n",
    "        #split in words\n",
    "        review = review.split()  \n",
    "    test_reviews.append(review)\n",
    "\n",
    "#Read and tokenize negative reviews\n",
    "for file_name in test_neg_rev_files:\n",
    "    with open(test_neg_dir+file_name, 'r',encoding=\"utf8\") as f:\n",
    "        review = f.read().lower()\n",
    "        #remove some contractions\n",
    "        review = remove_contractions(review)\n",
    "        #remove punctuation\n",
    "        review = ''.join([c for c in review if c not in punctuation])\n",
    "        #split in words\n",
    "        review = review.split()  \n",
    "    test_reviews.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "['the', 'dresser', 'is', 'perhaps', 'the', 'most', 'refined', 'of', 'backstage', 'films', 'the', 'film', 'is', 'brimming', 'with', 'wit', 'and', 'spirit', 'for', 'the', 'most', 'part', 'provided', 'by', 'the', 'energetic', 'character', 'of', 'norman', 'tom', 'courtenay', 'although', 'his', 'character', 'is', 'clearly', 'gay', 'and', 'certainly', 'has', 'an', 'attraction', 'for', 'the', 'lead', 'performer', 'albert', 'finney', 'that', 'he', 'assists', 'the', 'film', 'never', 'dwells', 'on', 'it', 'or', 'makes', 'it', 'more', 'than', 'it', 'isbr', 'br', 'the', 'gritty', 'style', 'of', 'peter', 'yates', 'that', 'worked', 'so', 'well', 'in', 'bullitt', 'is', 'again', 'on', 'display', 'and', 'gives', 'the', 'film', 'a', 'sense', 'of', 'realism', 'and', 'coherence', 'this', 'is', 'much', 'appreciated', 'in', 'a', 'story', 'that', 'could', 'so', 'easily', 'have', 'become', 'tedious', 'in', 'the', 'end', 'the', 'dresser', 'will', 'bore', 'many', 'people', 'silly', 'but', 'it', 'will', 'truly', 'be', 'a', 'delight', 'to', 'those', 'who', 'love', 'british', 'cinemabr', 'br', '77', 'out', 'of', '10']\n"
     ]
    }
   ],
   "source": [
    "print(len(test_reviews))\n",
    "print(test_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Sets\n",
    "\n",
    "We will divide our full train set into a train set and a validation set that will help us to control how our model generalize.\n",
    "The validation set will be just 10% of the original train set. We envision that depending of the star calification of the review it would be more or less complicated to infer its sentiment. Then, we will keep the same distribution of stars within the train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#90% will remain as training data\n",
    "train_size = 0.9; test_size = 1-train_size\n",
    "(train_reviews, valid_reviews, train_y, valid_y, \n",
    " train_stars, valid_stars) = train_test_split(full_train_reviews, full_train_target, full_train_stars,\n",
    "                                              random_state=123, shuffle=True,\n",
    "                                              train_size=train_size, test_size=test_size,stratify=full_train_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22500 (22500,) (22500,)\n",
      "2500 (2500,) (2500,)\n",
      "0.5 0.5\n"
     ]
    }
   ],
   "source": [
    "#Print out the shapes of your resultant feature data\n",
    "print(len(train_reviews), train_y.shape, train_stars.shape)\n",
    "print(len(valid_reviews), valid_y.shape, valid_stars.shape)\n",
    "print(train_y.mean(), valid_y.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To keep it simple let's create a test_y variable\n",
    "test_y = test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing the size of the reviews\n",
    "It is important to xhoose now the word size of our review before we build our vocabulary. That will prevent us from including\n",
    "words that may appear frequently in our reviews if we keep all the words but not more than five times if we keep, \n",
    "let's say, the first 50 words only. Of course, the larger the sequence considered the less important this would be.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Sequence length #########\n",
    "seq_length = 350"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "\n",
    "Let's get our data into the proper form to feed into the network. Since we're using embedding layers, we'll need to encode each word with an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put our the words of our training set into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4560680\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "for review in train_reviews:\n",
    "    all_words.extend(review[:seq_length]) #Only words that are going to be used in the model\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'worst',\n",
       " 'movie',\n",
       " 'i',\n",
       " 'ever',\n",
       " 'paid',\n",
       " 'to',\n",
       " 'see',\n",
       " 'and',\n",
       " 'with',\n",
       " 'the',\n",
       " 'exception',\n",
       " 'of',\n",
       " 'they',\n",
       " 'saved',\n",
       " 'hitler',\n",
       " 'is',\n",
       " 'brain',\n",
       " 'the',\n",
       " 'worst',\n",
       " 'movie',\n",
       " 'i',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'seen',\n",
       " 'period',\n",
       " 'when',\n",
       " 'this']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the words\n",
    "\n",
    "The embedding lookup requires that we pass in integers to our network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Then we can convert each of our reviews into integers so they can be passed into the network.Also we are going to keep the value zero to represent the padding and the value 1 to represent every word\n",
    "that appears least than five times in our train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "## Build a dictionary that maps words to integers\n",
    "counts = Counter(all_words)\n",
    "# Let's reorder counts by word fewuency so the least common words are at the end\n",
    "words = sorted(counts, key=counts.get, reverse=True)\n",
    "\n",
    "#Remove words that appear less than 5 times\n",
    "i = 0\n",
    "for val in counts.values():\n",
    "    if val<5:\n",
    "        i+=1\n",
    "#Let's keep the most common words and add '<pad>' and '<unk>'        \n",
    "words = ['<pad>','<unk>'] + words[:-i]\n",
    "#Create a dictionary, value 1 will indicate unseen or seen less than 5 times word (<unk>). Zero is for padding <pad>\n",
    "word_to_int = {word: i for i,word in enumerate(words)}\n",
    "\n",
    "# Let's tokenize the reviews into integers\n",
    "# train set\n",
    "train_reviews_int = []\n",
    "for review in train_reviews:\n",
    "    train_reviews_int.append([word_to_int.get(word,1) for word in review])\n",
    "# validation set\n",
    "valid_reviews_int = []\n",
    "for review in valid_reviews:\n",
    "    valid_reviews_int.append([word_to_int.get(word,1) for word in review])\n",
    "# test set\n",
    "test_reviews_int = []\n",
    "for review in test_reviews:\n",
    "    test_reviews_int.append([word_to_int.get(word,1) for word in review])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove: 75416 \t Kept: 27116\n"
     ]
    }
   ],
   "source": [
    "print(f'Remove: {i} \\t Kept: {len(words)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test your code**\n",
    "\n",
    "As a text that you've implemented the dictionary correctly, print out the number of unique words in your vocabulary and the contents of the first, tokenized review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:  27116\n",
      "\n",
      "Tokenized review: \n",
      " [11, 3, 2, 222, 17, 10, 121, 1413, 7, 69, 5, 18, 2, 1309, 6, 33, 1847, 1848, 3, 1261, 2, 222, 17, 10, 25, 121, 106, 809, 54, 11, 17, 349, 47, 10, 15, 4, 185, 310, 6, 3982, 5, 13131, 5, 1571, 15, 12099, 48, 10, 183, 51, 28, 2, 1347, 17, 12, 82, 28, 1076, 220, 8, 72, 13, 25, 2, 11660, 2, 228, 17867, 264, 206, 845, 18, 14, 14, 2, 883, 206, 25, 183, 74, 58, 1313, 9, 49, 22152, 49, 20381, 569, 49, 708, 383, 5, 1310, 74, 25, 4, 11661, 510, 15853, 1293, 199, 2, 474, 118, 5328, 22, 156, 55, 75, 708, 16809, 186, 13132, 23, 267, 3, 97, 4, 845, 18, 365, 22152, 2, 782, 19014, 186, 129, 1231, 2, 681, 232, 5, 40, 1398, 15, 42, 1005, 80, 308, 363, 453, 2, 22153, 1144, 9966, 15, 4536, 453, 19, 1, 868, 2, 987, 6490, 5329, 15, 1906, 453, 738, 5112, 1, 2, 726, 8939, 1297, 181, 1980, 1340, 12100, 39, 3, 60, 1454, 18, 4, 726, 1154, 15, 78, 24, 81, 57, 5551, 14, 2, 65, 150, 5329, 535, 7976, 1262, 67, 4111, 19, 4, 213, 843, 11, 793, 24, 81, 1907, 1617, 6, 706, 9, 11, 17, 437, 9417, 179, 306, 2, 704, 41, 2, 411, 669, 113, 8149, 9, 2, 2782, 3, 5, 24, 58, 38, 11, 17, 46, 24, 22, 308, 78, 90, 6, 2, 669, 15, 13, 8149, 9, 2, 1200, 3, 46, 2, 2582, 883, 68, 8149, 8, 206, 25, 83, 23, 1, 24325, 6958, 39, 68, 608, 85, 77, 9, 2, 709, 1289, 491, 54, 10, 187, 11, 17, 6, 254, 648, 72, 13, 81, 47, 242, 41, 89, 80, 2, 17, 15]\n",
      "22500\n"
     ]
    }
   ],
   "source": [
    "# stats about vocabulary\n",
    "print('Unique words: ', len((word_to_int)))  \n",
    "print()\n",
    "\n",
    "# print tokens in first review\n",
    "print('Tokenized review: \\n', train_reviews_int[0])\n",
    "print(len(train_reviews_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of the Reviews \n",
    "\n",
    "We have already set for a value of 500 words but it is convinient to check some statistic about the review lenght (mean,\n",
    "median, std). We may even decide to run the whole notebook with a different sequence lenght."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum review length: 10\n",
      "Maximum review length: 2501\n",
      "Mean and Median review length: 237.54746666666668\t177.0\n",
      "Std review length: 175.75619227979803\n"
     ]
    }
   ],
   "source": [
    "review_lens = np.array([len(x) for x in train_reviews_int])\n",
    "\n",
    "print(f\"Minimum review length: {review_lens.min()}\")\n",
    "print(f\"Maximum review length: {review_lens.max()}\")\n",
    "print(f'Mean and Median review length: {review_lens.mean()}\\t{np.median(review_lens)}')\n",
    "print(f'Std review length: {review_lens.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Padding sequences\n",
    "\n",
    "To deal with both short and very long reviews, we'll pad or truncate all our reviews to a specific length. For reviews shorter than some `seq_length`, we'll pad with 0s. For reviews longer than `seq_length`, we can truncate them to the first `seq_length` words. We will work with a sewuence lenght of 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length, alignment='right'):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    features=np.zeros((len(reviews_ints),seq_length),dtype=int)\n",
    "    \n",
    "    if alignment.lower() == 'right':\n",
    "        my_idxs = lambda length: ((seq_length-length),seq_length) \n",
    "    elif alignment.lower() == 'left':\n",
    "        my_idxs = lambda length: (0,seq_length-(seq_length-length)) \n",
    "    elif alignment.lower() == 'center':\n",
    "        my_idxs = lambda length: ((seq_length-length)//2,seq_length-((seq_length-length)-(seq_length-length)//2))\n",
    "    else:\n",
    "        print(alignment, 'is not a valid option for alignment')\n",
    "        pritn('options: right, left, center')\n",
    "        return None\n",
    "    \n",
    "    for i,review in enumerate(reviews_ints):\n",
    "        text = review[:seq_length]\n",
    "        (a,b) = my_idxs(len(text))\n",
    "        features[i,a:b] = text\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0    11     3     2   222    17    10\n",
      "    121  1413     7    69     5    18     2  1309     6    33  1847  1848\n",
      "      3  1261     2   222    17    10    25   121   106   809    54    11\n",
      "     17   349    47    10    15     4   185   310     6  3982     5 13131\n",
      "      5  1571    15 12099    48    10   183    51    28     2  1347    17\n",
      "     12    82    28  1076   220     8    72    13    25     2 11660     2\n",
      "    228 17867   264   206   845    18    14    14     2   883   206    25\n",
      "    183    74    58  1313     9    49 22152    49 20381   569    49   708\n",
      "    383     5  1310    74    25     4 11661   510 15853  1293   199     2\n",
      "    474   118  5328    22   156    55    75   708 16809   186 13132    23\n",
      "    267     3    97     4   845    18   365 22152     2   782 19014   186\n",
      "    129  1231     2   681   232     5    40  1398    15    42  1005    80\n",
      "    308   363   453     2 22153  1144  9966]]\n",
      "[[   15  4536   453    19     1   868     2   987  6490  5329    15  1906\n",
      "    453   738  5112     1     2   726  8939  1297   181  1980  1340 12100\n",
      "     39     3    60  1454    18     4   726  1154    15    78    24    81\n",
      "     57  5551    14     2    65   150  5329   535  7976  1262    67  4111\n",
      "     19     4   213   843    11   793    24    81  1907  1617     6   706\n",
      "      9    11    17   437  9417   179   306     2   704    41     2   411\n",
      "    669   113  8149     9     2  2782     3     5    24    58    38    11\n",
      "     17    46    24    22   308    78    90     6     2   669    15    13\n",
      "   8149     9     2  1200     3    46     2  2582   883    68  8149     8\n",
      "    206    25    83    23     1 24325  6958    39    68   608    85    77\n",
      "      9     2   709  1289   491    54    10   187    11    17     6   254\n",
      "    648    72    13    81    47   242    41    89    80     2    17    15\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "train_x = pad_features(train_reviews_int, seq_length=seq_length, alignment='center')\n",
    "valid_x = pad_features(valid_reviews_int, seq_length=seq_length, alignment='center')\n",
    "test_x = pad_features(test_reviews_int, seq_length=seq_length, alignment='center')\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(train_x)==len(train_reviews_int), \"Your features should have as many rows as reviews.\"\n",
    "assert len(train_x[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# Print the first review \n",
    "print(train_x[:1,:seq_length//2])\n",
    "print(train_x[:1,seq_length//2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DataLoaders and Batching\n",
    "\n",
    "After creating training, test, and validation data, we can create DataLoaders for batching our data into the net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x).long(), torch.from_numpy(train_y).long())\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_x).long(), torch.from_numpy(valid_y).long())\n",
    "test_data = TensorDataset(torch.from_numpy(test_x).long(), torch.from_numpy(test_y).long())\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader_50 = DataLoader(train_data, shuffle=True, batch_size=50)\n",
    "train_loader_100 = DataLoader(train_data, shuffle=True, batch_size=100)\n",
    "train_loader_150 = DataLoader(train_data, shuffle=True, batch_size=150)\n",
    "valid_loader = DataLoader(valid_data, shuffle=False, batch_size=100) #No need to shuffle\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=100) #No need to shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 350])\n",
      "Sample input: \n",
      " tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,    10,    15,     1,    32,     2,  1467,  3442,\n",
      "         8253,     6,     4,  6794,    62,    67,     3,  1750,     7,    90,\n",
      "           77,     2,   361,    68, 20252,     5,  1292,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
      "        0, 0])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader_50)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x[0])\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Sentiment Network with PyTorch\n",
    "\n",
    "As mentioned we will use a Convolutional Neural Network to analyze the sentiment of reviews. \n",
    "It is important to notice that before any convolutional network the words will be converted into 128-dimensions vectors throughout an ambedding layer, and also that the first filter of the convolution will get ride of those 128 dimensions returning 1-dimension points. The first filters will process the original features of the reviews as words, 3-grams and 5-grams. After the first maxpooling layer, the three lines will be concatenated and process together.\n",
    "It is worth mentioning that at the end of the convolutional part of the network a fixed number of values per filter (top_k) will be kept, which will garantee that indenpendently of the length of the sequence, always the same number of features are provided to the fully connected network as inputs. If avg=True, only the mean of those top_k values is provided. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The CNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, dp_2d=0.25, dp_fc=0.25, seq_length=None, top_k=3, avg=True):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentCNN, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_size = output_size\n",
    "        self.seq_length = seq_length\n",
    "        self.dp_2d = dp_2d\n",
    "        self.dp_fc = dp_fc        \n",
    "        self.top_k = top_k \n",
    "        self.avg = avg\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=0 )\n",
    "        \n",
    "        # Convolutional network\n",
    "        self.conv_1a = nn.Conv2d(1,8,kernel_size=(1,self.embedding_dim),stride=(1,1),padding=(0,0)) #1-grams\n",
    "        self.conv_1b = nn.Conv2d(1,16,kernel_size=(3,self.embedding_dim),stride=(1,1),padding=(1,0)) #3-grams\n",
    "        self.conv_1c = nn.Conv2d(1,32,kernel_size=(5,self.embedding_dim),stride=(1,1),padding=(2,0)) #5-grams\n",
    "                \n",
    "        self.conv_2a = nn.Conv2d(8,8,kernel_size=(3,1),stride=1,padding=(1,0))\n",
    "        self.conv_2b = nn.Conv2d(16,16,kernel_size=(3,1),stride=1,padding=(1,0))\n",
    "        self.conv_2c = nn.Conv2d(32,32,kernel_size=(3,1),stride=1,padding=(1,0))\n",
    "        \n",
    "        self.maxpool_3 = nn.MaxPool2d(kernel_size=(3,1),stride=(3,1),padding=0)\n",
    "        \n",
    "        self.conv_3 = nn.Conv2d(56,112,kernel_size=(3,1),stride=1,padding=(1,0))\n",
    "        self.conv_4 = nn.Conv2d(112,112,kernel_size=(3,1),stride=1,padding=(1,0))\n",
    "         \n",
    "        self.conv_5 = nn.Conv2d(112,224,kernel_size=(3,1),stride=1,padding=(1,0))\n",
    "        self.conv_6 = nn.Conv2d(224,224,kernel_size=(3,1),stride=1,padding=(1,0))\n",
    "        self.conv_7 = nn.Conv2d(224,224,kernel_size=(3,1),stride=1,padding=(1,0))\n",
    "        \n",
    "        #size = (((((self.seq_length-6-2)//3)-2-2)//3)-2-2)\n",
    "        #self.avgpool = nn.AvgPool2d(kernel_size=(size,1),stride=(size,1),padding=0)\n",
    "        \n",
    "        #Fully connected\n",
    "        if self.avg:\n",
    "            self.fc1 = nn.Linear(224,512)\n",
    "        else:    \n",
    "            self.fc1 = nn.Linear(224 * self.top_k,512)\n",
    "        self.fc2 = nn.Linear(512,128)\n",
    "        self.fc3 = nn.Linear(128, self.output_size)\n",
    "        \n",
    "        # dropout layers\n",
    "        self.dropout2d = nn.Dropout2d(dp_2d)\n",
    "        self.dropout = nn.Dropout(dp_fc)\n",
    "        \n",
    "        # relu and sigmoid layers\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        ####### Embedding #######\n",
    "        x = self.embedding(x)\n",
    "        x = torch.unsqueeze(x, dim=1)\n",
    "        \n",
    "        ####### CNN ###########\n",
    "        #First layer\n",
    "        line_a = self.relu(self.conv_1a(x))\n",
    "        line_b = self.relu(self.conv_1b(x))\n",
    "        line_c = self.relu(self.conv_1c(x))\n",
    "             \n",
    "        line_a = self.dropout2d(self.relu(self.conv_2a(line_a)))\n",
    "        line_b = self.dropout2d(self.relu(self.conv_2b(line_b)))\n",
    "        line_c = self.dropout2d(self.relu(self.conv_2c(line_c)))\n",
    "        \n",
    "        #MaxPooling\n",
    "        line_a = self.maxpool_3(line_a)\n",
    "        line_b = self.maxpool_3(line_b)\n",
    "        line_c = self.maxpool_3(line_c)\n",
    "        \n",
    "        #Merge Featre Maps\n",
    "        x = torch.cat((line_a,line_b,line_c),dim=1)\n",
    "        \n",
    "        #Second layer\n",
    "        x = self.relu(self.conv_3(x))\n",
    "        x = self.dropout2d(self.relu(self.conv_4(x)))\n",
    "        \n",
    "        #MaxPooling\n",
    "        x = self.maxpool_3(x)\n",
    "        \n",
    "        #Third layer\n",
    "        x = self.relu(self.conv_5(x))\n",
    "        x = self.relu(self.conv_6(x))\n",
    "        x = self.dropout2d(self.relu(self.conv_7(x)))\n",
    "        \n",
    "        #MaxPooling\n",
    "        x = self.maxpool_3(x)\n",
    "                             \n",
    "        #Extract top_k per channel\n",
    "        x, _ = torch.topk(x,self.top_k,dim=2)\n",
    "        \n",
    "        if self.avg:\n",
    "            x = F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "            \n",
    "        #Flatten the array\n",
    "        if self.avg:\n",
    "            x = x.view(-1, 224)\n",
    "        else:\n",
    "            x= x.view(-1, 224 * self.top_k)\n",
    "        \n",
    "        #### Fully Connected ####\n",
    "        x = self.relu(self.dropout(self.fc1(x)))\n",
    "        x = self.relu(self.dropout(self.fc2(x)))\n",
    "        x = self.sig(self.fc3(x))\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return x\n",
    "    \n",
    "    def freeze_emb(self):\n",
    "        print(\"Embedding won't be trained\")\n",
    "        for param in self.embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_emb(self):\n",
    "        print(\"Embedding will be trained\")\n",
    "        for param in self.embedding.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def get_params(self):\n",
    "        params = {'vocab_size': self.vocab_size, 'output_size': self.output_size, \n",
    "                  'embedding_dim': self.embedding_dim, 'seq_length': self.seq_length, \n",
    "                  'dp_2d': self.dp_2d, 'dp_fc': self.dp_fc, 'top_k': self.top_k, 'avg': self.avg}\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the network\n",
    "\n",
    "Here, we'll instantiate the network. First up, defining the hyperparameters.\n",
    "\n",
    "* `vocab_size`: Size of our vocabulary or the range of values for our input, word tokens.\n",
    "* `output_size`: Size of our desired output; the number of class scores we want to output (pos/neg).\n",
    "* `embedding_dim`: Number of columns in the embedding lookup table; size of our embeddings.\n",
    "* `dp_2d`: drop out regularization for the convolutional part\n",
    "* `dp_fc`: drop out regularization for the fully connected part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentCNN(\n",
       "  (embedding): Embedding(27116, 128, padding_idx=0)\n",
       "  (conv_1a): Conv2d(1, 8, kernel_size=(1, 128), stride=(1, 1))\n",
       "  (conv_1b): Conv2d(1, 16, kernel_size=(3, 128), stride=(1, 1), padding=(1, 0))\n",
       "  (conv_1c): Conv2d(1, 32, kernel_size=(5, 128), stride=(1, 1), padding=(2, 0))\n",
       "  (conv_2a): Conv2d(8, 8, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "  (conv_2b): Conv2d(16, 16, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "  (conv_2c): Conv2d(32, 32, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "  (maxpool_3): MaxPool2d(kernel_size=(3, 1), stride=(3, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_3): Conv2d(56, 112, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "  (conv_4): Conv2d(112, 112, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "  (conv_5): Conv2d(112, 224, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "  (conv_6): Conv2d(224, 224, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "  (conv_7): Conv2d(224, 224, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "  (fc1): Linear(in_features=224, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (dropout2d): Dropout2d(p=0.2)\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (relu): ReLU()\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "net_params = {'vocab_size': len(word_to_int), 'output_size': 1, 'embedding_dim': 128, 'seq_length': seq_length,\n",
    "              'dp_2d': 0.2, 'dp_fc': 0.5, 'top_k': 3, 'avg': True}\n",
    "net = SentimentCNN(**net_params)\n",
    "\n",
    "#Save the encoding dictionary and the list of words\n",
    "net.words = words\n",
    "net.word_to_int = word_to_int\n",
    "#Move to gpu or cpu device\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training\n",
    "\n",
    "Let's train our net. We will use the Adam optimizer and we evaluate the model every few steps and after each epoch. We will save the best model based on the evaluation loss\n",
    "\n",
    "Training hyparameters:\n",
    "\n",
    "* `lr`: Learning rate for our optimizer.\n",
    "* `L2`: L2 regularization used.\n",
    "* Batch size: we will used the a data loader with a batch size of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr = 0.001\n",
    "L2 = 0\n",
    "criterion = nn.BCELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function saves the model, optimizer and other parameters into a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(path=None, model=None, optimizer=None, epoch=None, train_loss=None, valid_loss=None, params=None,\n",
    "                   word_to_int=None, words=None):\n",
    "    if path:\n",
    "        my_path=path\n",
    "        print('Using', my_path, 'to save')\n",
    "    else:\n",
    "        my_path='my_model.pt'\n",
    "        print('Using', my_path, 'to save')\n",
    "        \n",
    "    checkpoint = {}\n",
    "    \n",
    "    if model:\n",
    "        checkpoint['model_state_dict']= model.state_dict()\n",
    "    else:\n",
    "        print('No model dictionary saved')\n",
    "    \n",
    "    if params:\n",
    "        checkpoint['params'] = params\n",
    "    else:\n",
    "        print('No model parameters saved')\n",
    "        \n",
    "    if optimizer:\n",
    "        checkpoint['optimizer_state_dict']= optimizer.state_dict()\n",
    "    else:\n",
    "        print('NNo optimizer dictionary saved')\n",
    "        \n",
    "    if epoch:\n",
    "        checkpoint['epoch'] = epoch\n",
    "    else:\n",
    "        print('No current epoch value saved')\n",
    "        \n",
    "    if train_loss:\n",
    "        checkpoint['train_loss'] = train_loss\n",
    "    else:\n",
    "        print('No value of the training loss saved')\n",
    "        \n",
    "    if valid_loss:\n",
    "        checkpoint['valid_loss'] = valid_loss\n",
    "    else:\n",
    "        print('No value of the validation loss saved')\n",
    "        \n",
    "    if word_to_int:\n",
    "        checkpoint['word_to_int'] = word_to_int\n",
    "    else:\n",
    "        print('No dictionary for encoding words saved')\n",
    "        \n",
    "    if words:\n",
    "        checkpoint['words'] = words\n",
    "    else:\n",
    "        print('No list of words saved')\n",
    "    \n",
    "    torch.save(checkpoint, my_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function loads the model, optimizer and other parameters from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path=None, model=None, optimizer=None):\n",
    "    '''\n",
    "    It overrrides the model and optimizer provided with the saved parameters \n",
    "    and returns the saved epoch, train loss and validation loss'''\n",
    "    if path:\n",
    "        checkpoint = torch.load(path, map_location='cpu') \n",
    "    else:\n",
    "        print('Nothing loaded. Plese provide a file')\n",
    "        return None\n",
    "        \n",
    "    #Load the model state dictionary\n",
    "    my_dict = checkpoint.get('model_state_dict', None)\n",
    "    if my_dict:\n",
    "        model.load_state_dict(my_dict)\n",
    "        model.word_to_int = checkpoint.get('word_to_int', None)\n",
    "        model.words = checkpoint.get('words', None)\n",
    "    else:\n",
    "        print('No model dictionary found')\n",
    "    \n",
    "    #Load the optimizer state dictionary\n",
    "    my_dict = checkpoint.get('optimizer_state_dict', None)\n",
    "    if my_dict:\n",
    "        optimizer.load_state_dict(my_dict)\n",
    "    else:\n",
    "        print('No optimizer dictionary found')    \n",
    "    \n",
    "    #Load the epoch value, train loss and validation loss\n",
    "    epoch = checkpoint.get('epoch', None)\n",
    "    train_loss = checkpoint.get('train_loss', None)\n",
    "    valid_loss = checkpoint.get('valid_loss', None)\n",
    "    \n",
    "    return epoch, train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE THE MODEL HERE ###\n",
    "my_path = 'my_CNN_0111.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing the number of intermediate evals at epoch 1\n",
      "225\n",
      "[112]\n",
      "Step: 112\n",
      "Epoch: 1/20... Train Loss: 0.683669... Val Loss: 0.606843\n",
      "Validation loss decreased (inf --> 0.606843).  Saving model ...\n",
      "Using my_CNN_0111.pt to save\n",
      "Step: 225\n",
      "Epoch: 1/20... Train Loss: 0.631107... Val Loss: 0.559139\n",
      "Validation loss decreased (0.606843 --> 0.559139).  Saving model ...\n",
      "Using my_CNN_0111.pt to save\n",
      "Changing the number of intermediate evals at epoch 2\n",
      "225\n",
      "[22, 45, 67, 90, 112, 135, 157, 180, 202]\n",
      "Step: 22\n",
      "Epoch: 2/20... Train Loss: 0.494387... Val Loss: 0.427642\n",
      "Validation loss decreased (0.559139 --> 0.427642).  Saving model ...\n",
      "Using my_CNN_0111.pt to save\n",
      "Step: 45\n",
      "Epoch: 2/20... Train Loss: 0.462284... Val Loss: 0.420139\n",
      "Validation loss decreased (0.427642 --> 0.420139).  Saving model ...\n",
      "Using my_CNN_0111.pt to save\n",
      "Step: 67\n",
      "Epoch: 2/20... Train Loss: 0.456185... Val Loss: 0.400728\n",
      "Validation loss decreased (0.420139 --> 0.400728).  Saving model ...\n",
      "Using my_CNN_0111.pt to save\n",
      "Step: 90\n",
      "Epoch: 2/20... Train Loss: 0.441878... Val Loss: 0.388094\n",
      "Validation loss decreased (0.400728 --> 0.388094).  Saving model ...\n",
      "Using my_CNN_0111.pt to save\n",
      "Step: 112\n",
      "Epoch: 2/20... Train Loss: 0.429275... Val Loss: 0.371263\n",
      "Validation loss decreased (0.388094 --> 0.371263).  Saving model ...\n",
      "Using my_CNN_0111.pt to save\n",
      "Step: 135\n",
      "Epoch: 2/20... Train Loss: 0.422256... Val Loss: 0.367619\n",
      "Validation loss decreased (0.371263 --> 0.367619).  Saving model ...\n",
      "Using my_CNN_0111.pt to save\n",
      "Step: 157\n",
      "Epoch: 2/20... Train Loss: 0.419052... Val Loss: 0.443226\n",
      "Step: 180\n",
      "Epoch: 2/20... Train Loss: 0.418423... Val Loss: 0.349284\n",
      "Validation loss decreased (0.367619 --> 0.349284).  Saving model ...\n",
      "Using my_CNN_0111.pt to save\n",
      "Step: 202\n",
      "Epoch: 2/20... Train Loss: 0.412610... Val Loss: 0.374007\n",
      "Step: 225\n",
      "Epoch: 2/20... Train Loss: 0.409492... Val Loss: 0.354761\n",
      "Changing the number of intermediate evals at epoch 3\n",
      "225\n",
      "[15, 30, 45, 60, 75, 90, 105, 120, 135, 150, 165, 180, 195, 210]\n",
      "Step: 15\n",
      "Epoch: 3/20... Train Loss: 0.306690... Val Loss: 0.355475\n",
      "Step: 30\n",
      "Epoch: 3/20... Train Loss: 0.317438... Val Loss: 0.344539\n",
      "Validation loss decreased (0.349284 --> 0.344539).  Saving model ...\n",
      "Using my_CNN_0111.pt to save\n",
      "Step: 45\n",
      "Epoch: 3/20... Train Loss: 0.315433... Val Loss: 0.379022\n",
      "Step: 60\n",
      "Epoch: 3/20... Train Loss: 0.317700... Val Loss: 0.332612\n",
      "Validation loss decreased (0.344539 --> 0.332612).  Saving model ...\n",
      "Using my_CNN_0111.pt to save\n",
      "Step: 75\n",
      "Epoch: 3/20... Train Loss: 0.312805... Val Loss: 0.359162\n",
      "Step: 90\n",
      "Epoch: 3/20... Train Loss: 0.314658... Val Loss: 0.351454\n",
      "Step: 105\n",
      "Epoch: 3/20... Train Loss: 0.313008... Val Loss: 0.352204\n",
      "Step: 120\n",
      "Epoch: 3/20... Train Loss: 0.307406... Val Loss: 0.335160\n",
      "Step: 135\n",
      "Epoch: 3/20... Train Loss: 0.301460... Val Loss: 0.316090\n",
      "Validation loss decreased (0.332612 --> 0.316090).  Saving model ...\n",
      "Using my_CNN_0111.pt to save\n",
      "Step: 150\n",
      "Epoch: 3/20... Train Loss: 0.295560... Val Loss: 0.337894\n",
      "Step: 165\n",
      "Epoch: 3/20... Train Loss: 0.298574... Val Loss: 0.353381\n",
      "Step: 180\n",
      "Epoch: 3/20... Train Loss: 0.295811... Val Loss: 0.313541\n",
      "Validation loss decreased (0.316090 --> 0.313541).  Saving model ...\n",
      "Using my_CNN_0111.pt to save\n",
      "Step: 195\n",
      "Epoch: 3/20... Train Loss: 0.294471... Val Loss: 0.400334\n",
      "Step: 210\n",
      "Epoch: 3/20... Train Loss: 0.294373... Val Loss: 0.299408\n",
      "Validation loss decreased (0.313541 --> 0.299408).  Saving model ...\n",
      "Using my_CNN_0111.pt to save\n",
      "Step: 225\n",
      "Epoch: 3/20... Train Loss: 0.293450... Val Loss: 0.301455\n",
      "Step: 15\n",
      "Epoch: 4/20... Train Loss: 0.200462... Val Loss: 0.341810\n",
      "Step: 30\n",
      "Epoch: 4/20... Train Loss: 0.219322... Val Loss: 0.357359\n",
      "Step: 45\n",
      "Epoch: 4/20... Train Loss: 0.231337... Val Loss: 0.328861\n",
      "Step: 60\n",
      "Epoch: 4/20... Train Loss: 0.221155... Val Loss: 0.321353\n",
      "Step: 75\n",
      "Epoch: 4/20... Train Loss: 0.223598... Val Loss: 0.303894\n",
      "Step: 90\n",
      "Epoch: 4/20... Train Loss: 0.218258... Val Loss: 0.306772\n",
      "Step: 105\n",
      "Epoch: 4/20... Train Loss: 0.214912... Val Loss: 0.327879\n",
      "Step: 120\n",
      "Epoch: 4/20... Train Loss: 0.218271... Val Loss: 0.291259\n",
      "Validation loss decreased (0.299408 --> 0.291259).  Saving model ...\n",
      "Using my_CNN_0111.pt to save\n",
      "Step: 135\n",
      "Epoch: 4/20... Train Loss: 0.216134... Val Loss: 0.303885\n",
      "Step: 150\n",
      "Epoch: 4/20... Train Loss: 0.213567... Val Loss: 0.335587\n",
      "Step: 165\n",
      "Epoch: 4/20... Train Loss: 0.212739... Val Loss: 0.298420\n",
      "Step: 180\n",
      "Epoch: 4/20... Train Loss: 0.214309... Val Loss: 0.294089\n",
      "Step: 195\n",
      "Epoch: 4/20... Train Loss: 0.213767... Val Loss: 0.291606\n",
      "Step: 210\n",
      "Epoch: 4/20... Train Loss: 0.213428... Val Loss: 0.304127\n",
      "Step: 225\n",
      "Epoch: 4/20... Train Loss: 0.214122... Val Loss: 0.305680\n",
      "Step: 15\n",
      "Epoch: 5/20... Train Loss: 0.142633... Val Loss: 0.365595\n",
      "Step: 30\n",
      "Epoch: 5/20... Train Loss: 0.127702... Val Loss: 0.350684\n",
      "Step: 45\n",
      "Epoch: 5/20... Train Loss: 0.129951... Val Loss: 0.340045\n",
      "Step: 60\n",
      "Epoch: 5/20... Train Loss: 0.133736... Val Loss: 0.320998\n",
      "Step: 75\n",
      "Epoch: 5/20... Train Loss: 0.136736... Val Loss: 0.317588\n",
      "Step: 90\n",
      "Epoch: 5/20... Train Loss: 0.135777... Val Loss: 0.332575\n",
      "Step: 105\n",
      "Epoch: 5/20... Train Loss: 0.140320... Val Loss: 0.306520\n",
      "Step: 120\n",
      "Epoch: 5/20... Train Loss: 0.143173... Val Loss: 0.309341\n",
      "Step: 135\n",
      "Epoch: 5/20... Train Loss: 0.147339... Val Loss: 0.333256\n",
      "Step: 150\n",
      "Epoch: 5/20... Train Loss: 0.154142... Val Loss: 0.322599\n",
      "Step: 165\n",
      "Epoch: 5/20... Train Loss: 0.159090... Val Loss: 0.383430\n",
      "Step: 180\n",
      "Epoch: 5/20... Train Loss: 0.159620... Val Loss: 0.304846\n",
      "Step: 195\n",
      "Epoch: 5/20... Train Loss: 0.159669... Val Loss: 0.325576\n",
      "Step: 210\n",
      "Epoch: 5/20... Train Loss: 0.158313... Val Loss: 0.328313\n",
      "Step: 225\n",
      "Epoch: 5/20... Train Loss: 0.158901... Val Loss: 0.335926\n",
      "Step: 15\n",
      "Epoch: 6/20... Train Loss: 0.098007... Val Loss: 0.353672\n",
      "Step: 30\n",
      "Epoch: 6/20... Train Loss: 0.121299... Val Loss: 0.422478\n",
      "Step: 45\n",
      "Epoch: 6/20... Train Loss: 0.120987... Val Loss: 0.379011\n",
      "Step: 60\n",
      "Epoch: 6/20... Train Loss: 0.116181... Val Loss: 0.375207\n",
      "Stopping optimization... DONE!\n",
      "...DONE\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "\n",
    "epochs = 20 \n",
    "patience = 25\n",
    "missteps = 0\n",
    "init_epoch = 1\n",
    "\n",
    "train_loader = train_loader_100\n",
    "\n",
    "eval_schedule = [1,2,3]\n",
    "inter_evals_list = [1,9,14]\n",
    "inter_evals = 9\n",
    "eval_list = [int(len(train_loader)/(inter_evals+1)*ii) for ii in range(1,inter_evals+1)]\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "\n",
    "\n",
    "# train for some number of epochs\n",
    "for e in range(init_epoch, init_epoch+epochs):\n",
    "    if missteps > patience:\n",
    "        print('...DONE')\n",
    "        break\n",
    "    if e in eval_schedule:\n",
    "        print('Changing the number of intermediate evals at epoch',e)\n",
    "        idx = eval_schedule.index(e)\n",
    "        #train_loader = loader_list[idx]\n",
    "        inter_evals = inter_evals_list[idx]\n",
    "        print(len(train_loader))\n",
    "        eval_list = [int(len(train_loader)/(inter_evals+1)*ii) for ii in range(1,inter_evals+1)]\n",
    "        print(eval_list)\n",
    "        \n",
    "    net.train()\n",
    "    # batch loop\n",
    "    train_loss = 0.0\n",
    "    train_size = 0\n",
    "    step = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        net.train()\n",
    "        step+=1\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        #Number of items in the batch\n",
    "        train_size += inputs.size(0)\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "        # get the output from the model\n",
    "        output = net(inputs)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        if step in eval_list + [len(train_loader)]:\n",
    "            print('Step:', step)\n",
    "            \n",
    "            # Get validation loss\n",
    "            valid_loss = 0.0\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "                # move tensors to GPU if CUDA is available\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                #Compute validation loss\n",
    "                output = net(inputs)\n",
    "                valid_loss += criterion(output.squeeze(), labels.float()).item()\n",
    "\n",
    "            # Print results\n",
    "            print(\"Epoch: {}/{}...\".format(e, epochs),\n",
    "                  \"Train Loss: {:.6f}...\".format(train_loss/train_size),\n",
    "                  \"Val Loss: {:.6f}\".format(valid_loss/len(valid_loader.dataset)))\n",
    "            if valid_loss <= valid_loss_min:\n",
    "                missteps = 0  \n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                valid_loss_min/len(valid_loader.dataset),\n",
    "                valid_loss/len(valid_loader.dataset)))\n",
    "                save_checkpoint(my_path, net, optimizer, epoch=e, train_loss=train_loss, valid_loss=valid_loss,\n",
    "                                words=net.words, word_to_int= net.word_to_int, params=net.get_params())\n",
    "                valid_loss_min = valid_loss\n",
    "            else:\n",
    "                missteps+=1\n",
    "                if missteps > patience:\n",
    "                    print('Stopping optimization... DONE!')\n",
    "                    break\n",
    "        \n",
    "else:\n",
    "    print('DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentCNN(\n",
       "  (embedding): Embedding(27116, 128, padding_idx=0)\n",
       "  (conv_1a): Conv2d(1, 8, kernel_size=(1, 128), stride=(1, 1))\n",
       "  (conv_1b): Conv2d(1, 16, kernel_size=(3, 128), stride=(1, 1), padding=(1, 0))\n",
       "  (conv_1c): Conv2d(1, 32, kernel_size=(5, 128), stride=(1, 1), padding=(2, 0))\n",
       "  (conv_2a): Conv2d(8, 8, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "  (conv_2b): Conv2d(16, 16, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "  (conv_2c): Conv2d(32, 32, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "  (maxpool_3): MaxPool2d(kernel_size=(3, 1), stride=(3, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_3): Conv2d(56, 112, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "  (conv_4): Conv2d(112, 112, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "  (conv_5): Conv2d(112, 224, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "  (conv_6): Conv2d(224, 224, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "  (conv_7): Conv2d(224, 224, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "  (fc1): Linear(in_features=224, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (dropout2d): Dropout2d(p=0.2)\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (relu): ReLU()\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch, train_loss, valid_loss = load_checkpoint(my_path, net, optimizer)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Testing\n",
    "\n",
    "We will test or neural network in two ways\n",
    "\n",
    "* **Test data performance:** First, we'll see how our trained model performs on all of our defined test_data, above. We'll calculate the average loss and accuracy over the test data, and plot some parameters to assess the model.\n",
    "\n",
    "* **Inference on user-generated data:** Second, we'll see if we can input just one example review at a time (without a label), and see what the trained model predicts and if the results are the ones expected based on the sentiment we infer from the used sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3244\n",
      "Test accuracy: 0.8609\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_loss = 0.0 # track loss\n",
    "num_correct = 0\n",
    "test_probas = []\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output = net(inputs)\n",
    "    test_probas.extend(output.squeeze().detach().cpu().numpy())\n",
    "    # calculate loss\n",
    "    test_loss += criterion(output.squeeze(), labels.float()).item()\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    #correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# Convert test_probas to numpy array\n",
    "test_probas = np.array(test_probas)\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.4f}\".format(test_loss/len(test_loader.dataset)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's save the predictions annd the test lables\n",
    "with open('Simple_CNN_probas.pkl','wb') as f:\n",
    "    pickle.dump(test_probas, f)\n",
    "with open('test_y.pkl','wb') as f:\n",
    "    pickle.dump(test_y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred_1</th>\n",
       "      <th>Pred_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Label_1</th>\n",
       "      <td>10763</td>\n",
       "      <td>1737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label_0</th>\n",
       "      <td>1741</td>\n",
       "      <td>10759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Pred_1  Pred_0\n",
       "Label_1   10763    1737\n",
       "Label_0    1741   10759"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cm = pd.DataFrame(cm(test_y, test_probas>=0.5, labels=[1,0]), index=['Label_1', 'Label_0'], columns=['Pred_1','Pred_0'] )\n",
    "test_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both types of errors are very similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's provide some metrics for the classification focused on the possitive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 0.8609\n",
      "Precision of the model for positive reviews: 0.8608\n",
      "Recall of the model for positive reviews: 0.8610\n",
      "f1-score of the model for positive reviews: 0.8609\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy of the model: {accuracy_score(test_y, test_probas>=0.5):.4f}')\n",
    "print(f'Precision of the model for positive reviews: {precision_score(test_y, test_probas>=0.5, pos_label=1):.4f}')\n",
    "print(f'Recall of the model for positive reviews: {recall_score(test_y, test_probas>=0.5, pos_label=1):.4f}')\n",
    "print(f'f1-score of the model for positive reviews: {f1_score(test_y, test_probas>=0.5, pos_label=1):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's provide some metrics for the classification focused on the negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 0.8609\n",
      "Precision of the model for negative reviews: 0.8610\n",
      "Recall of the model for negative reviews: 0.8607\n",
      "f1-score of the model for negative reviews: 0.8609\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy of the model: {accuracy_score(test_y, test_probas>=0.5):.4f}')\n",
    "print(f'Precision of the model for negative reviews: {precision_score(test_y, test_probas>=0.5, pos_label=0):.4f}')\n",
    "print(f'Recall of the model for negative reviews: {recall_score(test_y, test_probas>=0.5, pos_label=0):.4f}')\n",
    "print(f'f1-score of the model for negative reviews: {f1_score(test_y, test_probas>=0.5, pos_label=0):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the precision-recall curves for positive and negative reviews look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positive reviews\n",
    "pos_pre_rec = pd.DataFrame({name: values for values, name in \n",
    "                            zip(precision_recall_curve(test_y, test_probas, pos_label=1)[:2],['precision','recall'])})\n",
    "#Negative reviews\n",
    "neg_pre_rec = pd.DataFrame({name: values for values, name in \n",
    "                            zip(precision_recall_curve(test_y, 1-test_probas, pos_label=0)[:2],['precision','recall'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(num=1,figsize=(10,5))\n",
    "#Positive reviews\n",
    "ax = plt.subplot(1,2,1) \n",
    "sns.lineplot(x='recall',y='precision',data=pos_pre_rec,ax=ax,color='green', alpha=0.5)\n",
    "plt.fill_between(pos_pre_rec.recall.values, pos_pre_rec.precision.values, color='green', alpha=0.3)\n",
    "plt.title('Positive Class')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "#Negative reviews\n",
    "ax = plt.subplot(1,2,2) \n",
    "sns.lineplot(x='recall',y='precision',data=neg_pre_rec,ax=ax,color='red', alpha=0.5)\n",
    "plt.fill_between(neg_pre_rec.recall.values, neg_pre_rec.precision.values, color='red', alpha=0.3)\n",
    "plt.title('Negative Class')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check how the average probability of being positive and the percentage of predicted positive reviews varies \n",
    "with the number of stars of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stars</th>\n",
       "      <th>Avg Prob</th>\n",
       "      <th>Pos Revs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.127979</td>\n",
       "      <td>7.964954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.166115</td>\n",
       "      <td>10.729800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.227939</td>\n",
       "      <td>17.119244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.293977</td>\n",
       "      <td>25.009488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.714401</td>\n",
       "      <td>76.679671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0.793454</td>\n",
       "      <td>85.192982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>0.823808</td>\n",
       "      <td>88.950512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>0.844593</td>\n",
       "      <td>89.637928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stars  Avg Prob   Pos Revs\n",
       "0      1  0.127979   7.964954\n",
       "1      2  0.166115  10.729800\n",
       "2      3  0.227939  17.119244\n",
       "3      4  0.293977  25.009488\n",
       "4      7  0.714401  76.679671\n",
       "5      8  0.793454  85.192982\n",
       "6      9  0.823808  88.950512\n",
       "7     10  0.844593  89.637928"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stars = np.unique(test_stars)\n",
    "stars.sort()\n",
    "my_df={'Stars': stars, \n",
    "       'Avg Prob':[test_probas[test_stars==i].mean() for i in stars],\n",
    "      'Pos Revs': [(test_probas[test_stars==i]>=0.5).mean()*100 for i in stars]}\n",
    "\n",
    "my_df = pd.DataFrame(my_df)\n",
    "my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(num=2,figsize=(5,5))\n",
    "sns.barplot(x='Stars',y='Avg Prob',data=my_df)\n",
    "plt.title('Average Probability of Being Positive')\n",
    "plt.ylim(0,1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the higher the stars awarded to the review the higher the probability of it being positive based on our model (at least on average) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(num=3,figsize=(5,5))\n",
    "sns.barplot(x='Stars',y='Pos Revs',data=my_df)\n",
    "plt.title('Percentage of Predicted Positive Reviews')\n",
    "plt.ylim(0,100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the higher the stars awarded to a given review, the easier for the model to identify it as positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on a test review\n",
    "\n",
    "Let's write a predict function which will output if a provided review is positive or negative, as well as the probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, test_review, sequence_length=200, alignment='right'):\n",
    "    ''' Prints out whether a give review is predicted to be \n",
    "        positive or negative in sentiment, using a trained model.\n",
    "        \n",
    "        params:\n",
    "        net - A trained net \n",
    "        test_review - a review made of normal text and punctuation\n",
    "        sequence_length - the padded length of a review\n",
    "        '''\n",
    "    #Default sequence aligment: the one provided by the model if any\n",
    "    if sequence_length is None:\n",
    "        sequence_length = net.seq_length\n",
    "    #get lower case review\n",
    "    test_review = test_review.lower()\n",
    "    #remove some contractions\n",
    "    test_review = remove_contractions(test_review)\n",
    "    #remove punctuation\n",
    "    test_review = ''.join([c for c in test_review if c not in punctuation])\n",
    "    #split in words\n",
    "    test_review = test_review.split()\n",
    "    #Encode the words\n",
    "    unk = net.word_to_int.get('<unk>')\n",
    "    test_review = [[net.word_to_int.get(word,unk) for word in test_review[:sequence_length]]]\n",
    "    #Padding\n",
    "    test_review = pad_features(test_review, sequence_length, alignment=alignment)\n",
    "    #Convert into pytorch tensor\n",
    "    test_review = torch.from_numpy(test_review).type(torch.LongTensor).to(device)\n",
    "    \n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    pred = net(test_review).squeeze().item()\n",
    "    if pred >= 0.5:\n",
    "        pred = ('Positive', pred)\n",
    "    else:\n",
    "        pred = ('Negative', pred)\n",
    "    return pred\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative test review\n",
    "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive test review\n",
    "test_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Negative', 0.004679705481976271)\n",
      "('Positive', 0.8827115893363953)\n"
     ]
    }
   ],
   "source": [
    "# call function for positive and negative reviews\n",
    "print(predict(net, test_review_neg, alignment='center'))\n",
    "print(predict(net, test_review_pos, alignment='center'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dependency of the Predictions on the Length of the Sequence\n",
    "\n",
    "We have implemented our CNN in such a way that we only send to the classifier (fully connected network) a fixed number of values per filter. Then we can actually use sequences of diffetrent lengths without modifying the net. Out of curiosity, we will test how the size of the sequences modifies the predictions for the longest misclassified review and for the longest review correctly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the corretly classified review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the review: 2328\n",
      "Stars: 10\tLabel: 1\tPrediction:0.5637\n"
     ]
    }
   ],
   "source": [
    "test_goodies_mask = ((test_y) == (test_probas>0.5))\n",
    "test_goodies_idx = np.arange(0,len(test_y),dtype=np.int)[test_goodies_mask]\n",
    "test_goodies_lens = np.array([len(test_reviews_int[idx]) for idx in test_goodies_idx])\n",
    "argmax = test_goodies_lens.argmax()\n",
    "my_idx = test_goodies_idx[argmax]\n",
    "print(f\"Length of the review: {len(test_reviews_int[my_idx])}\")\n",
    "print(f\"Stars: {test_stars[my_idx]}\\tLabel: {test_y[my_idx]}\\tPrediction:{test_probas[my_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The review:\n",
      "There's a sign on The Lost Highway that says:<br /><br />*MAJOR SPOILERS AHEAD*<br /><br />(but you already knew that, didn't you?)<br /><br />Since there's a great deal of people that apparently did not get the point of this movie, I'd like to contribute my interpretation of why the plot makes perfect sense. As others have pointed out, one single viewing of this movie is not sufficient. If you have the DVD of MD, you can \"cheat\" by looking at David Lynch's \"Top 10 Hints to Unlocking MD\" (but only upon second or third viewing, please.) ;)<br /><br />First of all, Mulholland Drive is downright brilliant. A masterpiece. This is the kind of movie that refuse to leave your head. Not often are the comments on the DVDs very accurate, but Vogue's \"It gets inside your head and stays there\" really hit the mark.<br /><br />David Lynch deserves praise for creating a movie that not only has a beautifully stylish look to it - cinematography-wise, has great acting (esp. Naomi Watts), a haunting soundtrack by Badalamenti, and a very dream-like quality to it -- but on top of it all it also manages to involve the viewer in such a way that few movies have before. (After all, when is the last time you saw a movie that just wouldn't leave your mind and that everyone felt compelled to talk and write about, regardless of whether they liked it or hated it?)<br /><br />Allright, enough about all that, it's time to justify those statements.<br /><br />Most people that have gone through some effort to try to piece the plot together will have come to the conclusion that the first half of the picture is an illusion/a dream sequence.<br /><br />Of course, that's too bad for all those trying to make sense of the movie by expecting \"traditional\" methods in which the story is laid out in a timely, logic and linear manner for the viewer. But for those expecting that, I urge you to check the name of the director and come back again. ;)<br /><br />MD is the story of the sad demise of Diane Selwyn, a wannabe-actor who is hopelessly in love with another actor, Camilla Rowles. Due to Diane's lack of talent, she is constantly struggling to advance her career, and feels she failed to deliver on her own and her parents' expectations. Upon realizing that Camilla will never be hers (C. becomes engaged with Adam Kesher, the director), she hires a hitman to get rid of her, and subsequently has to deal with the guilt that it produces.<br /><br />The movie first starts off with what may seem as a strange opening for this kind of thriller; which is some 50s dance/jitterbug contest, in which we can see the main character Betty giving a great performance. We also see an elderly couple (which we will see twice more throughout the movie) together with her, and applauding her.<br /><br />No, wait. This is what most people see the first time they view it. There's actually another very significant fact that is given before the credits - the camera moving into an object (although blurry) and the scene quickly fading out. If you look closely, the object is actually a pillow, revealing that what follows is a dream.<br /><br />The main characters seen in the first half of the movie:<br /><br />Betty: Diane Selwyn's imaginary self, used in the first half of the movie that constitutes the \"dream-sequence\" - a positive portrayal of a successful, aspiring young actor (the complete opposite of Diane). 'Betty' was chosen as the name as that is the real name of the waitress at Winkies. Notice that in the dream version, the waitresses' name is 'Diane'.<br /><br />Rita: The fantasy version of Camilla Rhodes that, through Diane's dream, and with the help of an imaginary car-accident, is turned into an amnesiac. This makes her vulnerable and dependent on Diane's love. She is then conveniently placed in Betty/Diane's aunt's luxurious home which Betty has been allowed to stay in.<br /><br />Coco: In real life, Adam's mother. In the dream part, the woman in charge of the apartment complex that Betty stays in. She's mainly a strong authority figure, as can be witnessed in both parts of the film.<br /><br />Adam: The director. We know from the second half that he gets engaged with Camilla. His sole purpose for being in the first half of the movie is only to serve as a punching bag for Betty/Diane, since she develops such hatred towards him.<br /><br />Aunt Ruth: Diane's real aunt, but instead of being out of town, she is actually dead. Diane inherited the money left by her aunt and used that to pay for Camilla's murder.<br /><br />Mr. Roach: A typical Lynchian character. Not real; appears only in Diane's dream sequence. He's a mysterious, influential person that controls the chain of events in the dream from his wheelchair. He serves much of the same function as the backwards-talking dwarf (which he also plays) in Twin Peaks.<br /><br />The hitman: The person that murders Camilla. This character is basically the same in both parts of the movie, although rendered in a slightly more goofy fashion in the dream sequence (more on that below).<br /><br />Now, having established the various versions of the characters in the movie, we can begin to delve into the plot. Of course I will not go into every little detail (neither will I lay it out chronologically), but I will try to explain some of the important scenes, in relation to Lynch' \"hint-sheet\".<br /><br />As I mentioned above, Camilla was re-produced as an amnesiac through her improbable survival of a car-accident in the first 10 minutes of the movie, which left her completely vulnerable. What I found very intriguing with MD, is that Lynch constantly gives hints on what is real and what isn't. I've already mentioned the camera moving into the pillow, but notice how there's two cars riding in each lane approaching the limo.<br /><br />Only one of the cars actually hit the limo; what about the other? Even if they stayed clear of the accident themselves, wouldn't they try to help the others, or at least call for help? My theory is that, since this is a dream, the presence of the other car is just set aside, and forgotten about. Since, as Rogert Ebert so eloquently puts it \"Like real dreams, it does not explain, does not complete its sequences, lingers over what it finds fascinating, dismisses unpromising plotlines.\"<br /><br />Shortly after Rita crawls down from the crash site at Mulholland Dr., and makes her way down the hillside and sneaks into Aunt Ruth's apartment, Betty arrives and we see this creepy old couple driving away, staring ghoulishly at each other and grinning at themselves and the camera. This is the first indication that what we're seeing is a nightmare.<br /><br />Although the old couple seem to be unfamiliar to Betty, I think they're actually her parents (since they were applauding her at the jitterbug contest). Perhaps she didn't know them all that well, and didn't really have as good a relationship with them as she wanted, so the couple is shown as very pleasant and helpful to her in the dream. They also represent her feelings of guilt from the murder, and Diane's sense of unfulfillment regarding her unachieved goals in her life.<br /><br />A rather long and hilarious scene is the one involving the hitman. Diane apparently sees him as the major force behind the campaign trying to pressure the director to accept Camilla's part in the movie (from Adam's party in the second half of the movie), and he therefore occupies a major part of her dream. Because of her feelings of guilt and remorse towards the murder of Camilla, a part of her wants him to miss, so she turns him into a dumb criminal.<br /><br />This scene, I think, is also Lynch's attempt at totally screwing his audience over, since they're given a false pretence in which to view the movie.<br /><br />Gotta love that 'Something just bit me bad' line, though. :)<br /><br />The next interesting scene is the one with the two persons at Twinkies, who are having a conversation about how one of them keep having this recurring nightmare involving a man which is seen by him through a wall outside of the diner that they're sitting in. After a little talk, they head outside and keep walking toward the corner of a fence, accompanied of course by excellent music matching the mood of the scene.<br /><br />When reaching the corner, a bum-like character with a disfigured face appears out from behind the corner, scaring the living crap out of the man having the nightmare. This nightmare exists only in Diane's mind; she saw that guy in the diner when paying for the murder. So, in short, her obessions translate into that poor guy's nightmares. The bum also signifies Diane's evil side, as can be witnessed later in the movie.<br /><br />The Cowboy constitutes (along with the dwarf) one of the strange characters that are always present in the Lynchian landscape -- Diane only saw him for a short while at Adam's party, but just like our own dreams can award insignificant persons that we hardly know a major part in our dreams, so can he be awarded an important part in her dream. We are also given further clues during his scenes that what we're seeing is not real (his sudden disappearance, etc.)<br /><br />The Cowboy is also used as a tool to mock the Director, when he meets up with him at the odd location (the lights here give a clear indication that this is part of a dream). Also notice how he says that he will appear one more time if he (Adam) does good, or two more times if he does bad. Throughout the movie he appears two more times, indicating to Diane that she did bad. He is also the one to wake her up to reality (that scene is probably an illusion made to fit into her requirements of him appearing twice), and shortly thereafter she commits suicide.<br /><br />The espresso-scene with the Castigliane brothers (where we can see Badalamenti, the composer, as Luigi) is probably a result of the fact that Diane was having an espresso just before Camilla and Adam made their announcement at Adam's party in the second half. It could at the same time also be a statement from Lynch.<br /><br />During the scene in which they enter Diane's apartment, the body lying in the bed is Camilla, but notice how she's assumed Diane's sleeping position; Diane is seeing herself in her own dream, but the face is not hers, although it had the same wounds on the face as Diane would have after shooting herself. This scene is also filled with some genuine Lynchian creepiness. Since Diane did not know where (or when) the hitman would get to Camilla and finish her off, she just put her into her own home.<br /><br />In real life, Diane's audition for the movie part was bad. In her dream, she delivers a perfect audition - leaving the whole crew ecstatic about her performance.<br /><br />Also interesting is the fact that the money that in real-life was used to pay for Camilla's murder now appears in Rita/Camilla's purse. This is part of Diane's undoing of her terrible act by effectively being given the money back, as the murder now hasn't taken place.<br /><br />When her neighbor arrives to get her piano-shaped ashtray, another hint is given; she takes the ashtray from her table and leaves, yet later when Camilla and Betty have their encounter on the couch, we see the ashtray appear again when the camera pans over the table, suggesting that Betty's encounter with the neighbor was a fantasy.<br /><br />The catch phrase of the movie Adam is auditioning actresses for is \"She is the girl\"; which are the exact same words that Diane uses when giving the hitman Camilla's photo resume.<br /><br />The blue box and the key represent the major turning point in the movie, and is where the true identities of the characters are revealed. There's much symbolism going on here; the box may represent Diane's future (it's empty), or it may be a sort of a Pandora's box (the hitman laughs when she asks him what the key will open). Either way, it is connected to the murder by means of the blue key (which is placed next to her after the murder has taken place). The box is also seen at the end of the movie in the hands of the disfigured bum.<br /><br />Club Silencio is a neat little addition to further remind the viewer that what s/he is viewing is not real. It also signifies that Diane is about to wake up to her reality (her reality being a nightmare that she is unable to escape from, even in her dreams).<br /><br />During the chilling scene at the end where the creepy old couple reappear, Diane is tormented in such a way that she sees suicide as the only way out in order to escape the screams and to avoid being haunted by her fears.<br /><br />Anyway, that is my $0.02. Hope this could help people from bashing out at this movie and calling it 'the worst movie ever' or something to that effect, without realizing the plot.<br /><br />As usual, Lynch is all about creating irrational fears, and he certainly achieves that with this picture as well.<br /><br />10 out of 10.\n"
     ]
    }
   ],
   "source": [
    "print('The review:')\n",
    "with open(test_pos_dir+test_pos_rev_files[my_idx], 'r',encoding=\"utf8\") as f:\n",
    "    my_review=f.read()\n",
    "    print(my_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:200\t('Positive', 0.950771152973175)\n",
      "Length:250\t('Positive', 0.93666011095047)\n",
      "Length:300\t('Positive', 0.5891203284263611)\n",
      "Length:350\t('Positive', 0.5637445449829102)\n",
      "Length:400\t('Positive', 0.5630823969841003)\n",
      "Length:450\t('Positive', 0.555988073348999)\n",
      "Length:500\t('Positive', 0.5753641724586487)\n",
      "Length:550\t('Positive', 0.5977988243103027)\n",
      "Length:600\t('Positive', 0.5717465877532959)\n",
      "Length:650\t('Positive', 0.5618473291397095)\n",
      "Length:700\t('Positive', 0.5610382556915283)\n",
      "Length:750\t('Positive', 0.5610247254371643)\n",
      "Length:800\t('Positive', 0.5365739464759827)\n",
      "Length:850\t('Positive', 0.5361559987068176)\n",
      "Length:900\t('Positive', 0.5357306599617004)\n",
      "Length:950\t('Positive', 0.5361732244491577)\n",
      "Length:1000\t('Positive', 0.5341826677322388)\n",
      "Length:1050\t('Positive', 0.5341410636901855)\n",
      "Length:1100\t('Positive', 0.5341410636901855)\n",
      "Length:1150\t('Positive', 0.5341455936431885)\n",
      "Length:1200\t('Positive', 0.5343227386474609)\n",
      "Length:1250\t('Positive', 0.5345247983932495)\n",
      "Length:1300\t('Positive', 0.5344832539558411)\n",
      "Length:1350\t('Positive', 0.5346055030822754)\n",
      "Length:1400\t('Positive', 0.5343702435493469)\n",
      "Length:1450\t('Positive', 0.5341991782188416)\n",
      "Length:1500\t('Positive', 0.535144031047821)\n",
      "Length:1550\t('Positive', 0.5334023833274841)\n",
      "Length:1600\t('Positive', 0.5330477356910706)\n",
      "Length:1650\t('Positive', 0.5335860252380371)\n",
      "Length:1700\t('Positive', 0.5355088710784912)\n",
      "Length:1750\t('Positive', 0.5354769825935364)\n",
      "Length:1800\t('Positive', 0.5354769825935364)\n",
      "Length:1850\t('Positive', 0.5345996618270874)\n",
      "Length:1900\t('Positive', 0.5346487760543823)\n",
      "Length:1950\t('Positive', 0.5346224308013916)\n",
      "Length:2000\t('Positive', 0.5332590341567993)\n",
      "Length:2050\t('Positive', 0.5333507061004639)\n",
      "Length:2100\t('Positive', 0.533326268196106)\n",
      "Length:2150\t('Positive', 0.533326268196106)\n",
      "Length:2200\t('Positive', 0.5333158373832703)\n",
      "Length:2250\t('Positive', 0.5332974195480347)\n",
      "Length:2300\t('Positive', 0.5332505702972412)\n"
     ]
    }
   ],
   "source": [
    "for sl in range(200,len(test_reviews_int[my_idx]),50):\n",
    "    print(f\"Length:{sl}\\t{predict(net, my_review, sequence_length=sl, alignment='center')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independently of the size, the network predicts a possitive sentiment (the correct result). On the other hand the probability varies widly (0.53-0.95) and the best results are obtained if the sequence length is smaller that 300 words. For sequence lengths larger than 750, the probability remains almost constant at 0.53."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it affects the misclassified review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the review: 2154\n",
      "Stars: 10\tLabel: 1\tPrediction:0.1349\n"
     ]
    }
   ],
   "source": [
    "test_error_mask = ((test_y) == (test_probas<0.5))\n",
    "test_erorr_idx = np.arange(0,len(test_y),dtype=np.int)[test_error_mask]\n",
    "test_error_lens = np.array([len(test_reviews_int[idx]) for idx in test_erorr_idx])\n",
    "argmax = test_error_lens.argmax()\n",
    "my_idx = test_erorr_idx[argmax]\n",
    "print(f\"Length of the review: {len(test_reviews_int[my_idx])}\")\n",
    "print(f\"Stars: {test_stars[my_idx]}\\tLabel: {test_y[my_idx]}\\tPrediction:{test_probas[my_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The review:\n",
      "Back in the mid/late 80s, an OAV anime by title of \"Bubblegum Crisis\" (which I think is a military slang term for when technical equipment goes haywire) made its debut on video, taking inspiration from \"Blade Runner\", \"The Terminator\" and maybe even \"Robocop\", with a little dash of Batman/Bruce Wayne - Iron Man/Tony Stark and Charlie's Angel's girl power thrown in for good measure. 8 episodes long, the overall story was that in 21st century Tokyo, Japan, year 2032-2033, living machines called Boomers were doing manual labor and sometimes cause problems. A special, SWAT like branch of law enforcers, the Advanced Police (AD Police for short) were formed to handle the boomers, but were mostly ineffective, prompting millionaire scientist Sylia Stingray, the daughter of the scientist who made the boomers, to create four powered combat armor (hard suits) to be worn by women to fight the boomers and fight the evil corporation that produced the boomers, GENOM. That group becomes known as the Knight Sabers, and in addition to ring leader Sylia, her rag-tag band of rebel women included Priss Asagiri, a struggling rock and roll gal with a passion for motorcycles and a disdain for cops, Linna Yamazaki, an aerobics instructor with an eye for money and a tendency to blow through boyfriends, and Nene Romanova, a young officer of the ADP and expert computer hacker (the first in a long line). GENOM, meanwhile, is represented by Quincy, a tall, gaunt old guy who happens to own the company, his younger assistant Brian J. Mason (killed in episode 3) and an annoying boomer man named Largo. Other characters included Leon McNichol and Daley Wong, two AD Police detectives (Leon appeared in a spin-off/prequel anime, \"AD Police Files\" which I heard was very dark), their balding, overweight boss Chief Todo, Sylia's younger brother Mackey, and a funny little mechanic known as Dr. Raven, who apparently helps Sylia with maintaining the suits. Aside from the overall Knight Sabers & AD Police VS GENOM storyline, there was also another storyline involving a friend of Linna's who was apparently a daughter in a big crime family, the annoying Largo trying to usurp GENOM, and various Priss-wants-revenge-for-a-minor-character story. Oh and did I mention that there were hints that Sylia herself might have been a boomer?<br /><br />Well, it was a great watch, full of chaos and mayhem and even some very nice pop songs, but it was not without its flaws, some of which, unfortunately, were due to the fact that the series was discontinued after episode 8 when it was originally planned for 13 episodes in all. So some of the storylines, like Largo's scheme (or schemes), the family of Linna's ill-fated friend, and Sylia's origins, were never resolved. Another problem with the series was that at the time Priss was the most popular character, so a good portion of the series focused on her, and unfortunately, most of the Priss oriented episodes basically focused on Priss self-righteously seeking justice/revenge for some secondary character who had never appeared before but happened to be a friend of hers, yet she rarely went out of her way for her the Knight Sabers, who were always bailing her out of trouble and for some reason cared a great deal about her well-being (just to be fair though, she did go to rescue Linna in episode 7, and her boyfriend got killed by a boomer and the ADP acted wrongly in the investigation). This meant we didn't really get to focus on the more interesting back story of Sylia, or even the day-to-day antics of Nene and Linna. Linna had two episodes oriented around her, which pertained to her friend with the mafia family, while Nene managed to snag the last episode for herself, which showed her eternal good cheer was genuinely good spirits and not ditziness. Nene also got to put her computer skills to good use quite a bit, or she sometimes just acted like a lovable goof, which put her screen time and character development a few notches above poor Linna, who was often thrust into the background with only her greed and her tendency to eat up boyfriends to get her any attention. Don't get me wrong, I like it and I love the overall concept of it all, but it did irk me a little bit. Also this is one of those runner-ups for \"worst English voice dubbing of all time\" features, meaning you'd better stick to the Japanese. Some of the voices were okay (some really did match their characters personas) but others were just flat and passionless or, in the case of Priss, really overacted.<br /><br />Well, Tokyo 2040 comes along and pretty much tosses all that out the window. Set a few years ahead, the story here is that after earthquakes shattered Tokyo, GENOM's boomers rebuilt the city into a big old paradise, except the boomers still have a tendency to fly off the handle, which prompts the AD Police to be formed followed by the Knight Sabers being formed. So the overall story is the same, but the backstories of the characters and the look and attitudes of the characters have changed a lot.<br /><br />1) Originally Sylia had short purplish black hair and brown eyes, was usually dressed like a stern, proper business woman and was distant from others. 2040 Sylia has more of a super-model look to her, dressing more provocatively and possessing white hair and blue eyes that seem to change color depending on the light (runs the gamut from blue to purple to silver and eyes occasionally looking purple or gray), and also 2040 Sylia is more of an emotionally unstable woman who flies off the handle when she's not in public, and possibly keeps even more secrets than before. Sylia also doesn't take as much risk on the battlefield, as she is more of a stay-in-the-mobile-pit type here, but she does do battle when she has to.<br /><br />2) Originally Priss was a short woman with an Afro and a really bad temper, always picking fights with people who offended her, always biting off more than she could chew, etc. 2040 Priss, however, has gone the way of the Clint Eastwood loner - very cold, very stoic and emotionally distant (more like the original Sylia you might say), so she's not really attached to anyone. Also her hair is more stingry and cat-like (a big improvement) and she is clad in leather like Trinity from \"The Matrix\" (although much less annoying than before, unfortunately, the writers screw her in the end when revealing her reasons for hating the ADP).<br /><br />3) Originally Linna had this big black hair going for her, but now her hair is shorter, browner, and, well, more 90s like. 2040 Linna is also an office lady who has bad luck with being sexually harrassed. As if to apologize for the way she was treated by the OAV writers, the 2040 writers actually dedicated the first 6 episodes to Linna, writing her as a country girl new to the city but determined to meet the Knight Sabers and win a spot with them, which she eventually does.<br /><br />4) Originally a short red haired girl who was often the victim of ridicule and ate a lot of candy, Nene is now a short blonde haired girl who likes to tease and take pot shots at ADP detective Leon McNichol (revenge for him toying with her in the OAV?) and other characters, even her surrogate big sister Linna and Mackey, Sylia's \"brother\", whom she becomes infatuated with. Cockey and arrogant, she still eats a lot of candy and is a master hacker, but she is eventually deflated and grows beyond her comic relief status.<br /><br />5) Nigel Kirkland is a new character, a tall, stoic, ruggedly handsome man with long black hair (he looks like Adrian Paul from TV's Highlander), he replaces Dr. Raven from the old series and now serves as the man who gives maitenance to Sylia's hard suits. Nigel is also Sylia's lover, but you wouldn't know it by his demeanor. He's kind of the father/big brother/mentor figure to Mackey.<br /><br />6) Leon and Daley are back, but of course differently. The original Leon was a tall pretty boy built like a baseball player with slicked back brown hair, blue eyes, a black leather jacket, tight blue jeans, and always carrying a revolver that could magically pack more whallop than a howitzer if necessary; while he wasn't really a bad guy deep down, he was kind of a jerk, but he served mostly as comic relief, as he tried to pursue Priss romantically (exactly what he saw in her is a mystery) but occasionally he and Daley served as information guides to important plot points. Also the original Daley was a fairly muscular red head who dressed in pink/purple suits as he was a flamboyantly homosexual character who was always hitting on Leon when not providing important information. In 2040, Leon is no longer a pretty boy but more your typical rugged tough guy type, with spiked black hair, brown eyes, tall and sporting big muscles, a brown leather jacket and blue dockers (he actually looks like Arnold Schwarzenegger a little bit, or maybe a pumped up Colin Farrell, or Hugh Jackman), and he still carries a revolver, a BIG one, but it's not as powerful as before. Although 2040 Leon still has a bit of an attitude probelm (especially in approaching Priss), he's not nearly as much of a jerk as he was in the old series, but he does have a bad temper and he is easily annoyed by Nene and Daley (also he drinks way too much coffee). Oh, and Leon is still after Priss, but he has a lot more luck this time around. Daley, meanwhile, is now a taller (but not as tall as Leon) more pretty boyish looking guy with red rimmed glasses, a white suit, green eyes, and light brown hair, and he carries a big machine gun (he actually looks like James Marsden from the X-Men films); Daley is a lot smarter and more assertive in 2040 than the OAV and, although it's not completely clear, his homosexual tendencies have been almost totally disappeared, save a moment of what appears to be jealousy when he hears about Leon inquiring about Priss's e-mail.<br /><br />7) Brian J. Mason (what does that \"J\" stand for?) is back, and so is Quincy, but Mason is much more the main villain here, with Quincy as co-villain, as he is no longer a towering figure of terror but a vegetable with a bunch of batteries and wires plugged into him. Mason now sports slicked back brown hair instead of black hair as he did in the OAV (he actually looks like OAV Leon in a suit) and he is very much from the Alan Rickman school of villains.<br /><br />8) Though a pervert in the first series, Mackey is no longer a pervert in 2040. Of course, there are lots of things different about Mackey in 2040, but they won't be revealed here.<br /><br />9) Sylia now has a companion, an Alfred-the-butler type named Henderson, who worries about her and the gang.<br /><br />10) In the original series, boomers were like the Replicants in \"Blade Runner\", armed with their own thoughts and feelings and ambitions, but in 2040, they're more of the dumb-monsters-on-the-rampage type. Most of the time they're just big robots who do whatever they're programmed to do (heavy labor, combat, clean up, etc) and they have this tendency to \"go rogue\", which means try to evolve and become a monster in the process.<br /><br />What does stay the same is the theme of humanity VS technology (do machines have souls?). Sadly, this series, though well animated and well written, only runs 26 episodes, so it moves by faster than one might like, especially those of us who are used to more than one season of our most beloved characters, and unfortunately it still ends on a cliff hanger with unresolved storyline bits (which I will not discuss here. What saves this show and makes it what it is, however, is the characters, a colorful cast of screwballs they are, ranging from stoic loners, psycho women, genocidal mad men, rough neck cops, sardonic intellectuals, wise old sages, and loveable innocents, much more diverse than before and with a lot more to play off of, they're enough to make you wish this show had gone longer.<br /><br />It's not great, but it's a good watch. Also the English dub (by ADV) is quite good, though not without a few flat spots, but certainly better than the dub on the original.\n"
     ]
    }
   ],
   "source": [
    "print('The review:')\n",
    "with open(test_pos_dir+test_pos_rev_files[my_idx], 'r',encoding=\"utf8\") as f:\n",
    "    my_review=f.read()\n",
    "    print(my_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:200\t('Negative', 0.27168452739715576)\n",
      "Length:250\t('Negative', 0.2719045877456665)\n",
      "Length:300\t('Negative', 0.2028893679380417)\n",
      "Length:350\t('Negative', 0.13491219282150269)\n",
      "Length:400\t('Negative', 0.08929675072431564)\n",
      "Length:450\t('Negative', 0.23724815249443054)\n",
      "Length:500\t('Negative', 0.22157463431358337)\n",
      "Length:550\t('Negative', 0.21964150667190552)\n",
      "Length:600\t('Negative', 0.24306334555149078)\n",
      "Length:650\t('Negative', 0.476505309343338)\n",
      "Length:700\t('Positive', 0.5316502451896667)\n",
      "Length:750\t('Positive', 0.5303851962089539)\n",
      "Length:800\t('Positive', 0.5303165912628174)\n",
      "Length:850\t('Positive', 0.5303165912628174)\n",
      "Length:900\t('Positive', 0.5304065942764282)\n",
      "Length:950\t('Positive', 0.5302416682243347)\n",
      "Length:1000\t('Positive', 0.5302221775054932)\n",
      "Length:1050\t('Positive', 0.5301390886306763)\n",
      "Length:1100\t('Positive', 0.6024633646011353)\n",
      "Length:1150\t('Positive', 0.5627065300941467)\n",
      "Length:1200\t('Positive', 0.5617643594741821)\n",
      "Length:1250\t('Positive', 0.5617643594741821)\n",
      "Length:1300\t('Positive', 0.5617573857307434)\n",
      "Length:1350\t('Positive', 0.562123715877533)\n",
      "Length:1400\t('Positive', 0.562190055847168)\n",
      "Length:1450\t('Positive', 0.5608035326004028)\n",
      "Length:1500\t('Positive', 0.5608642101287842)\n",
      "Length:1550\t('Positive', 0.5608448386192322)\n",
      "Length:1600\t('Positive', 0.5608448386192322)\n",
      "Length:1650\t('Positive', 0.5608376264572144)\n",
      "Length:1700\t('Positive', 0.5608448386192322)\n",
      "Length:1750\t('Positive', 0.559873104095459)\n",
      "Length:1800\t('Positive', 0.5596996545791626)\n",
      "Length:1850\t('Positive', 0.5571394562721252)\n",
      "Length:1900\t('Positive', 0.5570389628410339)\n",
      "Length:1950\t('Positive', 0.5573877096176147)\n",
      "Length:2000\t('Positive', 0.5573672652244568)\n",
      "Length:2050\t('Positive', 0.5573672652244568)\n",
      "Length:2100\t('Positive', 0.5591776967048645)\n",
      "Length:2150\t('Positive', 0.5778908133506775)\n"
     ]
    }
   ],
   "source": [
    "for sl in range(200,len(test_reviews_int[my_idx]),50):\n",
    "    print(f\"Length:{sl}\\t{predict(net, my_review, sequence_length=sl, alignment='center')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again the probabilities strongly depend on the sequence length (0.09-0.57). Interestingly, the review is rightly classify as positive for sequence lengths of at least 700 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
